{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ee1674-e0a1-47b8-8a23-7a67373279cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "**In this practical session you will:**\n",
    "\n",
    "   - Learn the essential idea behind Machine Learning including several statistical concepts and the implementation steps under the point of view of the Data Science cycle.\n",
    "   - Download, explore and implement the preliminary processing of a multi-omics cancer dataset that will be used throughout the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aa72b7",
   "metadata": {},
   "source": [
    "## Definition:\n",
    "\n",
    "Machine learning is a subfield of artificial intelligence (AI) that focuses on developing algorithms and models that allow computers to learn patterns and make predictions or decisions without being explicitly programmed. In the context of data science and mathematical modeling, machine learning plays a crucial role in building models that represent real-world systems using mathematical concepts and language. A subfield of Machine learning is Deep learning, which uses a type of models called neural networks that are inspired in the architechture of human brains.\n",
    "\n",
    "[![AI diagram](http://danieljhand.com/images/AI_ML_DL_circles.jpeg)](http://danieljhand.com/the-relationship-between-artificial-intelligence-ai-machine-learning-ml-and-deep-learning-dl.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf8763",
   "metadata": {},
   "source": [
    "## Characteristics of Machine Learning:\n",
    "\n",
    "1. **Learning from Data:**\n",
    "   - Machine learning systems learn from data rather than relying on explicit programming by using some statistical techniques.\n",
    "   - Algorithms use available data to identify patterns, relationships, and trends.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "2. **Model Development:**\n",
    "   - Machine learning involves creating models that can generalize patterns from the training data to make predictions or decisions on new, unseen data.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "3. **Adaptability:**\n",
    "   - Machine learning models can adapt and evolve as new data becomes available, making them suitable for dynamic and changing environments.\n",
    "\n",
    "<!-- Add an empty line here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d915a3e",
   "metadata": {},
   "source": [
    "## Data Science Lifecycle:\n",
    "\n",
    "The data science lifecycle involves several key steps that machine learning implementations follows:\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "1. **Identification of the problem:**\n",
    "   - Allows the decision on the suitable model and algorithm and definition of training and test datasets.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "2. **Data Collection:**\n",
    "   - Gather relevant data from various sources, ensuring it is representative and suitable for the problem at hand.\n",
    "   - It is highly important to perform exploratory analysis to evaluate the quality of the data and, if suitable, define the subsequent necessary processing steps.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "3. **Data Processing:** This is probably the most relevant step: independently of a succesful implementation of the previous steps, if the data does not contain the information relevant to solve the problem and or present in an inadequate state for the algorithm to learn from, the resulting model will be useless (garbage-in -> garbage-out). It mainly consists of two steps.\n",
    "\n",
    "    3.1. **Data Pre-processing:**\n",
    "    - Clean and preprocess the data to handle missing values, outliers, and format issues.\n",
    "\n",
    "    <!-- Add an empty line here -->\n",
    "\n",
    "    3.2. **Feature Engineering:**\n",
    "    - Necessary in some cases but optional in others.\n",
    "    - Select or create features that are relevant and informative for the machine learning model.\n",
    "    - Common approaches are grouped into *Filter-based*, *Wrapper-based* and *Embedded-based* categories.\n",
    "   \n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "4. **Data modelling:** During this iterative process, each model's performance is assessed using different metrics depending if the algorithm works with categorical or continous variables.\n",
    "\n",
    "    <!-- Add an empty line here -->\n",
    "\n",
    "    4.1. **Model Training:**\n",
    "    - Use a learning algorithm to train the model on a labeled dataset, allowing it to learn patterns and relationships.\n",
    "\n",
    "    <!-- Add an empty line here -->\n",
    "\n",
    "    4.2. **Model Optimization:**\n",
    "    - Adjust model parameters and features to improve performance, often involving techniques like hyperparameter tuning. Within this step it is important to avoid overfitting (the model could be generalized to datasets beyond the training ones).\n",
    "\n",
    "    <!-- Add an empty line here -->\n",
    "\n",
    "    4.3. **Model Testing:**\n",
    "    - Validate the model on new, unseen data to ensure it generalizes well (without overfitting) and provides accurate predictions (without underfitting).\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "5. **Deployment:**\n",
    "   - Deploy the model into a real-world environment, integrating it into decision-making processes.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Data Science LyfeCycle](https://www.onlinemanipal.com/wp-content/uploads/2022/09/Data-Science-Life-cycle-768x767.png.webp)](https://www.onlinemanipal.com/blogs/data-science-lifecycle-explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3293b",
   "metadata": {},
   "source": [
    "## Types of Machine Learning\n",
    "\n",
    "Machine learning is broadly categorized into several types, each serving different purposes and solving distinct problems. Here are the main types:\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![AI diagram](https://www.freecodecamp.org/news/content/images/2020/08/ml-1.png)](https://www.freecodecamp.org/news/machine-learning-for-managers-what-you-need-to-know/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "In supervised learning, the algorithm is trained on a **labeled** dataset, where each input is paired with the corresponding output. The goal is to learn a mapping from inputs to outputs, and hence, **predict an output based on input**.\n",
    "\n",
    "The usefulness of these models is evaluated immediately since both the input and corresponding correct outputs are provided in the testing dataset.\n",
    "\n",
    "\n",
    "**a. Regression:**\n",
    "   - **Objective:** Predict a continuous target variable.\n",
    "   - **Examples:** Linear Regression, Polynomial Regression.\n",
    "\n",
    "**b. Classification:**\n",
    "   - **Objective:** Predict a discrete target variable (class labels).\n",
    "   - **Examples:** Logistic Regression, Decision Trees or Random Forest and Support Vector Machines.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "Unsupervised learning involves training on **unlabeled** data, and the algorithm tries to **discover patterns or relationships in the data** without explicit guidance on the output.\n",
    "\n",
    "Since the output is unknown in the training data, the usefulness is implicitly derived from the structure and relationships discovered in the data.\n",
    "\n",
    "**a. Clustering:**\n",
    "   - **Objective:** Group similar data points together.\n",
    "   - **Examples:** K-Means Clustering, Hierarchical Clustering.\n",
    "\n",
    "**b. Dimensionality Reduction:**\n",
    "   - **Objective:** Reduce the number of input features while preserving important information. It is also commonly used as a pre-processing step for feature extraction.\n",
    "   - **Examples:** Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE).\n",
    "\n",
    "**c. Association Rule Learning:** It will not be covered on this course.\n",
    "   - **Objective:** Discover interesting relationships between variables in large datasets.\n",
    "   - **Examples:** Apriori Algorithm, Eclat Algorithm.\n",
    "\n",
    "\n",
    "### Reinforcement Learning\n",
    "\n",
    "Reinforcement learning involves an **agent interacting with an environment**, learning to make decisions by receiving feedback in the form of rewards or penalties (mimics the human trial and error behaviour). Hence, the objective is to **learn a policy to make decisions achieving the most optimal result**.\n",
    "\n",
    "On this type of algorithms, the performance depends on the environment provided by the agent (reward or penalty) after each action, guiding it towards learning a successful policy for optimization (https://www.youtube.com/@aiwarehouse). It is usually employed for training AI for videogames rather than on -omics data analysis, so it won't be covered on this course.\n",
    "\n",
    "**a. Model-Based Reinforcement Learning:**\n",
    "\n",
    "   - **Objective:** Build an explicit model of the environment to make decisions.\n",
    "   - **Examples:** Monte Carlo Tree Search.\n",
    "\n",
    "**b. Model-Free Reinforcement Learning:**\n",
    "\n",
    "   - **Objective:** Learn to make decisions without an explicit model of the environment.\n",
    "   - **Examples:** Q-Learning, Deep Q Network (DQN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb21391",
   "metadata": {},
   "source": [
    "## Relationship with Statistical Concepts\n",
    "\n",
    "1. **Pattern Recognition:**\n",
    "   - Machine learning involves finding patterns in data, a concept deeply rooted in statistics.\n",
    "   - Depeding on the types of problem, and hence, the employed algorithm, different kinds of patterns can be extracted from data.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Pattern types](https://www.researchgate.net/profile/Gordon-Elger/publication/352727978/figure/fig2/AS:1153327744192512@1651986170131/Machine-learning-tasks-most-relevant-for-PdM.png)](https://www.researchgate.net/figure/Machine-learning-tasks-most-relevant-for-PdM_fig2_352727978)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "2. **Cross-Validation:** A key concept for supervised models when the available dataset is smaller than the optimal for the validation purposes.\n",
    "   - To assess a supervised model's generalization ability, cross-validation techniques are used to evaluate performance on multiple subsets of the data.\n",
    "   - There are multiple methodologies (https://www.turing.com/kb/different-types-of-cross-validations-in-machine-learning-and-their-explanations) although the most common is the **K-fold cross-validation** which involves partitioning the entire dataset into k number of random subsets, where k-1 are used for training and 1 for testing purposes. This is repeated for a number of iterations and the model is evaluated through the metrics obtained across interations.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Bias and Variance](https://d2mk45aasx86xg.cloudfront.net/image5_11zon_af97fe4b03.webp)](https://www.turing.com/kb/different-types-of-cross-validations-in-machine-learning-and-their-explanations) \n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "3. **Bias-Variance Tradeoff:** Also a key concept when dealing with supervised models.\n",
    "   - In statistics, the bias of an estimator is the difference between this estimator’s expected value and the true value of the parameter being estimated. On the other hand, the variance of an estimator measures how much the estimates from the estimator are likely to vary or spread out around the true, unknown parameter, through repeated sampling.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "   [![Bias and Variance](https://nvsyashwanth.github.io/machinelearningmaster/assets/images/bias_variance.jpg)](https://nvsyashwanth.github.io/machinelearningmaster/bias-variance/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "   - If we consider Machine learning predictions as estimations these two concepts acquire the following meaning in this context. \n",
    "       \n",
    "       - **Variance** is the consistency of the model predictions for a particular sample instance (for instance applying the model multiple times on subsets of the training dataset). In other words, is the sensitivity of the model to the randomness of the training dataset.\n",
    "       \n",
    "       - In contrast, **Bias** could be seen as the measure of the distance between predictions and the correct values (the labels) if we rebuild the model multiple times with different training datasets. Therefore, is the measure of the systematic error not due to randomness in the training data.\n",
    "             \n",
    "   - These two concepts are intrinsically related, and therefore, the bias-variance tradeoff is a fundamental concept in machine learning: there is an optimal model complexity that allows for good performance on the training data but still keeping the ability to generalize to new data. Deviations from these optimal area leads to either high bias (underfitting, there is still room to improve the model though training) or high variance (overfitting, excessive training on a specific dataset and unable to generalize to similar test datasets) of the model.\n",
    "   \n",
    "   <!-- Add an empty line here -->\n",
    "\n",
    "   [![Optimal complexity](https://ejenner.com/post/bias-variance-tradeoff/tradeoff_huad58a1a719791584e96223cc1385b715_74447_1200x1200_fit_q75_h2_lanczos_3.webp)](https://ejenner.com/post/bias-variance-tradeoff/)\n",
    "   \n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "   [![Underfitting and overfitting](https://www.endtoend.ai/assets/blog/misc/bias-variance-tradeoff-in-reinforcement-learning/underfit_right_overfit.png)](https://www.endtoend.ai/blog/bias-variance-tradeoff-in-reinforcement-learning/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "4. **Statistical Metrics:**\n",
    "   - Various statistical metrics are used to quantify the performance of both unsupervised, and mostly, supervised machine learning models.\n",
    "   - The type of metric used is related with the type of problem/algorithm used.\n",
    "   \n",
    "   <!-- Add an empty line here -->\n",
    "   \n",
    "   [![Supervised metrics](https://www.kdnuggets.com/wp-content/uploads/anello_machine_learning_evaluation_metrics_theory_overview_11.png)](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd29ed",
   "metadata": {},
   "source": [
    "## Case of use: Cancer genomics\n",
    "\n",
    "Cancer is a group of diseases involving abnormal cell growth with the potential to invade or spread to other parts of the body. At the very core of the etiology of cancer is somatic mutations: permanent alterations in the genetic material (either resulting from spontaneous errors during the DNA replication or as a result of DNA damage) originated throughout the somatic development (from the very first mitotic divisions of the Zygot to the human adult tissues).\n",
    "\n",
    "As sequencing technologies advanced in the past decade, the number of available tumoral whole genomes have increased exponentially, revealing that different tumors accumulate mutations with a variability of up to three orders of magnitude.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![ICGC TMB](images/ICGC_muts.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Not only the total number of mutation varies, but also the composition. The endogenous mutational processes active in a tissue as well as the mutagens a person has been exposed during their lifetime, e.g ultraviolet (UV)-light or tobacco smoking, define a set of probabilities for each nucleotide to mutate provided of its neighboring\n",
    "sequence. These probabilities can be inferred can be decomposed from the observed data into several\n",
    "components that roughly reflect the individual mutational processes affecting the cell, the so-called ‘mutation signatures’, some linked to specific mechanisms.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "**Tobacco-related signature of single base substitutions (SBS) 4**\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Tobacco Signature](https://cog.sanger.ac.uk/cosmic-signatures-production/images/v2_signature_profile_4.original.png)](https://cancer.sanger.ac.uk/signatures/signatures_v2/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "**Ultraviolet light-related signature of single base substitutions (SBS) 7**\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Tobacco Signature](https://cog.sanger.ac.uk/cosmic-signatures-production/images/v2_signature_profile_7.original.png)](https://cancer.sanger.ac.uk/signatures/signatures_v2/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Hence, the study of mutations within the Cancer Genomics field, integrated with other -omic data such as transcriptomics or epigenomics as well as clinical data has paved the latest advances in Cancer Research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e1084",
   "metadata": {},
   "source": [
    "Several international consortium have generated multi-omic cancer datasets. One of them, enmarked within The Cancer Genome Atlas (TCGA) is the Pan Cancer Analysis of Whole Genomes (PCAWG) initiative. Public available data is stored at the International Cancer Genome Consortium (ICGC) database: https://dcc.icgc.org/releases/PCAWG\n",
    "\n",
    "Some files are particularly interesting for analysis with Machine Learning techniques:\n",
    "\n",
    "- Clinical (phenotypical) information for each donor belonging to given project, contained at **pcawg_donor_clinical_August2016_v9.xlsx** file. Here you have relevant information such as the donor sex, the vital status, the treatment, the age at diagnosis and the history of smoking and alcohol habits.\n",
    "- Relationship between donor, specimen and sample identifications at **pcawg_sample_sheet.tsv** file. A donor is the individual with cancer, where several specimens (biopsies of tumor or healthy tissue) can be collected. Moreover, from these specimens more than one samples could be collected to extract omics information (WGS, RNA-seq,...).\n",
    "- A matrix with the expression in transcript per millions (TPMs) for multiple genes across several samples. This information is contained at **pcawg.rnaseq.transcript.expr.tpm.tsv.gz**.\n",
    "<!-- - A list of known detected driver mutations on samples, contained at **TableS3_panorama_driver_mutations_ICGC_samples.public.tsv.gz**. -->\n",
    "- A matrix of the proportion of mutations attributed to a given mutation signature across specimens with Signature Analyzer. The data is contained at the **SignatureAnalyzer_COMPOSITE.SBS.txt** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b28ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can start by downloading some files, for that we will need the pandas package\n",
    "import pandas as pd\n",
    "\n",
    "# We will also need numpy for some operations\n",
    "import numpy as np\n",
    "\n",
    "# Os is a basic python integrated library. The path utilities are useful to work with local files\n",
    "from os import path\n",
    "\n",
    "# To explore the datasets it is always useful to use some plotting packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Some dependencies on the seaborn package will generate warnings due to the version. Just ignore them\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5e4cde",
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno -3] Temporary failure in name resolution>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/http/client.py:1283\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1283\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/http/client.py:1329\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1329\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/http/client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/http/client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1038\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/http/client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 976\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/http/client.py:1448\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnect to a host on a given (SSL) port.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/http/client.py:942\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n\u001b[0;32m--> 942\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/socket.py:824\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    823\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 824\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    825\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    954\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# The clinical information of the donors\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m## It is an excel file, so we use the function read_excel from pandas\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m clinical_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://dcc.icgc.org/api/v1/download?fn=/PCAWG/clinical_and_histology/pcawg_donor_clinical_August2016_v9.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m clinical_df\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/site-packages/pandas/io/excel/_base.py:504\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    503\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 504\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/site-packages/pandas/io/excel/_base.py:1563\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1563\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1566\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1568\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1570\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/site-packages/pandas/io/excel/_base.py:1419\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1417\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1419\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1421\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m   1422\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1423\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/site-packages/pandas/io/common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    715\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    727\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/site-packages/pandas/io/common.py:372\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    371\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    373\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/site-packages/pandas/io/common.py:274\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MScOmicsVic/lib/python3.10/urllib/request.py:1351\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[1;32m   1349\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1352\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno -3] Temporary failure in name resolution>"
     ]
    }
   ],
   "source": [
    "# The clinical information of the donors\n",
    "## It is an excel file, so we use the function read_excel from pandas\n",
    "clinical_df = pd.read_excel('https://dcc.icgc.org/api/v1/download?fn=/PCAWG/clinical_and_histology/pcawg_donor_clinical_August2016_v9.xlsx')\n",
    "clinical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53ba2500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For starters we can explore both of these files.\n",
    "## The clinical information for donors contains mostly categorical variables but others such as the patient age at diagnosis is continous\n",
    "## Moreover, it seems there are a lot of missing data. Let's explore it.\n",
    "\n",
    "categorical_columns = ['project_code', 'donor_sex', 'donor_vital_status', 'first_therapy_type', 'first_therapy_response',\n",
    "                        'tobacco_smoking_history_indicator', 'alcohol_history', 'alcohol_history_intensity']\n",
    "\n",
    "continuous_columns = ['donor_age_at_diagnosis', 'tobacco_smoking_intensity', 'donor_survival_time', 'donor_interval_of_last_followup']\n",
    "\n",
    "# Create a list of tuples indicating whether each column is categorical or continuous\n",
    "column_types = [(col, 'categorical') if col in categorical_columns else (col, 'continuous') for col in categorical_columns + continuous_columns]\n",
    "\n",
    "n_rows = 4\n",
    "n_cols = (len(categorical_columns)+len(continuous_columns))//n_rows\n",
    "\n",
    "# Create a figure with multiple subplots (make a grid, 4 rows, 3 columns)\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(13, 16))\n",
    "\n",
    "for i, tupl in enumerate(column_types):\n",
    "    col = tupl[0]\n",
    "    cat = tupl[1]\n",
    "    if cat == 'categorical':\n",
    "        clinical_df[col].value_counts().plot.pie(autopct='%1.1f%%', ax=axes[i//n_cols, i%n_cols], startangle=90)\n",
    "        axes[i//n_cols, i%n_cols].set_title(f'Pie Chart - {col}')\n",
    "        axes[i//n_cols, i%n_cols].set_ylabel('')\n",
    "    elif cat == 'continuous':\n",
    "        sns.histplot(clinical_df[col], bins=20, kde=True, ax=axes[i//n_cols, i%n_cols])\n",
    "        axes[i//n_cols, i%n_cols].set_title(f'Histogram - {col}')\n",
    "        axes[i//n_cols, i%n_cols].set_xlabel(col)\n",
    "        axes[i//n_cols, i%n_cols].set_ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7efc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the warnings, the code ignores on the categorical plots the Non-Available data. Let's plot it with dropna=False on the value_counts() function\n",
    "# Create a figure with multiple subplots (make a grid, 4 rows, 3 columns)\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(13, 16))\n",
    "\n",
    "for i, tupl in enumerate(column_types):\n",
    "    col = tupl[0]\n",
    "    cat = tupl[1]\n",
    "    if cat == 'categorical':\n",
    "        clinical_df[col].value_counts(dropna=False).plot.pie(autopct='%1.1f%%', ax=axes[i//n_cols, i%n_cols], startangle=90)\n",
    "        axes[i//n_cols, i%n_cols].set_title(f'Pie Chart - {col}')\n",
    "        axes[i//n_cols, i%n_cols].set_ylabel('')\n",
    "    elif cat == 'continuous':\n",
    "        sns.histplot(clinical_df[col], bins=20, kde=True, ax=axes[i//n_cols, i%n_cols])\n",
    "        axes[i//n_cols, i%n_cols].set_title(f'Histogram - {col}')\n",
    "        axes[i//n_cols, i%n_cols].set_xlabel(col)\n",
    "        axes[i//n_cols, i%n_cols].set_ylabel('Frequency')\n",
    "\n",
    "# Save the figure for future uses\n",
    "plt.savefig(path.join('plots', 'Clinical_withNA.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad6bea4",
   "metadata": {},
   "source": [
    "Notice that know except for the sex and vital status categories, the NA category nan from numpy is the most common category across the information on the patients. This is going to complicate analysis using this clinical phenotypical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1a3fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save it on the data folder for later uses.\n",
    "clinical_df.to_csv(path.join('data', 'clinical_df.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47fa1bb",
   "metadata": {},
   "source": [
    "Now we can download the file with relation of specimens and samples extracted from each donor. It is just a file that helps connect by IDs other files, so let's have a quick look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea61e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It is a tabular separated file, so we read with the read_csv function specifying the tabulator \\t as the separator character\n",
    "## Moreover, we indicate that the file has a header that should be inferred as the column names.\n",
    "sample_df = pd.read_csv('https://dcc.icgc.org/api/v1/download?fn=/PCAWG/donors_and_biospecimens/pcawg_sample_sheet.tsv', sep='\\t', header='infer')\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5098d09",
   "metadata": {},
   "source": [
    "Note that for the same donor, several specimens are extracted. Usually, one from a normal tissue and another from a primary tumor (to find mutations through WGS it is necessary to remove germline mutations that are present on both normal and tumoral tissue, that is why the mutations found on normal tissues are usually substracted from the tumor mutation calls).\n",
    "\n",
    "Moreover, notice that from the same tumoral specimen several samples might be extracted to extract information with different techniques: in this case for WGS or for RNA-seq. This will be relevant to map multiomic information across samples from the same tumoral specimen from a given donor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8590e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge it with a file provided in the data folder to extract the primary site\n",
    "project_info = pd.read_csv(path.join('data', 'projects_PCAWG_info.txt'), sep='\\t', header='infer')\n",
    "\n",
    "primary_location_dict = dict(zip(project_info.project, project_info.primary_location))\n",
    "\n",
    "sample_df['primary_location'] = sample_df['dcc_project_code'].map(primary_location_dict)\n",
    "\n",
    "# Save it on the data folder for later uses\n",
    "sample_df.to_csv(path.join('data', 'sample_df.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416cbc4",
   "metadata": {},
   "source": [
    "Next, the expression data is downloaded. This **is a very large file and using pandas function will take time**. As a simple alternative, you can download it directly with your browser **into the *data* folder where this jupyter notebook is located**: https://dcc.icgc.org/api/v1/download?fn=/PCAWG/transcriptome/transcript_expression/pcawg.rnaseq.transcript.expr.tpm.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d44eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the expression data\n",
    "expression_df = pd.read_csv(path.join('data', 'pcawg.rnaseq.transcript.expr.tpm.tsv.gz'), sep='\\t', header='infer', compression='gzip')\n",
    "expression_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12ee5d6",
   "metadata": {},
   "source": [
    "Here the first column shows a the Ensembl Transcript ID and the rest of the columns, whose name is the aliquot ID (present at the **pcawg_sample_sheet.tsv** file).\n",
    "\n",
    "If we want to add gene IDs or Symbols instead of transcript IDs, we will need to process the file that the PCAWG consortium uses for annotation, located at: https://dcc.icgc.org/api/v1/download?fn=/PCAWG/drivers/expression/rnaseq.gc19_extNc.gtf.tar.gz\n",
    "\n",
    "Another alternative than using the browser is to use a bash script. In Jupyter notebooks, you can use other interpreters rather than python. The script below is able to download and process the necessary file (If you don't have a linux-based operative system, skip this step. The file is already provided in the data folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5650e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# The %% above indicates to the jupyter notebook to use bash as interpreter\n",
    "\n",
    "# Change into the data directory\n",
    "cd data\n",
    "\n",
    "# Download the file from the ICGC\n",
    "wget -O rnaseq.gc19_extNc.gtf.tar.gz https://dcc.icgc.org/api/v1/download?fn=/PCAWG/drivers/expression/rnaseq.gc19_extNc.gtf.tar.gz\n",
    "\n",
    "# Process the file using a bash code\n",
    "zcat rnaseq.gc19_extNc.gtf.tar.gz | cut -f9 | cut -d';' -f2 | sed 's/.*gencode::\\([^:]*\\)::tc_\\([^._]*\\)[^:]*::\\([^._]*\\)[^:]*.*/\\1\\t\\2\\t\\3/' | sort | uniq | tail -n +3 | gzip > gencode_transcript.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cfa031",
   "metadata": {},
   "source": [
    "Summarizing until here we have:\n",
    "\n",
    "**pcawg.rnaseq.transcript.expr.tpm.tsv.gz**: A large file with the first column being the ensembl Transcript ID and the rest of the columns with an aliquot ID.\n",
    "\n",
    "**sample_df.tsv**: A file that contains the relationship between donors, specimens and samples. The aliquotID is also a column of this file.\n",
    "\n",
    "**gencode_transcript.tsv.gz**: A file that contains the transcript information.\n",
    "\n",
    "For the analysis at the following sessions we will need to process the expression data, specifically:\n",
    "\n",
    "- Get the information on a gene, instead than on a transcript level.\n",
    "\n",
    "- Get only the expression for tumoral specimens (and that will not cover all the tumoral specimens of the PCAWG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4da42517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open expression matrix\n",
    "expression_matrix = pd.read_csv(path.join('data', 'pcawg.rnaseq.transcript.expr.tpm.tsv.gz'),\n",
    "                                                                    sep=\"\\t\", header='infer', compression='gzip')\n",
    "\n",
    "expression_matrix['Transcript'] = expression_matrix['Transcript'].str.extract('^(\\w+)\\.\\w+$')\n",
    "\n",
    "expression_matrix = expression_matrix.set_index('Transcript', drop=True)\n",
    "\n",
    "\n",
    "# Specimen information PCAWG\n",
    "sample_df = pd.read_csv(path.join('data', 'sample_df.tsv'), sep=\"\\t\", header='infer')\n",
    "\n",
    "# Get an aliquot to specimen ID dictionary\n",
    "specimen_dict = dict(zip(sample_df.aliquot_id, sample_df.icgc_specimen_id))\n",
    "\n",
    "\n",
    "# Let's translate the columns into the specimen ID\n",
    "translated_columns = []\n",
    "aliq_ID_not_found_on_files = []\n",
    "for aliqID in expression_matrix.columns:\n",
    "    try:\n",
    "        translated_columns.append(specimen_dict[aliqID])\n",
    "    except:\n",
    "        aliq_ID_not_found_on_files.append(aliqID)\n",
    "        \n",
    "print('Total number of aliquots with expression data: ' + str(len(expression_matrix.columns)))\n",
    "print('Aliquot that could be translated into specimenID: ' + str(len(translated_columns)))\n",
    "print('Dropped samples because of unknown translation of IDs: ' + str(len(aliq_ID_not_found_on_files)))\n",
    "\n",
    "# Extract the columns\n",
    "print(expression_matrix.shape[1])\n",
    "expression_matrix = expression_matrix.drop(aliq_ID_not_found_on_files, axis=1)\n",
    "print(expression_matrix.shape[1])\n",
    "\n",
    "expression_matrix.columns = translated_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0082d4d0",
   "metadata": {},
   "source": [
    "Apparently all the aliquotIDs can be translated into SpecimenIDs thanks to the **sample_df.tsv**, so there was no lost of information. However, how many of these specimens that were RNA-Sequenced are from tumoral samples? We do not want on the next analysis to include non-tumoral tissues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aca3a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rom the Specimen IDs that we could obtain using the RNA-Seq, library strategy, are all specimen types from tumoral samples?\n",
    "category_series = sample_df[(sample_df['icgc_specimen_id'].isin(translated_columns))&(sample_df['library_strategy']=='RNA-Seq')]['dcc_specimen_type'].value_counts()\n",
    "category_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a5fb4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a pie plot for the different categories\n",
    "category_series.plot.pie(autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "# Add a title\n",
    "plt.title('Distribution of Specimen Types for RNA-Seq')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a20af",
   "metadata": {},
   "source": [
    "Most of the expression data come from specimens of primary solid tumors. Other specimens are from lymph nodes or blood (not solid primary tumors) or even metastasis. However, a non-negligible proportion comes from eaither Normal tissue adjacent to the primary tumor or just regular healthy tissues. Hence we need to remove them from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f069269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get specimens that do not come form normal healthy tissues (do not contain the normal word)\n",
    "clean_sample_df = sample_df[(sample_df['icgc_specimen_id'].isin(translated_columns))&(sample_df['library_strategy']=='RNA-Seq')&(~sample_df['dcc_specimen_type'].str.startswith('Normal'))].copy()\n",
    "# Check that no healthy tissue derived specimens remain\n",
    "print(clean_sample_df['dcc_specimen_type'].value_counts())\n",
    "\n",
    "# Remove the columns on the expression_df with expression for healthy tissue specimens\n",
    "print('Original specimens with expression: ' + str(len(expression_matrix.columns)))\n",
    "print('Specimens that belong to a tumoral tissue: ' + str(len(clean_sample_df['icgc_specimen_id'])))\n",
    "expression_matrix = expression_matrix[clean_sample_df['icgc_specimen_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c50c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c50da",
   "metadata": {},
   "source": [
    "Finally, we need to process the matrix to get expression information at the gene level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff4ba797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The annotation file does not have a header, so the column names are specified\n",
    "annotation_df = pd.read_csv(path.join('data', 'gencode_transcript.tsv.gz'), \n",
    "                                sep=\"\\t\", header=None, names=['Symbol', 'Gene', 'Transcript'], compression='gzip')\n",
    "annotation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adf148be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To group the transcripts and sum their expression by gene IDs we have to do the following steps\n",
    "\n",
    "## Merge the expression_matrix with annotation_df on the 'Transcript' column.\n",
    "## An Inner join is done to work with the Transcript IDs that are on both dataframes\n",
    "merged_df = pd.merge(expression_matrix.reset_index(), annotation_df , left_on='Transcript', right_on='Transcript', how='inner')\n",
    "print(\"Expression available for \" + str(len(merged_df)) + \" transcripts.\")\n",
    "\n",
    "## Group by 'Gene' and sum the values for each gene\n",
    "collapsed_df = merged_df.groupby('Gene').sum()\n",
    "\n",
    "## Drop unnecessary columns\n",
    "collapsed_df = collapsed_df.drop(columns=['Transcript', 'Symbol']).reset_index()\n",
    "\n",
    "print(\"After merging, expression for \" + str(len(collapsed_df)) + \" genes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b34003fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it on the data folder for later uses.\n",
    "collapsed_df.to_csv(path.join('data' , 'gene_expression.tsv.gz'), sep='\\t', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb131e",
   "metadata": {},
   "source": [
    "Finally, we can download the signature number of attributed mutations for each specimen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58ecb0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "signatures_df = pd.read_csv('https://dcc.icgc.org/api/v1/download?fn=/PCAWG/mutational_signatures/Signatures_in_Samples/SA_Signatures_in_Samples/SA_Full_PCAWG_Attributions/SA_COMPOSITE_SNV.activity.FULL_SET.031918.txt', sep='\\t', header='infer')\n",
    "signatures_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc9ca8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the tumor mutation burden to do some exploratory analysis\n",
    "TMB_proxy = signatures_df.iloc[:, 1:].sum(axis=0)\n",
    "TMB_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fecf7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first column to extract SBS code\n",
    "signatures_df['Unnamed: 0'] = signatures_df['Unnamed: 0'].str.extract(r'_(SBS\\w+)_')\n",
    "\n",
    "# Change column names: the first is signature and the rest are the specimenID\n",
    "signatures_df.columns = ['signature'] + [col.split('__')[-1] for col in signatures_df.columns[1:]]\n",
    "\n",
    "# Save the information for later uses\n",
    "signatures_df.to_csv(path.join('data' , 'signatures.tsv.gz'), sep='\\t', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95584bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, going back to the TMB value\n",
    "specimen_IDs = [col.split('__')[-1] for col in TMB_proxy.index]\n",
    "Histological_type = [col.split('__')[0] for col in TMB_proxy.index]\n",
    "\n",
    "# Generate de novo pandas dataframe with the info\n",
    "TMB_df = pd.DataFrame({'specimenID': specimen_IDs, 'hist_type': Histological_type, 'TMB_proxy': TMB_proxy.values})\n",
    "TMB_df\n",
    "\n",
    "TMB_df.to_csv(path.join('data' , 'TMB.tsv.gz'), sep='\\t', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b244de7",
   "metadata": {},
   "source": [
    "It might be interesting to explore the data with a plot. For that we will generate a plot similar to the one showed when the case of use was introduced: a complex plot with two panels, one showing the distribution of total number of mutations for each histological class in logarithmic scale and one showing the proportion of attribution of mutations to the different signatures, across samples throughout different histological classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d03e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First set the signature name as the index (row name)\n",
    "signatures_df = signatures_df.set_index('signature')\n",
    "\n",
    "# Normalize the values in each column to generate the proportions of each signature\n",
    "signatures_df = signatures_df.div(signatures_df.sum(axis=0), axis=1)\n",
    "\n",
    "# Traspose and reorganize index to have as columns (independent variables) each signature\n",
    "signatures_df = signatures_df.transpose().reset_index()\n",
    "\n",
    "# Some signatures that were extracted at the start of the cancer genomics field were subdivided into more components\n",
    "# (7 was subdivided into 7a, 7b and 7c while 17 into 17a and 17b). To simplify we will merge into one component.\n",
    "# Create new columns by summing the specified columns\n",
    "signatures_df['SBS7a'] = signatures_df[['SBS7a', 'SBS7b', 'SBS7c']].sum(axis=1)\n",
    "signatures_df['SBS17a'] = signatures_df[['SBS17a', 'SBS17b']].sum(axis=1)\n",
    "# Rename the columns ('index' column to 'specimenID' and the others)\n",
    "signatures_df = signatures_df.rename(columns={'index': 'specimenID', \n",
    "                                              'SBS7a': 'SBS7', \n",
    "                                              'SBS17a': 'SBS17',\n",
    "                                              'SBS10a': 'SBS10'})\n",
    "# Drop the original columns\n",
    "signatures_df = signatures_df.drop(['SBS7b', 'SBS7c', 'SBS17b'], axis=1)\n",
    "\n",
    "# Drop signatures with no contribution across specimens \n",
    "sum_over = signatures_df[signatures_df.columns[1:]].sum(axis=0)\n",
    "signatures_df = signatures_df.drop(columns=list(sum_over[sum_over==0].index))\n",
    "\n",
    "# Convert TMB_proxy to logarithmic scale\n",
    "TMB_df['log_TMB_proxy'] = np.log10(TMB_df['TMB_proxy'])\n",
    "\n",
    "# Include the total number of elements in the hist_type label for the plots\n",
    "TMB_df['hist_type'] = TMB_df['hist_type'] + ' (n=' + TMB_df.groupby('hist_type').transform('count')['specimenID'].astype(str) + ')'\n",
    "\n",
    "# Merge the two dataframes\n",
    "merged_df = pd.merge(signatures_df, TMB_df , left_on='specimenID', right_on='specimenID', how='inner')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "822b57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the plotting order of hist_type by increasing median in log TMB\n",
    "order = TMB_df.groupby('hist_type')['log_TMB_proxy'].median().sort_values().index\n",
    "\n",
    "# Create a figure and axes\n",
    "plt.figure(figsize=(10, 40))\n",
    "\n",
    "# Create a violin plot with a boxplot inside\n",
    "ax = sns.violinplot(y='hist_type', x='log_TMB_proxy', data=merged_df, order=order, inner='box')\n",
    "\n",
    "# Set X-axis label\n",
    "ax.set_xlabel('log(TMB_proxy)')\n",
    "\n",
    "# Set Y-axis label\n",
    "ax.set_ylabel('Hist Type')\n",
    "\n",
    "# Save the figure for future uses\n",
    "plt.savefig(path.join('plots', 'Violin.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723388e2",
   "metadata": {},
   "source": [
    "Definetly, different tumor types from the histological point of view show different levels of mutations, although there is a large variability within each histological type. For instance, it will be very difficult to distinguish by the number of mutations a sample of a bone benign tumor or a myelodisplasic syndrome type of blood cancer. But what about the composition of these mutations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df3c927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Get only relevant columns\n",
    "prop_df = merged_df[merged_df.columns[:-2]].set_index('specimenID')\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=len(order), ncols=1, figsize=(5, 5 * len(order)))\n",
    "\n",
    "# Iterate over hist_type\n",
    "for i, hist_type in enumerate(order):    \n",
    "\n",
    "    # Plot the stacked bar plot on the right side\n",
    "    ax_bar = axes[i]\n",
    "    sub_prop_df = prop_df[prop_df['hist_type']==hist_type].copy()\n",
    "    sub_prop_df = sub_prop_df.drop(columns=['hist_type'])\n",
    "\n",
    "    # Step 1: Drop signatures that do not contribute to the class or less than 1% mean across samples\n",
    "    mean_over = sub_prop_df[sub_prop_df.columns].mean(axis=0)\n",
    "    sub_prop_df = sub_prop_df.drop(columns=list(mean_over[mean_over<0.01].index))\n",
    "\n",
    "    # Step 2: Identify and sort columns by contribution\n",
    "    contribution_columns = sub_prop_df.iloc[:, :-1].sum().sort_values(ascending=False).index\n",
    "\n",
    "    # Step 3: Sort rows (specimens) by total contribution of selected SBS columns\n",
    "    sorted_specimens = sub_prop_df.sort_values(by=list(contribution_columns), ascending=False)\n",
    "\n",
    "    # Automatically generate a ListedColormap with unique colors based on the number of labels\n",
    "    num_colors = len(contribution_columns)\n",
    "    color_map = plt.get_cmap('tab20', num_colors)\n",
    "\n",
    "    # Step 4: Plot the stacked bar plot\n",
    "    stacked_bar = sorted_specimens.plot(kind='bar', stacked=True, colormap=color_map, edgecolor='none', width=1, ax=ax_bar)\n",
    "    ax_bar.set_xlabel('Specimen')\n",
    "    ax_bar.set_ylabel('Contribution')\n",
    "    # Remove X-axis tick labels\n",
    "    ax_bar.set_xticklabels([])  \n",
    "    ax_bar.set_title(f'Stacked Bar Plot - {hist_type}')\n",
    "    \n",
    "    # Move the legend to the right\n",
    "    ax_bar.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Save the figure for future uses\n",
    "plt.savefig(path.join('plots', 'Barplot_signatures.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee85def",
   "metadata": {},
   "source": [
    "There is clearly larger differences in term of composition of the mutations which might help with the identification of tumor types just by using the proportions of signatures on a given sample, although there is still high variability. This might help with the decision of the type of data to use if we consider to build a model that looks on genomic data and wants to identify the histological (or even molecular subtype) of tumor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a995217e",
   "metadata": {},
   "source": [
    "# Unsupervised algorithms\n",
    "\n",
    "**In this practical session you will:**\n",
    "\n",
    "   - Learn to use several common unsupervised methods (dimensionality reduction and clustering algotithms) used in multi-omics data analysis.\n",
    "   - Explore part of the multi-omics dataset and discover the underlying structure of the trasncriptomic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b874ef3",
   "metadata": {},
   "source": [
    "## Dimensionality reduction methods\n",
    "\n",
    "Dimensionality reduction algorithms are techniques used to reduce the number of features (or dimensions) in a dataset while preserving its essential information: this is particularly useful for **visualization, meaningful compression and discovery of the underlying structure of the data**. Two popular dimensionality reduction algorithms are Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a91a90",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a statistical technique that on an n-dimensional matrix of values that:\n",
    "\n",
    "- Identifies the directions (specific axis in the matrix) at which, if the rest of the data is projected into, the data varies the most: the principal components.\n",
    "\n",
    "- Represents the data in a new coordinate system defined by these principal components.\n",
    "    \n",
    "Therefore, the key idea is to find a lower-dimensional representation of the data that captures the maximum amount of variance. Hence, the first principal component is the one that captures the most significant amount of variance in the data, followed by the second principal component, and so on.\n",
    "\n",
    "To achieve this, the algorithm follows these steps:\n",
    "\n",
    "1. PCA starts by computing the **covariance matrix** of the original data, which represents the relationships between the different features.\n",
    "\n",
    "2. **Eigenvectors and eigenvalues** are extracted from the covariance matrix. The **eigenvectors** are the principal components and the **eigenvalues** indicate the variance along each principal component.\n",
    "\n",
    "3. The **eigenvectors** are sorted in descending order based on the **eigenvalues**.\n",
    "\n",
    "4. The dimensionality of the data is reduced and the data is transformed **linearly** into a new coordinate system aligned with the an amount of first principal components depending on the new dimensionality.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![PCA in a nutshell](https://pbs.twimg.com/media/F9XIOm1boAEhsL2?format=jpg&name=small)](https://twitter.com/akshay_pachaar/status/1717519050706952695)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "PCA is probably the most used dimensionality reduction technique thanks to its multiple advantatges, although it also has its own problems:\n",
    "\n",
    "**Advantages:**\n",
    "- Computationally efficient for linear dimensionality reduction.\n",
    "- Preserves as much variance as possible.\n",
    "- Clear interpretation of the principal components.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Assumes linearity.\n",
    "- May not capture complex nonlinear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb6024",
   "metadata": {},
   "source": [
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "t-SNE is another common dimensionality reduction algorithm primarily used for visualizing high-dimensional data in a lower-dimensional space. However, unlike linear methods like PCA, t-SNE focuses on preserving local structures and capturing non-linear relationships between data points.\n",
    "\n",
    "To this effect, the algorithm uses:\n",
    "\n",
    "- **Measures of pairwise similarity** between data points since similar data points in the high-dimensional space are intended to remain close to each other in the low-dimensional space.\n",
    "\n",
    "- Moreover, t-SNE constructs **probability distributions** for the pairwise similarities in both the high-dimensional and low-dimensional spaces to model the similarities using conditional probabilities.\n",
    "\n",
    "- The algorithm minimizes the **divergence between the probability distributions** in the high-dimensional and low-dimensional spaces low-dimensional representation to reflect the structure of the high-dimensional data.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Following this criteria and statistical tools, the steps of the algorithm are:\n",
    "\n",
    "1. **Compute Pairwise Similarities:** For each pair of data points in the high-dimensional space the pairwise similarity is computed.\n",
    "\n",
    "2. **Construct Probability Distributions:** The pairwise similarities are converted into probability distributions. In the high-dimensional space a Gaussian distribution is used to represent the similarities while in the low-dimensional space is a Student's t-distribution (this distribution has heavier tails compared to the Gaussian making it more flexible and better suited for capturing local structures).\n",
    "\n",
    "3. **Minimize the Divergence:** The Kullback-Leibler divergence is minimized between the two sets of probability distributions by iteratively adjusting the positions of data points in the low-dimensional space. Once the minimum is achieved, the final output is the resulting low-dimensional embedding of the data.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "The advantatges and disadvantatges of t-SNE remark the complementarity of this technique to PCA:\n",
    "\n",
    "**Advantages:**\n",
    "- Effective for preserving local structure and capturing non-linear relationships.\n",
    "- Well-suited for visualization of high-dimensional data.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Computationally expensive for large datasets.\n",
    "- Optimizing t-SNE involves non-convex optimization, which may result in different solutions for different initializations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7318ffdd",
   "metadata": {},
   "source": [
    "### PCA or t-SNE?\n",
    "\n",
    "The different characteristics of these techniques is key to choose the appropiate one based on the nature of the data and the problem at hand: PCA is often preferred for linear relationships and dimensionality reduction, while t-SNE is powerful for visualizing complex, non-linear structures in high-dimensional data.\n",
    "\n",
    "**Linearity vs. Non-Linearity:**\n",
    "- PCA: Assumes linear relationships.\n",
    "- t-SNE: Captures non-linear relationships.\n",
    "\n",
    "**Preservation of Global vs. Local Structure:**\n",
    "- PCA: Emphasizes preserving global variance.\n",
    "- t-SNE: Focuses on preserving local structures and similarities.\n",
    "\n",
    "**Interpretability:**\n",
    "- PCA: Principal components have a clear interpretation.\n",
    "- t-SNE: The mapping is more difficult to interpret, especially for distances in the high-dimensional space.\n",
    "\n",
    "**Computational Complexity:**\n",
    "- PCA: Computationally efficient.\n",
    "- t-SNE: Computationally expensive, especially for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9de3de",
   "metadata": {},
   "source": [
    "## Clustering algorithms\n",
    "\n",
    "Clustering algorithms are used to group data points together into clusters based on their relation to surrounding data points. Hence, they use similarity or distance measures in the feature space in an effort to discover dense regions of data points (hence, it is good practice to scale data prior to using clustering algorithms).\n",
    "\n",
    "There are many types of clustering algorithms but they have in common an iterative process identified clusters are evaluated and reported back to the algorithm configuration until the desired or appropriate number of clusters is achieved.\n",
    "\n",
    "Therefore, some clustering algorithms require the user to specify the number of clusters to discover in the data while others require only some minimum distance between observations, a theshold at which data points might be considered as \"close\" or \"connected\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa417e85",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering generates a tree-like hierarchy of clusters known as a dendrogram through the iterative process. It does not require to specify the number of clusters beforehand but the user should subjectively define a posteriori the amount of clusters based on the dendogram. The iterative steps are the following:\n",
    "\n",
    "1. **Evaluate the distance between clusters** The algorithm computes the pairwise distance between all the clusters at the iteration (the algorithm starts by considering each data point as an individual cluster and ends when all data points are assigned to one cluster).\n",
    "\n",
    "2. **Merge the closest clusters**: The two clusters with the lowest distance between them are merged toghether into a new cluster. This distance is recorded on the dendogram as the length of the branch between the original two clusters and the new cluster (a new node in the dendogram). The algorith enters into the next iteration.\n",
    "\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Hierarchical clustering](https://cdn-dfnaj.nitrocdn.com/xxeFXDnBIOflfPsgwjDLywIQwPChAOzV/assets/images/optimized/rev-8c385a7/www.displayr.com/wp-content/uploads/2018/03/Hierarchical-clustering-3-1.png)](https://www.displayr.com/what-is-hierarchical-clustering/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Once the dendogram is generated, the shape could be interpreted to define the amount of desired clusters. Together with visual inspection and several performance metrics, such as the **Cophenetic Correlation Coefficient** that measures how faithfully the hierarchical clustering preserves pairwise distances between data points (close to 1 indicates good clustering) or the **Ward's method** (see below), it allows to evaluate how succesful the clustering has been.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Dendogram](https://cdn-dfnaj.nitrocdn.com/xxeFXDnBIOflfPsgwjDLywIQwPChAOzV/assets/images/optimized/rev-8c385a7/www.displayr.com/wp-content/uploads/2018/03/Screen-Shot-2018-03-28-at-11.48.48-am.png)](https://www.displayr.com/what-is-hierarchical-clustering/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "There are various linkage methods, that is, methods to measure the distance between clusters, where each one of them has the advantatge to proficiently detect specific shapes of clusters or the disadvantatge of be misguided by data with a different nature. Some examples are:\n",
    "\n",
    "- **Single linkage**: The distance is computed as the closest between two points such that one point lies in one cluster and the other point lies in the other. This method is able to separate non-elliptical shapes as long as the gap between the two clusters is not small, however, it has bad performance when there is noise between clusters.\n",
    "\n",
    "- **Complete linkage**: The distance is computed as the furthest between two points such that one point lies in one cluster and the other point lies in the other. In contrast, this method has a good performance when there is noise between clusters but is biased towards detecting globular clusters and tends to disgregate the large clusters.\n",
    "\n",
    "- **Average linkage**: The distance is computed as the average between all possible pairs of data points between clusters. Similar to the complete linkage, has a good performance with noise between clusters but is biased towards globular ones.\n",
    "\n",
    "- **Ward's method**: It is similar to the average linkage, but the average is computed over the sum of the square of pair-wise distances. The ward's method also serves as a performance metric where low values within each cluster suggest better performance.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Likage methods](https://miro.medium.com/v2/resize:fit:640/format:webp/0*s2KrCgCQIlEqcK_X)](https://medium.com/@u0808b100/unsupervised-learning-in-python-hierarchical-clustering-t-sne-41f17bbbd350)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "With respect to other clustering algotithms, the hierarchical clustering presents:\n",
    "\n",
    "**Advantages:**\n",
    "- No need to pre-specify the number of clusters.\n",
    "- Provides a hierarchical structure.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Computationally expensive, especially for large datasets.\n",
    "- Difficult to determine the optimal number of clusters (highly subjective)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b706bac",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "K-Means clustering partitions the data into a predefined k number of clusters, where each cluster is defined by a centroid: a data point calculated as the mean of all the data points in the cluster. There are different algorithms, but all of them use an iterative procedure until a convergence solution is achieved. Roughly, they follow these two steps:\n",
    "\n",
    "1. **Assignment**: Each data point is assigned to the nearest centroid, generating K clusters at the current iteration. At the first iteration, the k initial cluster centroids are choosen at random in the space.\n",
    "\n",
    "2. **Update the centroids**: The k-centroids are recalculated based on the mean of the data points in each cluster. If after updating several times the data points on each cluster remain the same after assigment, the centroids remain the same after the update: convergence has been achieved and the algorithm stops.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "<a href=\"https://stackoverflow.com/questions/60312401/when-using-the-k-means-clustering-algorithm-is-it-possible-to-have-a-set-of-dat\" target=\"_blank\">\n",
    "  <img src=\"https://i.stack.imgur.com/ibYKU.png\" alt=\"K-means clustering\" width=\"800\"/>\n",
    "</a>\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Similar to hierarchical clustering, there are some metrics that reflect performance such as the **silhouette score**, which evaluates the intra-cluster compactness and between clusters separation or the **Ward's method**. Despite being unsupervised methods, if there is any information about the \"true\" clusters in the data, one can compute the **Adjusted Rand Index (ARI)** which measures the similarity between true and predicted clusters, adjusted for chance.\n",
    "\n",
    "With respect to advantages and disadvantages compared to other clustering methods:\n",
    "\n",
    "**Advantages**:\n",
    "- Efficient and works well with large datasets.\n",
    "- Simple and easy to implement.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "**Disadvantages**:\n",
    "- Sensitive to initial centroid placement.\n",
    "- Assumes clusters are spherical and equally sized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf1479e",
   "metadata": {},
   "source": [
    "### Hierarchical o K-Means Clustering?\n",
    "\n",
    "As always, depends on the nature of the data and the goals of the analysis. Hierarchical does not require to specify the initial number of clusters and decision can be done a posteriori evaluating the hierarchy, however it is computationally expensive. K-means clustering is more efficient, but requires a predefined expected number of clusters and is very sensitive to initializations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce5b1df",
   "metadata": {},
   "source": [
    "## Practical session: visualization of transcriptomes and identification of cancer types by expression\n",
    "\n",
    "The expression data across genes and specimens that we generated in the previous session is highly multidimensional, given that for each specimen we have the expression across more than 20000 thousand genes or features.\n",
    "\n",
    "In order to make any sense of this data, we can start by employing techniques such as PCA or t-SNE to preliminary investigate for interesting patterns in the data. For this purpose we can use the utilities available on the scikit-learn package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6578fb31",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "From a practical point of view, these are the steps we are going to implement:\n",
    "\n",
    "1. Standardize the data: The objetive of PCA is to maximize the variance. If the features (in this case >20000 gene expressions have different scales or units, it is important to standardize the data by subtracting the mean and dividing by the standard deviation. This step ensures that all features are on a similar scale and prevents dominance by features with larger variances. For that, we will use the StandardScaler of sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "2. Compute the covariance matrix: To understand the relationships between pairs of gene expression in the data.\n",
    "\n",
    "3. Perform the eigen-decomposition: The covariance matrix is decomposed into its eigenvectors and eigenvalues. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "4. Select the principal components: The principal components are ranked based on their corresponding eigenvalues, and the top components capturing the most variance are selected. Since we will do a 2D visualization, we need the two components that explain most of the variance in gene expression across samples.\n",
    "\n",
    "5. Project the data onto the new coordinate system: The original data is transformed by projecting it onto the selected principal components. Each data point is represented by its new coordinates in the principal component space.\n",
    "\n",
    "Steps 2, 3, 4 and 5 could be implemented easily with numpy through linear algebra operations (if anyones wants to, you can try the exercise. If you need help, see https://stackoverflow.com/questions/58666635/implementing-pca-with-numpy) however, this is a standard procedure and already implemented in machine learning packages such as **skicit-learn** as the PCA module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "574a7882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To ignore some plot warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "n_components = 2\n",
    "\n",
    "# We load the expression data\n",
    "expression_df = pd.read_csv(path.join('data', 'gene_expression.tsv.gz'),\n",
    "                                                        sep=\"\\t\", header='infer', index_col=0, compression='gzip')\n",
    "\n",
    "# We preprocess the data with standarization\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(expression_df.T)\n",
    "\n",
    "# We perform the PCA\n",
    "pca2D = PCA(n_components=n_components)\n",
    "proj_data = pca2D.fit_transform(data)\n",
    "\n",
    "# We can check the amount of variance explained by the two Principcal components\n",
    "print(pca2D.explained_variance_ratio_)\n",
    "\n",
    "# We plot the data (the first two components are the first two columns of proj_data)\n",
    "scatter = plt.scatter(proj_data[:,0], proj_data[:,1], s=2)  # Adjust the 's' parameter to control the size\n",
    "plt.title('Transcriptome')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "375760ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can even use the third PC and do a 3D plot\n",
    "n_components = 3\n",
    "# We perform the PCA\n",
    "pca3D = PCA(n_components=n_components)\n",
    "proj_data3D = pca3D.fit_transform(data)\n",
    "\n",
    "print(pca3D.explained_variance_ratio_)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.scatter(proj_data3D[:,0],\n",
    "              proj_data3D[:,1],\n",
    "              proj_data3D[:,2], s=2)\n",
    "ax.set_title(\"Transcriptome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48496fab",
   "metadata": {},
   "source": [
    "A priori we cannot distinguish much, but we could try to colour each sample based on the primary tumor type and see if it is more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6d58991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Get sample dataframe with the information\n",
    "sample_df = pd.read_csv(path.join('data', 'sample_df.tsv'), sep=\"\\t\", header='infer')\n",
    "\n",
    "# Generate a dictionary to translate the Specimen IDs to the tumor type code.\n",
    "tumortype_dict = dict(zip(sample_df.icgc_specimen_id, sample_df.primary_location))\n",
    "\n",
    "# Get the tumor type for each sample (each column)\n",
    "labels = expression_df.columns.map(tumortype_dict)\n",
    "\n",
    "# Automatically create a mapping from label categories to integers\n",
    "unique_labels = np.unique(labels)\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "# Convert labels to integers based on the automatic mapping\n",
    "label_integers = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "# Automatically generate a ListedColormap with unique colors based on the number of labels\n",
    "num_colors = len(unique_labels)\n",
    "color_map = plt.get_cmap('tab20', num_colors)\n",
    "\n",
    "# Create a ListedColormap with unique colors\n",
    "listed_color_map = ListedColormap([color_map(idx) for idx in range(num_colors)])\n",
    "\n",
    "# Plot with colours \n",
    "# We plot the data (the first two components are the first two columns of proj_data)\n",
    "scatter = plt.scatter(proj_data[:,0], proj_data[:,1], s=2, c=label_integers, cmap=listed_color_map)\n",
    "\n",
    "# Create legend handles and labels\n",
    "legend_handles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map(idx), markersize=10, label=label)\n",
    "                  for idx, label in enumerate(unique_labels)]\n",
    "\n",
    "# Add legend\n",
    "plt.legend(handles=legend_handles, title='Tumor Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.title('Transcriptome')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "\n",
    "# Save image\n",
    "plt.savefig(path.join('plots', 'PCA.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fba1208",
   "metadata": {},
   "source": [
    "Apparently the PCA can separate some blood cancer clusters, but has problems to separate other samples from a wide variety of primary tumor locations. Definetly, the implementation of PCA does not solve the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236abc20",
   "metadata": {},
   "source": [
    "### t-SNE\n",
    "\n",
    "t-SNE does not assume linearity and might be a better proxy to separate the different types of tumors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a648605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Transpose the matrix\n",
    "transposed_matrix = expression_df.to_numpy().T\n",
    "\n",
    "# Initialize TSNE with 2 components for 2D visualization)\\\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform the data\n",
    "tsne_result = tsne.fit_transform(transposed_matrix)\n",
    "\n",
    "# Create a DataFrame with the t-SNE results\n",
    "tsne_df = pd.DataFrame(tsne_result, columns=['TSNE1', 'TSNE2'])\n",
    "\n",
    "# Finally, plot the t-SNE results\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(tsne_df['TSNE1'], tsne_df['TSNE2'], s=5, c=label_integers, cmap=listed_color_map)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(handles=legend_handles, title='Tumor Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.title('t-SNE Visualization')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(path.join('plots', 'tSNE.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2f87eb",
   "metadata": {},
   "source": [
    "This non-linear transformation that t-SNE uses for reducing the dimensionality allows for a better differentiation of specific groups, namely the pancreas tumors, prostate, brain and some kidney cancers. Other types of tumors are not so clearly differentiated although they tend to be together. Within primary cancer types there is also variability, for instance multiple types of blood cancers tend to show subclusters (similarly colorectal tumors and others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3290eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a binary category\n",
    "binary_blood = pd.Series(labels).apply(lambda x: 'Blood' if x == 'Blood' else 'Others')\n",
    "\n",
    "# Automatically create a mapping from label categories to integers\n",
    "unique_labels = np.unique(binary_blood)\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "# Convert labels to integers based on the automatic mapping\n",
    "label_integers = np.array([label_mapping[label] for label in binary_blood])\n",
    "\n",
    "color_map = ListedColormap(['red', 'lightgrey',])\n",
    "\n",
    "# Finally, plot the t-SNE results\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(tsne_df['TSNE1'], tsne_df['TSNE2'], s=5, c=label_integers, cmap=color_map)\n",
    "\n",
    "# Create legend handles and labels\n",
    "legend_handles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map(idx), markersize=10, label=label)\n",
    "                  for idx, label in enumerate(unique_labels)]\n",
    "\n",
    "# Add legend\n",
    "plt.legend(handles=legend_handles, title='Tumor Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.title('t-SNE Visualization')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(path.join('plots', 'tSNE_blood.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10c7beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a binary category\n",
    "binary_colorectal = pd.Series(labels).apply(lambda x: 'Colorectal' if x == 'Colorectal' else 'Others')\n",
    "\n",
    "# Automatically create a mapping from label categories to integers\n",
    "unique_labels = np.unique(binary_colorectal)\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "# Convert labels to integers based on the automatic mapping\n",
    "label_integers = np.array([label_mapping[label] for label in binary_colorectal])\n",
    "\n",
    "color_map = ListedColormap(['green', 'lightgrey',])\n",
    "\n",
    "# Finally, plot the t-SNE results\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(tsne_df['TSNE1'], tsne_df['TSNE2'], s=5, c=label_integers, cmap=color_map)\n",
    "\n",
    "# Create legend handles and labels\n",
    "legend_handles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map(idx), markersize=10, label=label)\n",
    "                  for idx, label in enumerate(unique_labels)]\n",
    "\n",
    "# Add legend\n",
    "plt.legend(handles=legend_handles, title='Tumor Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.title('t-SNE Visualization')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(path.join('plots', 'tSNE_colorectal.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2112f60",
   "metadata": {},
   "source": [
    "### Hierarchical clustering\n",
    "\n",
    "A priori we do not have any preliminary information like the primary types, but we can try to extract clusters directly from the t-SNE output. This is not devoid of interpretation problems (https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne), so well-differentiated clusters might not show real biological features that differentiate them.\n",
    "\n",
    "For starters let's assume that we do not know the underlying structure of primary types and try to extract 18 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2686b9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Apply hierarchical clustering\n",
    "linkage_matrix = linkage(tsne_result, method='ward')\n",
    "\n",
    "# Compute cophenetic correlation (Ward's metric)\n",
    "c, coph_dists = cophenet(linkage_matrix, pdist(tsne_result))\n",
    "print(f\"Cophenetic correlation coefficient: {c}\")\n",
    "\n",
    "# Set the number of clusters using maxclust\n",
    "num_clusters = 18\n",
    "\n",
    "# Assign cluster labels based on the maxclust criterion\n",
    "clusters = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
    "\n",
    "# Get the threshold distance used for clustering\n",
    "threshold_distance = linkage_matrix[-(num_clusters - 1), 2]\n",
    "\n",
    "# Compute silhouette score\n",
    "silhouette_avg = silhouette_score(tsne_result, clusters)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "# Plot the dendrogram with a horizontal line at the threshold\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(linkage_matrix, leaf_rotation=90., leaf_font_size=8., color_threshold=num_clusters)\n",
    "plt.xticks([]) # Remove x-axis labels\n",
    "plt.axhline(y=threshold_distance, color='r', linestyle='--', label=f'Max Clusters ({num_clusters})')\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample point')\n",
    "plt.ylabel('Distance')\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(path.join('plots', 'HierarchClust_dendogram.png'))\n",
    "plt.show()\n",
    "\n",
    "# Plot the t-SNE results with cluster colors\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(tsne_df['TSNE1'], tsne_df['TSNE2'], c=clusters, cmap='tab20', s=5)\n",
    "plt.title('t-SNE Visualization with Hierarchical Clustering')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(path.join('plots', 'tSNE_HierarchClust.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87ccb00",
   "metadata": {},
   "source": [
    "The red dashed line on the dendogram marks the threshold of ward's distance that has been used to define 18 clusters. How good is the clustering then?\n",
    "\n",
    "From a strictly geometric point of view, the **Cophenetic Correlation Coefficient (or Ward's method)**, which measures how faithfully the hierarchy preserves the pairwise distances between the original data points, shows a moderate level of fidelity (this metric ranges from -1 to 1, where higher values indicate better preservation). Moreover, the **Silhouette Score**, which measures cohesion of the clusters and also ranges from -1 to 1 with higher values indicating cohesed clusters, reflects a rather mid cohesion. However, these metrics do not have any value unless interpreted under the light of the nature of the data.  \n",
    "\n",
    "Some clusters are clearly defined, and we know that reflect real expression pattern differences due to primary type location and even tumor subtypes (different blood cancers show different clusters that, in reality, reflect different types of tumors). However, on the center of the t-SNE plot there are some clusters that were extracted from groups of points with not so clear distinction between them. Here the expression patterns are not enough different to distinguish clear subdivisions and might not reflect any biological feature (at least it does not reflect the primary type location). We can make this interpretation thanks to external information about tumor type, but as an unsupervised method this is not implemented on its methodology.\n",
    "\n",
    "We can play with the dendogram to define other numbers of clusters (independently of this 18 clusters value we obtain from external information) or use the primary type external information to compute the **Adjusted Rand Index (ARI)** as a proxy of performance of the clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "842c1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import to_hex\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Function to calculate and plot hierarchical clustering\n",
    "def hierarchical_clustering_and_tsne(tsne_result, true_labels, num_clusters_list, method):\n",
    "    \n",
    "    # Get the colours for the threshold\n",
    "    color_map = plt.get_cmap('tab10', len(num_clusters_list))\n",
    "    \n",
    "    # Apply hierarchical clustering\n",
    "    linkage_matrix = linkage(tsne_result, method=method)\n",
    "    \n",
    "    # Plot dendrogram and t-SNE for different numbers of clusters\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    dendrogram(linkage_matrix, leaf_rotation=90., leaf_font_size=8.)\n",
    "    plt.title(f'Hierarchical Clustering Dendrogram')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Distance')\n",
    "    \n",
    "    all_clusters = list()\n",
    "    for i, num_clusters in enumerate(num_clusters_list):\n",
    "        \n",
    "        # Assign cluster labels based on the maxclust criterion\n",
    "        clusters = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
    "        all_clusters.append(clusters)\n",
    "    \n",
    "        # Get the threshold distance used for clustering\n",
    "        threshold_distance = linkage_matrix[-(num_clusters - 1), 2]\n",
    "        plt.axhline(y=threshold_distance, color=to_hex(color_map.colors[i]), linestyle='--', label=f'Max Clusters ({num_clusters})')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    for i, clusters in enumerate(all_clusters):\n",
    "\n",
    "        # Plot the t-SNE results with cluster colors\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(tsne_df['TSNE1'], tsne_df['TSNE2'], c=clusters, cmap='tab20', s=5)\n",
    "        plt.title(f't-SNE Visualization (Clusters={num_clusters_list[i]})', color=to_hex(color_map.colors[i]))\n",
    "        plt.xlabel('t-SNE Component 1')\n",
    "        plt.ylabel('t-SNE Component 2')\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate Adjusted Rand Index (ARI)\n",
    "        ari = adjusted_rand_score(true_labels, clusters)\n",
    "        print(f'Adjusted Rand Index (ARI) for Clusters={num_clusters_list[i]}: {ari:.4f}\\n')\n",
    "\n",
    "\n",
    "# List of different numbers of clusters to try\n",
    "num_clusters_list = [4, 7, 10, 14, 18]\n",
    "\n",
    "# Also we can try different methods by changing this parameter\n",
    "method = 'ward' \n",
    "\n",
    "# Call the function to perform hierarchical clustering and t-SNE for each number of clusters\n",
    "hierarchical_clustering_and_tsne(tsne_result, label_integers, num_clusters_list, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d879eb",
   "metadata": {},
   "source": [
    "As shown in the plot, the Adjusted Rand Index is higher using 14 clusters, less than the 18. Note on the code that it is possible to change the linkage metric to another different than the **ward's method**. Feel free to experiment with other methodologies such as the **minimum** (also named single method), the **maximum** (or complete), ... The different values that the method parameter can take for the linkage function are collected in the scipy documentation (https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb093a3",
   "metadata": {},
   "source": [
    "### K-means clustering\n",
    "\n",
    "Similar to hierarchical clustering, we can use k-means clustering to extract plots from the t-SNE results using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e88ae384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Function to calculate and plot k-means clustering\n",
    "def kmeans_clustering_and_tsne(tsne_result, true_labels, num_clusters_list, random_seed=42):\n",
    "\n",
    "    all_clusters = list()\n",
    "    for num_clusters in num_clusters_list:\n",
    "        \n",
    "        # Apply k-means clustering\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=random_seed, n_init='auto')\n",
    "        clusters = kmeans.fit_predict(tsne_result)\n",
    "        all_clusters.append(clusters)\n",
    "    \n",
    "        # Plot the t-SNE results with cluster colors\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(tsne_result['TSNE1'], tsne_result['TSNE2'], c=clusters, cmap='tab20', s=5)\n",
    "        plt.title(f't-SNE Visualization (Clusters={num_clusters})')\n",
    "        plt.xlabel('t-SNE Component 1')\n",
    "        plt.ylabel('t-SNE Component 2')\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate Adjusted Rand Index (ARI)\n",
    "        ari = adjusted_rand_score(true_labels, clusters)\n",
    "        print(f'Adjusted Rand Index (ARI) for Clusters={num_clusters}: {ari:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e48b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of different numbers of clusters to try\n",
    "num_clusters_list = [4, 7, 10, 14, 18]\n",
    "\n",
    "# Change the random seed to see how much the algorithm depends on initialization conditions\n",
    "random_seed = 12\n",
    "\n",
    "# Call the function to perform k-means clustering and t-SNE for each number of clusters\n",
    "kmeans_clustering_and_tsne(tsne_result, label_integers, num_clusters_list, random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f2ef63",
   "metadata": {},
   "source": [
    "Using the code above, you can try to explore how the **Adjusted Rand Index (ARI)** (using the primary cancer types as proxy of \"true\" groups to observe) changes with different number of clusters for the **K-means clustering** algorithm. How different it is compared to the hierarchical clustering?\n",
    "\n",
    "You can also try different seeds to initialize the clustering algorithm at random. You will notice how dependent on the initial random conditions the algorithm is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c48ec48",
   "metadata": {},
   "source": [
    "# Basic supervised machine learning methods\n",
    "\n",
    "As already explained, supervised methods rely on datasets whose expected ouptut is previously known to train models. Therefore, this kind of machine learning methods rely on several datasets (or partitions of the same dataset) for the development, fine-tuning and evaluation of models:\n",
    "\n",
    "- **Training dataset:** The initial portion of the available data used to train the model, when the model learns patterns, relationships, and features present in the data. This is acomplished through the machine learning algorithm by optimizing the internal parameters of the model through minimization of the difference between the predicted outputs and the expected output (the labels).\n",
    "\n",
    "- **Validation dataset:** A separate portion of the data that is not used during the training phase. It serves as an independent set to assess the model's performance during development and hyperparameter tuning, allowing an adjustment to prevent overfitting towards the training data. This dataset and the validation step might be skipped in some cases or substituted by **cross-validation** techniques (see below).\n",
    "\n",
    "- **Testing dataset:** The testing dataset is another independent portion of the data that is kept completely separate and untouched during both training and validation. It is used to evaluate the final performance of the model after the training and validation phases, providing an unbiased estimate of its ability to generalize to new, unseen data. It helps assess whether the model has learned meaningful patterns after both training and validation instead of simply memorizing specific data.\n",
    "\n",
    "The process of sividing the original dataset is known as **data splitting**, which tends to favour the training set as the largest sub-dataset and the validation and testing equally smaller. However, the relative sizes can greatly vary between ~60% to ~80% for the training dataset depending on the total size of the dataset and the machine learning method to use. Once the model is fully trained, validated and tested it can be used with new sets of data and generate trustful outputs from which meaningful biological information could be extracted.\n",
    "\n",
    "![Use of data in supervised learning](images/Scheme1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cdc29f",
   "metadata": {},
   "source": [
    "It is also very important the concept of **feature selection** as part of the data processing steps prior to the use of a supervised method. **Feature selection** algorithms try to find combinations feature subsets, along with an evaluation measure to score them, to optimize the training steps. The idea behind performing this step is that the data might contain some **irrelevant or redundant** features and excluding them provides several advantatges:\n",
    "\n",
    "- Simpler models are more easy to manage and interpret.\n",
    "- Simpler models take shorter training times.\n",
    "- Reducing the dimensionality helps to avoid the curse of dimensionality (as the number of dimensions increases, the amount of data we need to produce accurate generalistic models grows exponentially).\n",
    "\n",
    "Since it is virtually impossible (computationally intractable) to test each possible subset of features and find the one that minimizes the error for datasets highly multidimensional, there are several types of algorithms that approach this problem classified by the evaluation metrics:\n",
    "\n",
    "\n",
    "- **Wrapper methods** use a predictive model to score the feature subsets. Each new subset is used to train a model, which is tested on a test set, from which an error rate is obtained as the score for that specific feature subset.\n",
    "\n",
    "\n",
    "    - **Advantatge**: Usually provide the best performing feature set model and problem-wise.\n",
    "    - **Disadvantatge**: Very computationally intensive (train a new model for each subset).\n",
    "    - **Examples**: Recursive Feature Elimination (RFE), Forward Feature Selection (RFS), Backward Feature Elimination (BFE).\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "- **Filter methods** use a statistical metric, a proxy, to score the subsets instead of the error rate. This metric is fast to compute but still able to capture the usefulness of the feature set. Some algorithms also provide a feature ranking rather than the best feature subset, which allows to empirically choose some cutoff threshold through cross-validation techniques (then it is an **Hybrid method**). These methods are also used as a pre-processing step before applying more reliable but computationally expensive Wrapper methods.\n",
    "\n",
    "    - **Disadvantatge**: The feature subset is not model and problem-wise tunned.\n",
    "    - **Advantatge**: Usually less computationally intensive than wrappers. Moreover, it does not contain any predictive model assumnptions, so it is more useful for exposing relationships between the features.\n",
    "    - **Examples**: ANOVA, correlation metrics.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "- **Embedded methods** is a very diverse group of methods that perform the feature selection step as part of the model construction process. The computational complexity is usually between the one of filter and wrapper methods.\n",
    "\n",
    "    - **Examples**: LASSO regression and derivates, random forest.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "- **Hybrid methods** have both characteristic of Wrapper and Filter methods.\n",
    "\n",
    "    - **Examples**: Recursive Feature Elimination with cross-validation (RFECV)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Feature selection methods](https://medium.com/analytics-vidhya/feature-selection-extended-overview-b58f1d524c1c)](https://miro.medium.com/v2/resize:fit:720/format:webp/1*9h2qPmOJonbCdthfeVkuyg.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900fbe23",
   "metadata": {},
   "source": [
    "## Regression Methods\n",
    "\n",
    "Regression methods are used to model the output of a continous variable (the dependent variable) based on one or multiple continous explanatory variables (independent variables). These methods aim to model the relationship between the input features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fed7075",
   "metadata": {},
   "source": [
    "### Linear Regression:\n",
    "\n",
    "Linear regression is a simple and widely used regression method that models the relationship between the dependent variable and one or more independent variables.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Linear regression](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/05/2.3.png)](https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-your-first-machine-learning-model-linear-regression/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "The model assumes:\n",
    "\n",
    "- **Linear relationship** between the independent and dependent variables.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Linear relationship](https://editor.analyticsvidhya.com/uploads/96503linear-nonlinear-relationships.png)](https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-your-first-machine-learning-model-linear-regression/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "- **Normality:** Both dependent and independent variables should be normally distributed. It is important to check for deviations of normality both on skweness or kurtosis.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Normality](https://miro.medium.com/v2/resize:fit:1024/1*OylqZbAZGJJMDiuM1gHUnw.jpeg)](https://medium.com/omics-diary/how-to-test-normality-skewness-and-kurtosis-using-python-18fb2d8e35b9)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "- **Homoskedasticity:** The model assumes that the error term is constant across the explanatory variable. If the variance of the the error term depends on the explanatory variable, the linear regression model will be innapropiate to model the data (since it assumes a constant variability).\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Homoskedasticity](https://editor.analyticsvidhya.com/uploads/51367residuals.png)](https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-your-first-machine-learning-model-linear-regression/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Therefore, linear regression presents:\n",
    "\n",
    "**Advantages:**\n",
    "- Simple and interpretable.\n",
    "- Fast computation.\n",
    "\n",
    "**Disadvantages:**\n",
    "- The model has little no none predictive power beyond assumptions.\n",
    "- Assumes absence of collinearity between independent variables (if the model includes more than one)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f251c",
   "metadata": {},
   "source": [
    "We will try to generate a model that relates some clinical features (phenotype) with a genomic characteristic: the total tumor mutation burden. Mutations, and somatic mutations too, accumulate with time on the body tissues. Tumors arising on older people could show more somatic mutations, and hence, the age of the patient could be used to roughly predict the amount of mutations on their tumors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b7fc494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# To ignore some plot warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the clinical information of patients\n",
    "clinical_df = pd.read_csv(path.join('data', 'clinical_df.tsv'), sep='\\t', header='infer')\n",
    "\n",
    "# Load the clinical information of patients\n",
    "sample_df = pd.read_csv(path.join('data', 'sample_df.tsv'), sep='\\t', header='infer')\n",
    "specimen_dict = dict(zip(sample_df.icgc_specimen_id, sample_df.icgc_donor_id))\n",
    "\n",
    "TMB_df = pd.read_csv(path.join('data', 'TMB.tsv.gz'), sep='\\t', header='infer', compression='gzip')\n",
    "TMB_df['donor'] = TMB_df['specimenID'].map(specimen_dict)\n",
    "\n",
    "# There are more than one tumoral specimen per donor\n",
    "TMB_df['donor'].value_counts()\n",
    "\n",
    "# Since the analysis is performed by donor clinical data, we can take the mean value across specimens\n",
    "TMB_df.groupby('donor')\n",
    "TMB_clean_df = TMB_df.groupby('donor')['TMB_proxy'].mean().to_frame().reset_index()\n",
    "\n",
    "# Merge the dataframes\n",
    "merged_df = pd.merge(clinical_df, TMB_clean_df, left_on='icgc_donor_id', right_on='donor', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aee67c",
   "metadata": {},
   "source": [
    "One of the problems we might encounter during the training of models is **missing data** in one of the variables used by the model. There are multiple ways to deal with that, the most simple (but more restrictive) is excluding any data element whose value on some feature is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "340a4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the total amount of donors.\n",
    "print('Total donors: ' + str(len(merged_df)))\n",
    "# Drop rows with missing values in either TMB or donor_age_at_diagnosis:\n",
    "df_regression = merged_df[['TMB_proxy', 'donor_age_at_diagnosis']].dropna()\n",
    "# Note that around 100 donors are dropped from analysis due to non-available age at diagnosis data.\n",
    "# This is not a huge proportion.\n",
    "print('Donors with available age at diagnosis: ' + str(len(df_regression)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "965c7bab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import probplot\n",
    "\n",
    "# Check for normality using Shapiro-Wilk test\n",
    "_, p_value_tmb = shapiro(df_regression['TMB_proxy'])\n",
    "_, p_value_age = shapiro(df_regression['donor_age_at_diagnosis'])\n",
    "\n",
    "# Definitely the two variables do not follow a normal distribution\n",
    "print(f'Shapiro-Wilk p-value for TMB: {p_value_tmb}')\n",
    "print(f'Shapiro-Wilk p-value for Age at Diagnosis: {p_value_age}')\n",
    "\n",
    "# Plot histograms for TMB and donor_age_at_diagnosis\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_regression['TMB_proxy'], kde=True, color='skyblue')\n",
    "plt.title('Histogram of TMB')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_regression['donor_age_at_diagnosis'], kde=True, color='salmon')\n",
    "plt.title('Histogram of Age at Diagnosis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1e13bb",
   "metadata": {},
   "source": [
    "Definetly, the two variables do not follow a normal distribution at all. The first outcome of this fact is that a linear regression model is not suitable for modelling this type of data. However, there are ways to partially solve this problem by adapting the model through transformation of the variables, although this will have implications on the interpretation of the model, since the linearity will be between two transformed variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4207b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can try to apply some transformations:\n",
    "## A logarithmic transformation for the TMB could work well\n",
    "## (it looks like a gamma distribution or other long-tailed distributions)\n",
    "df_regression['transformed_TMB'] = np.log1p(df_regression['TMB_proxy'])\n",
    "\n",
    "# The age seems to arise from a mixture of two normal distributions, a boxcox transformation might help\n",
    "from scipy.stats import boxcox\n",
    "df_regression['transformed_age'] = boxcox(df_regression['donor_age_at_diagnosis'])[0]\n",
    "\n",
    "# Check for normality using Shapiro-Wilk test\n",
    "_, p_value_tmb = shapiro(df_regression['transformed_TMB'])\n",
    "_, p_value_age = shapiro(df_regression['transformed_age'])\n",
    "\n",
    "# Definitely the two variables do not follow a normal distribution\n",
    "print(f'Shapiro-Wilk p-value for log TMB: {p_value_tmb}')\n",
    "print(f'Shapiro-Wilk p-value for Age at Diagnosis (box cox): {p_value_age}')\n",
    "\n",
    "# Plot histograms for TMB and donor_age_at_diagnosis\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_regression['transformed_TMB'], kde=True, color='skyblue')\n",
    "plt.title('Histogram of log TMB')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_regression['transformed_age'], kde=True, color='salmon')\n",
    "plt.title('Histogram of Age at Diagnosis (box cox transf.)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6805898",
   "metadata": {},
   "source": [
    "Still after the transformations the distributions do not look normal at all, which is reflected on the Shapiro-Wilk test. We can do a deeper analysis with qpplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39a13f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a quantile-quantile plot for the transformed TMB\n",
    "probplot(df_regression['transformed_TMB'], dist=\"norm\", plot=plt)\n",
    "plt.title('QQ Plot for Transformed TMB')\n",
    "plt.show()\n",
    "\n",
    "# Create a quantile-quantile plot for the transformed age\n",
    "probplot(df_regression['transformed_age'], dist=\"norm\", plot=plt)\n",
    "plt.title('QQ Plot for Transformed Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ed6c44",
   "metadata": {},
   "source": [
    "The qqplot from the logarithm of TMB indicates negative kurtosis (data too accumulated on the peak): the distribution is leptokurtic. With respect to the age at diagnosis, still the distribution is bimodal. Still, we can train a linear regression model and test it with a subset of the data to see the distribution of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07039b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform linear regression\n",
    "y = df_regression['transformed_TMB']\n",
    "X = df_regression['transformed_age'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print the coefficients\n",
    "print(f'Intercept: {model.intercept_}')\n",
    "print(f'Age Coefficient: {model.coef_[0]}')\n",
    "\n",
    "# Get the root of the mean squared error as a metric of the fit\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'Root of the mean squared error: {rmse}')\n",
    "\n",
    "# Scatter plot of the original data\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual Data')\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(X_test, y_pred, color='red', linewidth=2, label='Linear Regression')\n",
    "\n",
    "# Labeling the plot\n",
    "plt.title('Linear Regression')\n",
    "plt.xlabel('Transformed Age')\n",
    "plt.ylabel('Transformed TMB')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals to check for homoskedasticity\n",
    "residuals = y_test - y_pred\n",
    "sns.scatterplot(x=y_pred, y=residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Residuals vs Fitted Values')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f26fd7",
   "metadata": {},
   "source": [
    "Not only the variables follow a normal distribution but also the variance of the residuals is not constant across the predicted values when evaluated with the test dataset: there is heteroskedasticity (specifically, there is a larger variance for low ages of diagnosis).\n",
    "\n",
    "Still there are ways to process the data and try to train a better model. With respect to the age at diagnosis, there are two populations: pediatric cancers and non-pediatric ones. The pediatric cancers usually arise from predisposition syndromes (germline mutations that predispose to develop a cancer), with different mutational dynamics in terms of mutation load than other cancers, which are hypothesized to arise from the accumulation of somatic mutations with time. Hence, we can try to exclude the pediatric cancers and model the relationship of the age and tumor mutation burden for non-pediatric neoplasies.\n",
    "\n",
    "There are two projects including pediatric cancers: the ones from the PBCA-DE cohort (pediatric brain cancer) and some donors from the malignant lymphomas MALY-DE cohort (which also is a bimodal, whith pediatric lymphomas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1647fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df[merged_df['donor_age_at_diagnosis']<18]['project_code'].value_counts())\n",
    "\n",
    "merged_df[merged_df['project_code']=='MALY-DE']['donor_age_at_diagnosis'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77f28914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find total donors with TMB computed\n",
    "print('Total donors: ' + str(len(merged_df)))\n",
    "# Remove pediatric brain cancers and pediatric malignant lymphomas\n",
    "df_regression = merged_df[(merged_df['project_code']!='PBCA-DE')&~((merged_df['project_code']=='MALY-DE')&(merged_df['donor_age_at_diagnosis']<20))][['TMB_proxy', 'donor_age_at_diagnosis']].dropna().copy()\n",
    "\n",
    "# Now more than 300 donors are dropped from analysis\n",
    "print('Non-pediatric donors with available age at diagnosis: ' + str(len(df_regression)))\n",
    "\n",
    "# A logarithmic transformation for the TMB (exponential, long-tailed distribution)\n",
    "df_regression['transformed_TMB'] = np.log1p(df_regression['TMB_proxy'])\n",
    "\n",
    "# Check for normality using Shapiro-Wilk test\n",
    "_, p_value_tmb = shapiro(df_regression['transformed_TMB'])\n",
    "_, p_value_age = shapiro(df_regression['donor_age_at_diagnosis'])\n",
    "\n",
    "# Definitely the two variables do not follow a normal distribution\n",
    "print(f'Shapiro-Wilk p-value for log TMB: {p_value_tmb}')\n",
    "print(f'Shapiro-Wilk p-value for Age at Diagnosis: {p_value_age}')\n",
    "\n",
    "# Plot histograms for TMB and donor_age_at_diagnosis\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(df_regression['transformed_TMB'], kde=True, color='skyblue')\n",
    "plt.title('Histogram of log TMB')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(df_regression['donor_age_at_diagnosis'], kde=True, color='salmon')\n",
    "plt.title('Histogram of Age at Diagnosis')\n",
    "\n",
    "# Create a quantile-quantile plot for the transformed TMB\n",
    "plt.subplot(2, 2, 2)\n",
    "probplot(df_regression['transformed_TMB'], dist=\"norm\", plot=plt)\n",
    "plt.title('QQ Plot for Transformed TMB')\n",
    "\n",
    "# Create a quantile-quantile plot for the transformed age\n",
    "plt.subplot(2, 2, 4)\n",
    "probplot(df_regression['donor_age_at_diagnosis'], dist=\"norm\", plot=plt)\n",
    "plt.title('QQ Plot for Age at Diagnosis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b74d8",
   "metadata": {},
   "source": [
    "With this change, now the independent variable approximates more a normal distribution although the qqplot shows that is negatively skewed. This can be partially corrected with another transformation, using the square of the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "102f0206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A square transformation for the age at diagnosis might helpt with the negative skewness\n",
    "df_regression['transformed_age'] = np.square(df_regression['donor_age_at_diagnosis'])\n",
    "\n",
    "# Check for normality using Shapiro-Wilk test\n",
    "_, p_value_age = shapiro(df_regression['transformed_age'])\n",
    "\n",
    "# Definitely the two variables do not follow a normal distribution\n",
    "print(f'Shapiro-Wilk p-value for square Age at Diagnosis: {p_value_age}')\n",
    "\n",
    "# Plot histograms for TMB and donor_age_at_diagnosis\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_regression['transformed_age'], kde=True, color='salmon')\n",
    "plt.title('Histogram of square Age at Diagnosis')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "probplot(df_regression['transformed_age'], dist=\"norm\", plot=plt)\n",
    "plt.title('QQ Plot for square Age at Diagnosis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed0c18d",
   "metadata": {},
   "source": [
    "Now the age at diagnosis has some level of positive kurticity: it is platykurtic. Still the transformation helped to fit better the data into the assumptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4e9fab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform linear regression\n",
    "y = df_regression['transformed_TMB']\n",
    "X = df_regression['transformed_age'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print the coefficients\n",
    "print(f'Intercept: {model.intercept_}')\n",
    "print(f'Age square Coefficient: {model.coef_[0]}')\n",
    "\n",
    "# Print mean squared error\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'Root of Mean Squared Error: {rmse}')\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "# Scatter plot of the original data\n",
    "ax1.scatter(X_test, y_test, color='blue', label='Actual Data')\n",
    "\n",
    "# Plot the regression line\n",
    "ax1.plot(X_test, y_pred, color='red', linewidth=2, label='Linear Regression')\n",
    "\n",
    "# Labeling the plot\n",
    "ax1.set_title('Linear Regression')\n",
    "ax1.set_xlabel('Square Age at diagnosis')\n",
    "ax1.set_ylabel('log TMB')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot residuals to check for homoskedasticity\n",
    "residuals = y_test - y_pred\n",
    "sns.scatterplot(x=y_pred, y=residuals, ax=ax2)\n",
    "ax2.axhline(y=0, color='r', linestyle='--')\n",
    "ax2.set_title('Residuals vs Fitted Values')\n",
    "ax2.set_xlabel('Fitted Values')\n",
    "ax2.set_ylabel('Residuals')\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig(path.join('plots', 'Age_TMB_LinearRegression.png'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20c0bad",
   "metadata": {},
   "source": [
    "Now there is a pattern of residuals that better fits homoskedasticity and the variables approach more a normal distribution than before the transformations. The root of the mean square error is slightly higher but similar when removing the pediatric tumors. However, does this mean that it is a model with a good predictive power? No, using the age at diagnosis to predict the expected TMB with this model will be a terrible decision. Let's see why is that by interpreting what has been obtained.\n",
    "\n",
    "The first plot shows that even if there seems to be a positive trend between the square of the age at diagnosis vs the log of the TMB, there is a large variance in the log TMB unexplained by the square of the age at diagnosis alone which renders the predictive model useless.\n",
    "\n",
    "This is somehow reflected on the RMSE value and the residuals vs fitted values plots, which remember that are computed for a dependent variable transformed into logrithmic scale: this largely complicates the interpretation of the metric (https://stats.stackexchange.com/questions/314490/regression-rmse-when-dependent-variable-is-log-transformed). Long story short, a RMSE computed from a log-transformation looses its original units and cannot be interpreted anymore as number of mutations but as % of deviations from the geometric mean (which in our case translates into orders of magnitude of difference of error for TMB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e850d71",
   "metadata": {},
   "source": [
    "### Polynomial Regression:\n",
    "\n",
    "Polynomial regression is an extension of linear regression, allowing for the modeling of non-linear relationships. It includes higher-order terms of the independent variable, which allows to capture non-linear relationships in contrast to linear regressions. Therefore:\n",
    "\n",
    "**Advantages:**\n",
    "- Captures non-linear relationships.\n",
    "\n",
    "**Disadvantages:**\n",
    "- May overfit the data.\n",
    "- Assumes absence of collinearity between independent variables (if the model includes more than one)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b592e966",
   "metadata": {},
   "source": [
    "Although the total TMB and the number of mutations attrributed to a specific signature are strictly related (one is a subset of another), the proportion of mutations should be independent (it is independent from the total number of mutations as it has been corrected by them). However, tumors whose somatic mutation burden is dominated by specific mutational processes usually show an hypermutation phenotype, for instance melanomas are hypermutated cancers mostly dominated by signature 7 (UV-light derived mutations) or signature 10 (associated with Polymerase-Epsilon deficient *POLE* mutants) in colorectal and endometrial cancers.\n",
    "\n",
    "We can try to model the logarithm of the TMB (the order of magnitude) based on signature proportions, whose relationship is not expected to be completely linear. Which signatures are relevant for this, and what problems pose to work with proportions as independent variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8792a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the signatures data\n",
    "raw_signatures_df = pd.read_csv(path.join('data' , 'signatures.tsv.gz'), sep='\\t', header='infer', compression='gzip')\n",
    "raw_signatures_df = raw_signatures_df.set_index('signature')\n",
    "\n",
    "# Traspose and reorganize index to have as columns (independent variables) each signature\n",
    "raw_signatures_df = raw_signatures_df.transpose().reset_index()\n",
    "\n",
    "# Some signatures that were extracted at the start of the cancer genomics field were subdivided into more components\n",
    "# (7 was subdivided into 7a, 7b and 7c while 17 into 17a and 17b). To simplify we will merge into one component.\n",
    "# Create new columns by summing the specified columns\n",
    "raw_signatures_df['SBS7a'] = raw_signatures_df[['SBS7a', 'SBS7b', 'SBS7c']].sum(axis=1)\n",
    "raw_signatures_df['SBS17a'] = raw_signatures_df[['SBS17a', 'SBS17b']].sum(axis=1)\n",
    "# Rename the columns ('index' column to 'specimenID' and the others)\n",
    "raw_signatures_df = raw_signatures_df.rename(columns={'index': 'specimenID', \n",
    "                                              'SBS7a': 'SBS7', \n",
    "                                              'SBS17a': 'SBS17',\n",
    "                                              'SBS10a': 'SBS10'})\n",
    "# Drop the original columns if needed and also drop\n",
    "raw_signatures_df = raw_signatures_df.drop(['SBS7b', 'SBS7c', 'SBS17b', 'SBS60', 'SBS83'], axis=1)\n",
    "\n",
    "# Normalize the values in each column to generate the proportions of each signature\n",
    "prop_signatures_df = raw_signatures_df.iloc[:, 1:].div(raw_signatures_df.iloc[:, 1:].sum(axis=1), axis=0)\n",
    "prop_signatures_df.insert(0, 'specimenID', raw_signatures_df['specimenID'])\n",
    "\n",
    "# Load the TMB data\n",
    "TMB_df = pd.read_csv(path.join('data' , 'TMB.tsv.gz'), sep='\\t', header='infer', compression='gzip')\n",
    "TMB_df ['transformed_TMB'] = np.log1p(TMB_df ['TMB_proxy'])\n",
    "\n",
    "# Merge with the signatures dataframe to get the dependent variable\n",
    "signatures_df = pd.merge(prop_signatures_df, TMB_df , left_on='specimenID', right_on='specimenID', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280614f",
   "metadata": {},
   "source": [
    "We can start to check the relationship of some specific signatures with the total TMB. For instance, starting with signature SBS4 (tobacco)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "188fa6b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from scipy.stats import probplot\n",
    "\n",
    "# Select features (only signature 4) and target (log of TMB)\n",
    "X = signatures_df[['SBS9']].values\n",
    "y = signatures_df['transformed_TMB'].values\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel('Proportion of SBS4')\n",
    "plt.ylabel('log TMB')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bd01a5",
   "metadata": {},
   "source": [
    "Most of the data points do not show any contributtion of tobacco (probably non-lung cancer types where tobacco-derived mutagens cannot reach or non-smoker patients). However, with respect to the samples that have mutations attributed to tobacco, the relationship is not lineal as the order of magnitude of mutations seem to reach a plateau as the proportion increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab4898ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can build a polynomial model only using this variable as explanatory\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Skicit learn also allows to use pipelines for models\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "    \n",
    "# First we split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# We will work with a second degree polynomial, that will be enough\n",
    "degree = 2\n",
    "# We will define that the features are polinomial and define a linear model to solve a polinomial transformation\n",
    "pm = make_pipeline(PolynomialFeatures(degree=degree, include_bias=False), LinearRegression())\n",
    "# Fit our data points into our pipeline\n",
    "pm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pm.predict(X_test)\n",
    "# Print the coefficients\n",
    "print(f'Intercept: {model.intercept_}')\n",
    "print(f'Coefficient of SBS4 proportion square: {model.coef_[0]}')\n",
    "# Print mean squared error\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'Root of Mean Squared Error: {rmse}')\n",
    "# Print rsquare score (amount of variance of the dependent variable explained by the independent one)\n",
    "print(f'R^2 score: {r2_score(y_test, y_pred)}')\n",
    "\n",
    "# Generate some values to print the a continous polynomial function\n",
    "X_values = np.linspace(min(X_train), max(X_train), 1000).reshape(-1, 1)\n",
    "# Predict the values from our polynomial regression model\n",
    "y_predict = pm.predict(X_values)\n",
    " \n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "ax1.scatter(X_train, y_train)  # plot original data points\n",
    "ax1.set_title('Polynomial Regression')\n",
    "ax1.plot(X_values, y_predict, color='r')  # plot our best fit curve\n",
    "ax1.set_xlabel('Proportion of SBS4')\n",
    "ax1.set_ylabel('log TMB')\n",
    "\n",
    "# Plot residuals to check for homoskedasticity\n",
    "residuals = y_test - y_pred\n",
    "sns.scatterplot(x=y_pred, y=residuals)\n",
    "ax2.axhline(y=0, color='r', linestyle='--')\n",
    "ax2.set_title('Residuals vs Fitted Values')\n",
    "ax2.set_xlabel('Fitted Values')\n",
    "ax2.set_ylabel('Residuals')\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig(path.join('plots', 'SBS4_TMB_PolynomialRegression.png'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f744619",
   "metadata": {},
   "source": [
    "You might have notice that the model has been built using a pipeline, where the features are transformed with `PolynomialFeatures(degree=degree, include_bias=False)` and then passed to a linear regression through `LinearRegression()`. Why is that? \n",
    "\n",
    "For a detailed explanation on why the include_bias=False while initializing the polynomial model object, you can check https://stackoverflow.com/questions/59725907/scikit-learn-polynomialfeatures-what-is-the-use-of-the-include-bias-option, but long story short by default it assumes that the intercept is non-zero. Why to assume for starters that the intercept is 0? Since we will transform the data to work with a linear model, the intercept problem is taken care on the *LinearRegression* object so there is no need include the bias term while initializing the polynomial model.\n",
    "\n",
    "Moreover, why we use the `LinearRegression()`function if now we do not assume a linear relationship? This is just the way skicit-learn works, by first defining the a polynomial model with `PolynomialFeatures()` and then we instruct to train the model with the regression (solve the coefficients based on the training data of the polynomial function).\n",
    "\n",
    "A quick look might suggest that specimens were at least tobacco-related mutations are present tend to have a higher total tumor mutation burden in logarithmic scale. However, only using this signature is not enough to have a good predictive model. As reflected on the rather large RMSE, lots of samples do not carry any mutation attributed to tobacco but still there is a large variability of log TMB due to other mutational processes. This is reflected on a huge heteroskedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bd4c39",
   "metadata": {},
   "source": [
    "Since a lot of variability in log TMB is not explained by the tobacco signature, we could include other relevant signatures. For instance, signature 7 (attributed to UV-light exposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "883f27eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features (only signature 4) and target (log of TMB)\n",
    "X = signatures_df[['SBS7']].values\n",
    "y = signatures_df['transformed_TMB'].values\n",
    "\n",
    "# First we split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# We will work with a second degree polynomial, that will be enough\n",
    "degree = 2\n",
    "# We will define that the features are polinomial and define a linear model to solve a polinomial transformation\n",
    "pm = make_pipeline(PolynomialFeatures(degree=degree, include_bias=False), LinearRegression())\n",
    "# Fit our data points into our pipeline\n",
    "pm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pm.predict(X_test)\n",
    "# Print the coefficients\n",
    "print(f'Intercept: {model.intercept_}')\n",
    "print(f'Coefficient of SBS4 proportion square: {model.coef_[0]}')\n",
    "# Print mean squared error\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'Root of Mean Squared Error: {rmse}')\n",
    "# Print rsquare score (amount of variance of the dependent variable explained by the independent one)\n",
    "print(f'R^2 score: {r2_score(y_test, y_pred)}')\n",
    "\n",
    "# Generate some values to print the a continous polynomial function\n",
    "X_values = np.linspace(min(X_train), max(X_train), 1000).reshape(-1, 1)\n",
    "# Predict the values from our polynomial regression model\n",
    "y_predict = pm.predict(X_values)\n",
    " \n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "ax1.scatter(X_train, y_train)  # plot original data points\n",
    "ax1.set_title('Polynomial Regression')\n",
    "ax1.plot(X_values, y_predict, color='r')  # plot our best fit curve\n",
    "ax1.set_xlabel('Proportion of SBS4')\n",
    "ax1.set_ylabel('log TMB')\n",
    "\n",
    "# Plot residuals to check for homoskedasticity\n",
    "residuals = y_test - y_pred\n",
    "sns.scatterplot(x=y_pred, y=residuals)\n",
    "ax2.axhline(y=0, color='r', linestyle='--')\n",
    "ax2.set_title('Residuals vs Fitted Values')\n",
    "ax2.set_xlabel('Fitted Values')\n",
    "ax2.set_ylabel('Residuals')\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig(path.join('plots', 'SBS7_TMB_PolynomialRegression.png'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92b627",
   "metadata": {},
   "source": [
    "Here the relationship seems different, where the pipeline build a polynomial model that approximates the relationship through two differentiated populations: samples with low contribution of the UV-light signature seem to have more mutations that the mean across samples with no presence of this signature but still lower than a population of hypermutated samples that tend to have high proportions of SBS7 signature (intermediate samples are almost non-existent).\n",
    "\n",
    "Now let's see what is the relationship with signatures largely common across specimens like SBS1, which have been attributed to the background processes that generate mutations on all types of cells (even healthy ones)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "36633916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features (only signature 4) and target (log of TMB)\n",
    "X = signatures_df[['SBS1']].values\n",
    "y = signatures_df['transformed_TMB'].values\n",
    "\n",
    "# First we split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# We will work with a second degree polynomial, that will be enough\n",
    "degree = 2\n",
    "# We will define that the features are polinomial and define a linear model to solve a polinomial transformation\n",
    "pm = make_pipeline(PolynomialFeatures(degree=degree, include_bias=False), LinearRegression())\n",
    "# Fit our data points into our pipeline\n",
    "pm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pm.predict(X_test)\n",
    "# Print the coefficients\n",
    "print(f'Intercept: {model.intercept_}')\n",
    "print(f'Coefficient of SBS4 proportion square: {model.coef_[0]}')\n",
    "# Print mean squared error\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'Root of Mean Squared Error: {rmse}')\n",
    "# Print rsquare score (amount of variance of the dependent variable explained by the independent one)\n",
    "print(f'R^2 score: {r2_score(y_test, y_pred)}')\n",
    "\n",
    "# Generate some values to print the a continous polynomial function\n",
    "X_values = np.linspace(min(X_train), max(X_train), 1000).reshape(-1, 1)\n",
    "# Predict the values from our polynomial regression model\n",
    "y_predict = pm.predict(X_values)\n",
    " \n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "ax1.scatter(X_train, y_train)  # plot original data points\n",
    "ax1.set_title('Polynomial Regression')\n",
    "ax1.plot(X_values, y_predict, color='r')  # plot our best fit curve\n",
    "ax1.set_xlabel('Proportion of SBS4')\n",
    "ax1.set_ylabel('log TMB')\n",
    "\n",
    "# Plot residuals to check for homoskedasticity\n",
    "residuals = y_test - y_pred\n",
    "sns.scatterplot(x=y_pred, y=residuals)\n",
    "ax2.axhline(y=0, color='r', linestyle='--')\n",
    "ax2.set_title('Residuals vs Fitted Values')\n",
    "ax2.set_xlabel('Fitted Values')\n",
    "ax2.set_ylabel('Residuals')\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig(path.join('plots', 'SBS1_TMB_PolynomialRegression.png'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33384b",
   "metadata": {},
   "source": [
    "Here it is relevant to discuss the behaviour of SBS1. As stated above, it is a mutational process present in mostly all types of cells (healthy ones included, it usually reflects the dominant mutational processes across the cells of the human body and even species: https://www.nature.com/articles/s41586-022-04618-z). The polynomial regression here reflects that cells with high proportions of this 'base' mutational process  tend to have less mutations than others, although still there is a large variance that shoulb de explained by the presence of other mutational processes. Here the residuals whos a pattern more compatible with homoskedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdadab97",
   "metadata": {},
   "source": [
    "Therefore, after reviewing the relationships with these key signatures we can conclude that we will need a combination of them to explain a large part of the variance in the order of magnitude of mutations. However, the fact that all the variables are proportions (they are not entirely independent since they all sum 1) will have implications since that largely **violates the assumption of absence in collinearity**. This, in turn, will generate problems on the model through estimations of artefactually large parameters (regression coeficients) during the training phase.\n",
    "\n",
    "To assess the strength for collinearity between independent variables, we can compute the **Variation Inflation Factor (VIF)**:\n",
    "\n",
    "$$ \\text{VIF} = \\frac{1}{1 - R^2} $$\n",
    "\n",
    "where $R^2$ is the unadjusted correlation coefficient of the one variable against all the others. This statistic ranges from 1 to +Inf, being 1 no correlation at all and infinite a total correlation. The **Tolerance Index**, which is the denominator, is also used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "68875a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute VIF using sklearn linear model.\n",
    "def sklearn_vif(exogs, data):\n",
    "\n",
    "    # initialize dictionaries\n",
    "    vif_dict, tolerance_dict = {}, {}\n",
    "\n",
    "    # form input data for each exogenous variable\n",
    "    for exog in exogs:\n",
    "        not_exog = [i for i in exogs if i != exog]\n",
    "        X, y = data[not_exog], data[exog]\n",
    "\n",
    "        # extract r-squared from the fit\n",
    "        r_squared = LinearRegression().fit(X, y).score(X, y)\n",
    "\n",
    "        # calculate VIF\n",
    "        vif = 1/(1 - r_squared)\n",
    "        vif_dict[exog] = vif\n",
    "\n",
    "        # calculate tolerance\n",
    "        tolerance = 1 - r_squared\n",
    "        tolerance_dict[exog] = tolerance\n",
    "\n",
    "    # return VIF DataFrame\n",
    "    df_vif = pd.DataFrame({'VIF': vif_dict, 'Tolerance': tolerance_dict})\n",
    "\n",
    "    return df_vif\n",
    "\n",
    "# Work only with the independent variables to compute VIF\n",
    "sklearn_vif(exogs=list(signatures_df.columns[1:-3]), data=signatures_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670b18a2",
   "metadata": {},
   "source": [
    "Obviously, since all the variables sum up to 1, the correlation of one against the other is absolute. Given that this collinearity is structural, that means, it comes mostly due to the own definition of the variables into proportions, and that moderate levels will not affect severely the model, it can be partially fixed by standarizing the independent variables directly from the TMB and reducing the collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0e59f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the variables\n",
    "scaler = StandardScaler()\n",
    "df_standardized = pd.DataFrame(scaler.fit_transform(raw_signatures_df[raw_signatures_df.columns[1:]]), columns=raw_signatures_df.columns[1:])\n",
    "df_standardized.insert(0, 'specimenID', raw_signatures_df['specimenID'])\n",
    "\n",
    "# Merge with the signatures dataframe to get the dependent variable\n",
    "df_standardized = pd.merge(df_standardized, TMB_df , left_on='specimenID', right_on='specimenID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "77f0ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work only with the independent variables to compute VIF\n",
    "VIF_df = sklearn_vif(exogs=list(df_standardized.columns[1:-3]), data=df_standardized[df_standardized.columns[1:-3]])\n",
    "VIF_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41a008ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIF_df['VIF'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9c0e38",
   "metadata": {},
   "source": [
    "A rule of a thumb is to consider high levels of collinearity with VIF greater than 4. The mean across independent variables is below that, but some of them are above that value. Considering the large amount of variables a filtering step removing any signature above 2.5 will remove around half of the features with more collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da99afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work only with the independent variables to compute VIF\n",
    "df_standardized = df_standardized.drop(columns=list(VIF_df[VIF_df['VIF']>2.5].index))\n",
    "\n",
    "# Work only with the independent variables to compute VIF\n",
    "VIF_df = sklearn_vif(exogs=list(df_standardized.columns[1:-3]), data=df_standardized[df_standardized.columns[1:-3]])\n",
    "VIF_df['VIF'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0ada5",
   "metadata": {},
   "source": [
    "We can further restrict the amount of features (signatures). For instance we can select the top most relevant features (a process called **feature selection**) or even use **dimensionality reduction** methodologies such as the ones explained on a previous section. \n",
    "\n",
    "Here, we will try to apply a **feature selection** process known as **Recursive Feature Elimination (RFE)**, which consists on developing/training the model while removing recursively the less relevant features until defining a model with a desired number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3e0fed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# This function will perform a multiple polynomial regression while doinf a RFE of the k best features.\n",
    "def multiple_polynomial_regression_RFE(signatures_df, indep_variable, degree, k_best):\n",
    "    \n",
    "    # Select features\n",
    "    features = signatures_df.columns[1:-3]\n",
    "    X = signatures_df[features]\n",
    "    y = signatures_df[indep_variable]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the pipeline, this time we will do an estimation before using a support vector linear regression\n",
    "    estimator = SVR(kernel=\"linear\")\n",
    "    selector = RFE(estimator, n_features_to_select=k_best)\n",
    "    pm = make_pipeline(selector, PolynomialFeatures(degree=degree, include_bias=False), LinearRegression())\n",
    "\n",
    "    # Fit the data points into the pipeline\n",
    "    pm.fit(X_train, y_train)\n",
    "\n",
    "    # Test the model and plot residuals\n",
    "    y_pred = pm.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "\n",
    "    sns.scatterplot(x=y_pred, y=residuals)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.title('Residuals vs Fitted Values')\n",
    "    plt.xlabel('Fitted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.show()\n",
    "\n",
    "    # Print root mean squared error\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f'Root of Mean Squared Error: {rmse}')\n",
    "    # Print rsquare score (amount of variance of the dependent variables explained by the independent one)\n",
    "    print(f'R^2 score: {r2_score(y_test, y_pred)}')\n",
    "    \n",
    "    # Get the selected features using the RFE indices and the corresponding names to show them.\n",
    "    selected_feature_indices = np.where(selector.support_)[0]\n",
    "    selected_features = signatures_df.columns[2:-3][selected_feature_indices]\n",
    "    print(f'Top {k_best} selected features: {selected_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b2bbe74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with the top 2 best features, it might take some time.\n",
    "degree = 2\n",
    "k_best = 2\n",
    "multiple_polynomial_regression_RFE(df_standardized, 'transformed_TMB', degree, k_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b4e5d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with the top 5 best features, it might take some time.\n",
    "degree = 2\n",
    "k_best = 5\n",
    "multiple_polynomial_regression_RFE(df_standardized, 'transformed_TMB', degree, k_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "910c8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with the top 10 best features, it might take some time.\n",
    "degree = 2\n",
    "k_best = 10\n",
    "multiple_polynomial_regression_RFE(df_standardized, 'transformed_TMB', degree, k_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ece35",
   "metadata": {},
   "source": [
    "After training several models the one with the better performance if we use the RMSE as metric is the one with that uses 5 signatures. However, a close look to the distribution of errors it is clear that there is no homoskedasticity. Furthermore, the mean squared error indicates mean deviations on the predictions of around on order of magnitude of the amount of mutations, which from a predictive point of view does not seem to very powerful to use the signature composition to predict the raw amount of mutations (even if they are not entirely independent, at least for some of the signatures, the features).\n",
    "\n",
    "There are much more other factors that have an influence on the amount of mutations of a tumor (the age of the patient is one, as shown on the linear regression example) and not even working with large amounts of data (and build large models) can lead to useful models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc33c7",
   "metadata": {},
   "source": [
    "However, we can try to build models restricting to specific types of tumors. This might remove a lot of the variance due to the own idiosincracy of the tumor type and the dynamics of the tissue where it arises. For instance, we can restrict the model to melanomas only, which is a type with a large variance in TMB and enough samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ddeed8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the specimen IDs exclusive from melanomas and subset\n",
    "melanoma_specimens = TMB_df[TMB_df['hist_type']=='Skin_Melanoma']['specimenID']\n",
    "unprocessed_melanoma_df = raw_signatures_df[raw_signatures_df['specimenID'].isin(melanoma_specimens)].copy()\n",
    "\n",
    "# Find signature (features) present in melanoma sample (exclude non present)\n",
    "sumover = unprocessed_melanoma_df[unprocessed_melanoma_df.columns[1:]].sum()\n",
    "valid_features = sumover[sumover!=0.0].index\n",
    "\n",
    "# Standardize the variables\n",
    "scaler = StandardScaler()\n",
    "melanoma_df = pd.DataFrame(scaler.fit_transform(unprocessed_melanoma_df[unprocessed_melanoma_df.columns[1:]]), columns=unprocessed_melanoma_df.columns[1:])[valid_features]\n",
    "melanoma_df.insert(0, 'specimenID', list(unprocessed_melanoma_df['specimenID']))\n",
    "\n",
    "# Merge with the signatures dataframe to get the dependent variable\n",
    "melanoma_df = pd.merge(melanoma_df, TMB_df, left_on='specimenID', right_on='specimenID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9769b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work only with the independent variables to compute VIF\n",
    "VIF_df = sklearn_vif(exogs=list(melanoma_df.columns[1:-3]), data=melanoma_df[melanoma_df.columns[1:-3]])\n",
    "VIF_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "05f28ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIF_df['VIF'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f361af0",
   "metadata": {},
   "source": [
    "The mean here is higher, which means that for this specific cancer (removed a lot of variability coming from cancer type) in general the different signatures are highly correlated. Given the relatively low amount of features, we can apply another feature selection method which is similar to **RFE** but uses cross-validation to find the optimal number of features (it might be computationally more expensive but can provide better results): **Recursive Feature Elimination with Cross-Validation (RFECV)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5ecdac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# This function will perform a multiple polynomial regression while doinf a RFECV of the k best features.\n",
    "def multiple_polynomial_regression_RFECV(signatures_df, indep_variable, degree):\n",
    "    \n",
    "    # Select features\n",
    "    features = signatures_df.columns[1:-3]\n",
    "    X = signatures_df[features]\n",
    "    y = signatures_df[indep_variable]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the pipeline, this time we will do an estimation before using a support vector linear regression\n",
    "    estimator = SVR(kernel=\"linear\")\n",
    "    selector = RFECV(estimator, step=1, cv=5)\n",
    "    pm = make_pipeline(selector, PolynomialFeatures(degree=degree, include_bias=False), LinearRegression())\n",
    "    \n",
    "    # Fit the data points into the pipeline\n",
    "    pm.fit(X_train, y_train)\n",
    "\n",
    "    # Test the model and plot residuals\n",
    "    y_pred = pm.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "\n",
    "    sns.scatterplot(x=y_pred, y=residuals)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.title('Residuals vs Fitted Values')\n",
    "    plt.xlabel('Fitted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.show()\n",
    "\n",
    "    # Print root mean squared error\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f'Root of Mean Squared Error: {rmse}')\n",
    "    # Print rsquare score (amount of variance of the dependent variable explained by the independent one)\n",
    "    print(f'R^2 score: {r2_score(y_test, y_pred)}')\n",
    "    \n",
    "    # Get the selected features using the RFECV\n",
    "    selected_feature_indices = np.where(selector.support_)[0]\n",
    "    selected_features = signatures_df.columns[2:-3][selected_feature_indices]\n",
    "    print(f'selected features: {selected_features}')\n",
    "    \n",
    "    # Print a 2D plot if there is only one feature\n",
    "    if len(selected_features) == 1:\n",
    "        selected_feature = selected_features[0]\n",
    "        \n",
    "        # Accessing the coefficients and intercept\n",
    "        coefficients = pm.named_steps['linearregression'].coef_\n",
    "        intercept = pm.named_steps['linearregression'].intercept_\n",
    "        print(f'Coefficients: {coefficients}')\n",
    "        print(f'Intercept: {intercept}')\n",
    "\n",
    "        # Create the figure\n",
    "        plt.scatter(X_test[selected_feature], y_test, color='blue', label='Actual Data')\n",
    "\n",
    "        # Generate some values to print the a continous polynomial function\n",
    "        X_values = np.linspace(min(X_train[selected_feature]), max(X_train[selected_feature]), 1000)\n",
    "        # Predict the values from our polynomial regression model\n",
    "        y_predict = (X_values**2)*coefficients[1] + X_values*coefficients[0] + intercept\n",
    "\n",
    "        # Plot the regression line\n",
    "        plt.plot(X_values, y_predict, color='red', linewidth=2, label='Polynomial Regression')\n",
    "\n",
    "        # Labeling the plot\n",
    "        plt.title('Polynomial Regression')\n",
    "        plt.xlabel(selected_feature)\n",
    "        plt.ylabel('log TMB')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ad15a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best features, it might take some time.\n",
    "degree = 2\n",
    "multiple_polynomial_regression_RFECV(melanoma_df, 'transformed_TMB', degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bf410",
   "metadata": {},
   "source": [
    "The model has a higher predictive ability, but still there is a large heteroskedasticity. However, in my experience the SBS7 signature is the one that mostly defines the melanoma mutation burden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a101341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will perform a multiple polynomial regression while doinf a RFECV of the k best features.\n",
    "def multiple_polynomial_regression(signatures_df, indep_variable, feature, degree):\n",
    "    \n",
    "    # Select feature\n",
    "    X = signatures_df[feature].values.reshape(-1,1)\n",
    "    y = signatures_df[indep_variable].values\n",
    "    \n",
    "    # First we split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # We will define that the features are polinomial and define a linear model to solve a polinomial transformation\n",
    "    pm = make_pipeline(PolynomialFeatures(degree=degree, include_bias=False), LinearRegression())\n",
    "    # Fit our data points into our pipeline\n",
    "    pm.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = pm.predict(X_test)\n",
    "    # Print the coefficients\n",
    "    intercept = pm[1].intercept_\n",
    "    print(f'Intercept: {intercept}')\n",
    "    coefficients = pm[1].coef_\n",
    "    print(f'Coefficients: {coefficients}')\n",
    "    # Print mean squared error\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f'Root of Mean Squared Error: {rmse}')\n",
    "    # Print rsquare score (amount of variance of the dependent variable explained by the independent one)\n",
    "    print(f'R^2 score: {r2_score(y_test, y_pred)}')\n",
    "\n",
    "    # Generate some values to print the a continous polynomial function\n",
    "    X_values = np.linspace(min(X_train), max(X_train), 1000)\n",
    "    # Predict the values from our polynomial regression model\n",
    "    y_predict = pm.predict(X_values)\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "    ax1.scatter(X_train, y_train)  # plot original data points\n",
    "    ax1.set_title('Polynomial Regression')\n",
    "    ax1.plot(X_values, y_predict, color='r')  # plot our best fit curve\n",
    "    ax1.set_xlabel('Feature')\n",
    "    ax1.set_ylabel('Indep variable')\n",
    "\n",
    "    # Plot residuals to check for homoskedasticity\n",
    "    residuals = y_test - y_pred\n",
    "    sns.scatterplot(x=y_pred, y=residuals)\n",
    "    ax2.axhline(y=0, color='r', linestyle='--')\n",
    "    ax2.set_title('Residuals vs Fitted Values')\n",
    "    ax2.set_xlabel('Fitted Values')\n",
    "    ax2.set_ylabel('Residuals')\n",
    "\n",
    "    return(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cb3e37",
   "metadata": {},
   "source": [
    "We can try with different degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e71e124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 1\n",
    "fig = multiple_polynomial_regression(melanoma_df, 'transformed_TMB', 'SBS7', degree)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d5b7a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 2\n",
    "fig = multiple_polynomial_regression(melanoma_df, 'transformed_TMB', 'SBS7', degree)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "783438a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 3\n",
    "fig = multiple_polynomial_regression(melanoma_df, 'transformed_TMB', 'SBS7', degree)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "998b3eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 4\n",
    "fig = multiple_polynomial_regression(melanoma_df, 'transformed_TMB', 'SBS7', degree)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35839dd4",
   "metadata": {},
   "source": [
    "Notice the underfitting while using a linear model (degree 1), and the error generally diminishes as the model becomes more complex (increases the degree), however, we can notice that the model overfits to predict an outlier which probably impairs its predictability on other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b5316f",
   "metadata": {},
   "source": [
    "## Classification Methods\n",
    "\n",
    "Classification methods are used when the target (dependent) variable is categorical, and the goal is to assign each data point to a specific category. Here the explanatory variables (independent) can be either categorical or continous. Hence, the amount of classification algorithms is very long.\n",
    "\n",
    "Classification algorithms, unlike regressions, use evaluation metrics that are based on the amount of correct predictions of the model based on the known labels (true values). This is usually expressed as a **confusion matrix**, a table with four entries for a binary classification model: the **True Positives (TP)** and **True Negatives (TN)**, where the model correctly predicts one class or the other, and the **False Positives (FP)** and **False Negatives (FN)**, where the model incorrectly predicts the positive and negative class, respectively.\n",
    "\n",
    "From these four elements of the table, several metrics could be extracted:\n",
    "\n",
    "- **Precision** is the ratio of correctly predicted positive observations to the total predicted positives. It is a measure of the accuracy of positive predictions.\n",
    "\n",
    "- **Specificity** is the ratio of correctly predicted negative observations to the total actual negatives. It is a measure of the accuracy of negative predictions.\n",
    "\n",
    "- **Sensitivity**, also known as **recall** or **true positive rate**, is the ratio of correctly predicted positive observations to the total actual positives. It measures the model's ability to correctly identify positive instances.\n",
    "\n",
    "- **Accuracy** is the ratio of correctly predicted observations to the total observations. It provides an overall measure of the model's correctness.\n",
    "\n",
    "- The **F1 score** is the harmonic mean of precision and sensitivity. It provides a balance between precision and sensitivity.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Classification metrics](https://miro.medium.com/v2/resize:fit:640/format:webp/1*NhPwqJdAyHWllpeHAqrL_g.png)](https://medium.com/all-about-ml/evaluation-metrics-in-classification-algorithms-79c036a131cb)\n",
    "\n",
    "<!-- Add an empty line here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd48e53",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "The perceptron is one of the most basic binary classifiers, and the fundamental building block of artificial neural networks. The idea behind the perceptron is the biological neuron, where based on the different inputs collected from the dendrites it triggers/not triggers a signal on the axon to the next neuron:\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Neuron](images/Neurons.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Analogously, the perceptron takes as input one or a vector of continous independent variables and their weights, which is summed up and passes the result through a **binary step or unit step function** to produce a binary output $\\hat{y}$:\n",
    "\n",
    "$$\\hat{y} = \\begin{cases} 1 & \\text{if } w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n > 0 \\\\ 0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "where:\n",
    "- $\\hat{y}$ is the predicted output.\n",
    "- $w_0, w_1, w_2, ..., w_n$ are the weights associated with the inputs.\n",
    "- $x_1, x_2, ..., x_n$ are the input features.\n",
    "\n",
    "As we will while discussing advanced supervised methods there are multiple **activation functions** besides the binary step function that can be used. At any rate, the Perceptrons are trained (learn from the data) by adjusting its weights based on the error in its predictions: \n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Perceptron scheme](images/Perceptron.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "To understand the perceptron, we need to understand a concept that is key also for training neural networks: the **learning rule**. The learning rule is how the weights are updated based on the error in its predictions. For the perceptron, the updated weight $w^\\prime_i$ is computed iteratively during the training process until convergence is achieved as follows:\n",
    "\n",
    "$$w^\\prime_i \\leftarrow w_i + \\alpha \\cdot (y - \\hat{y}) \\cdot x_i$$\n",
    "\n",
    "where:\n",
    "- $w_i$ is the weight associated with the $i$-th input feature.\n",
    "- $\\alpha$ is the learning rate, a small positive constant of range [0, 1] that determines the step size in weight updates.\n",
    "- $y$ is the true label (the actual class of the instance).\n",
    "- $\\hat{y}$ is the predicted output of the perceptron.\n",
    "- $x_i$ is the $i$-th input feature.\n",
    "\n",
    "Hence, the **learning rule** of the perceptron essentially adjusts the weights to reduce the difference between the predicted output ($\\hat{y}$) and the true label ($y$), where the term $(y - \\hat{y})$ represents the error of prediction at iteration $i$-th. With this, the weights are updated in the direction that minimizes this error at a **learning rate** $\\alpha$. This constant is defined as an hyperparameter in order to maximize the search for the optimal solution: with a too small value, the alrgorithm will take too much time and computational resources to find the optimal solution (and also it might get stucked in local optimums) whereas a too large learning rate the algorithm might not use properly the gradient to find the optimas.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Different learning rates](images/Learning_rate.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "**Advantages:**\n",
    "- Perceptrons are conceptually **simple** and computationally **efficient**.\n",
    "- Perceptrons will **converge** to a solution if the data is linearly separable.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Perceptrons are **limited to linear separability** (can only learn linear decision boundaries).\n",
    "- Sensitivity to Outliers: Outliers can significantly impact perceptron performance.\n",
    "- Perceptrons only produce binary outputs **without associated probabilisties**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953be41",
   "metadata": {},
   "source": [
    "As a practical example, we will use the output of PCA applied to the transcriptomic profile across blood tumors. We will try to train models that are able to differentiate different histological subtypes of primary blood cancers based on the two first principal components (linear combinations of genes expression that explain most of the variability in expression).\n",
    "\n",
    "In order to define histological subtypes, we will have to download that information from ICGC, contained in the **pcawg_specimen_histology_August2016_v9.xlsx** excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a99fd075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "# To ignore some plot warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We load the expression data\n",
    "expression_df = pd.read_csv(path.join('data', 'gene_expression.tsv.gz'),\n",
    "                                                        sep=\"\\t\", header='infer', index_col=0, compression='gzip')\n",
    "\n",
    "# Get sample dataframe with the information\n",
    "sample_df = pd.read_csv(path.join('data', 'sample_df.tsv'), sep=\"\\t\", header='infer')\n",
    "\n",
    "# Finally we load directly from the ICGC database the histology excel file\n",
    "histology_df = pd.read_excel('https://dcc.icgc.org/api/v1/download?fn=/PCAWG/clinical_and_histology/pcawg_specimen_histology_August2016_v9.xlsx')\n",
    "histology_df.columns = ['icgc_specimen_id'] + list(histology_df.columns[1:])\n",
    "histology_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "11232648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dictionary to translate the Specimen IDs to the histological subtype\n",
    "tumortype_dict = dict(zip(histology_df.icgc_specimen_id, histology_df.histology_tier3))\n",
    "\n",
    "# Get the specimensID of blood cancers\n",
    "specimens_blood = sample_df[sample_df['primary_location']=='Blood']['icgc_specimen_id']\n",
    "\n",
    "# Intersect with the available columns to find the blood cancer specimens with available expression\n",
    "subset_specimens = list(set(expression_df.columns).intersection(set(specimens_blood)))\n",
    "\n",
    "# Filter the data to do the PCA only with blood cancer specimens\n",
    "blood_expression_df = expression_df[subset_specimens].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dbd7f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We preprocess the data with standarization\n",
    "scaler = StandardScaler()\n",
    "blood_data = scaler.fit_transform(blood_expression_df.T)\n",
    "\n",
    "# We perform the PCA\n",
    "blood_pca2D = PCA(n_components=2)\n",
    "blood_proj_data = blood_pca2D.fit_transform(blood_data)\n",
    "\n",
    "# We can check the amount of variance explained by the two Principcal components\n",
    "print(blood_pca2D.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9c28efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def plot_PCA_withlabels(proj_data, labels, plotname):\n",
    "    \n",
    "    # Automatically create a mapping from label categories to integers\n",
    "    unique_labels = np.unique(labels)\n",
    "    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    # Convert labels to integers based on the automatic mapping\n",
    "    label_integers = np.array([label_mapping[label] for label in labels])\n",
    "    # Automatically generate a ListedColormap with unique colors based on the number of labels\n",
    "    num_colors = len(unique_labels)\n",
    "    color_map = plt.get_cmap('tab20', num_colors)\n",
    "    # Create a ListedColormap with unique colors\n",
    "    listed_color_map = ListedColormap([color_map(idx) for idx in range(num_colors)])\n",
    "\n",
    "    # Plot with colours \n",
    "    # We plot the data (the first two components are the first two columns of proj_data)\n",
    "    scatter = plt.scatter(proj_data[:,0], proj_data[:,1], s=2, c=label_integers, cmap=listed_color_map)\n",
    "\n",
    "    # Create legend handles and labels\n",
    "    legend_handles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map(idx), markersize=10, label=label)\n",
    "                      for idx, label in enumerate(unique_labels)]\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend(handles=legend_handles, title='Tumor Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.title('Transcriptome')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "\n",
    "    # Save image\n",
    "    plt.savefig(path.join('plots', plotname))\n",
    "    plt.show()\n",
    "    \n",
    "    return(label_integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "05b5b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tumor type for each sample (each column)\n",
    "labels_blood = blood_expression_df.columns.map(tumortype_dict)\n",
    "blood_label_integers = plot_PCA_withlabels(blood_proj_data, labels_blood, 'Blood_PCA.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11a551",
   "metadata": {},
   "source": [
    "There is a clear separation between chronic lymphocitic leukemias and mature B-cell lymphomas. Now, we can train a perceptron using scikit-learn that can separate both groups. For this kind of classifier models it is useful to **standarize** the data as we saw for the dimensionality reduction algorithms, however, the output of the PCA is already standarized so both **training** and **test** dataset, which derive from the same standarize output, will have the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c45af3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the dependent and independent variables\n",
    "## The labels are the two types of blood cancers we want to separate, encoded as 0s and 1s.\n",
    "y = pd.DataFrame(blood_label_integers)\n",
    "## The two features are the two first PC, already on the 2D numpy array format (columns are the PC)\n",
    "X = pd.DataFrame(blood_proj_data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "ppn = Perceptron(eta0=0.1, random_state=42)\n",
    "ppn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c454dfa6",
   "metadata": {},
   "source": [
    "The model is already trained with the training dataset, but before plotting the results on the test dataset we can define a function to conviniently visualize the decision boundary regions that arise from training the perceptron or other classifier models that we will see on this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c786e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, y, classifier, resolution, test_idx=None, labels=None):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    cmap = plt.get_cmap('Paired', len(markers))\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n",
    "    \n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=1, c=cmap(idx), \n",
    "                    marker=markers[idx], label=labels[idx])\n",
    "        \n",
    "    # highlight test samples\n",
    "    if test_idx.any():\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c='black',\n",
    "        alpha=0.2, linewidth=0.1, marker='o',\n",
    "        s=55, label='test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "95a7bcce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the indexes into numpy format to mark the test dataset\n",
    "test_idx = X_test.index.to_numpy()\n",
    "# The function needs the independent and dependent dataframes to be passed as numpy arrays\n",
    "X_numpy = X.to_numpy()\n",
    "y_numpy = y[0].to_numpy()\n",
    "\n",
    "# The names of the classes, not encoded as 0s ans 1s for plot purposes\n",
    "unique_labels = list(labels_blood.unique())\n",
    "\n",
    "# Call the function to generate the plot\n",
    "plot_decision_regions(X_numpy, y_numpy, ppn, 0.1, test_idx, unique_labels)\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Save image\n",
    "plt.savefig(path.join('plots', 'Perceptron_blood.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2c86e",
   "metadata": {},
   "source": [
    "Since the two classes can be separated linearly, the perceptron works well on the classification task. This is reflected on the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5b974957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
    "\n",
    "def compute_evaluation_metrics(model, X_test, y_test, labels):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "    # Precision, Recall, F1 Score for binary classification\n",
    "    if len(np.unique(y_test)) == 2:\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        print(f'Precision: {precision:.2f}')\n",
    "        print(f'Recall: {recall:.2f}')\n",
    "        print(f'F1 Score: {f1:.2f}')\n",
    "\n",
    "    # Precision, Recall, F1 Score for multi-class classification\n",
    "    else:\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        print(f'Precision: {precision:.2f}')\n",
    "        print(f'Recall: {recall:.2f}')\n",
    "        print(f'F1 Score: {f1:.2f}')\n",
    "\n",
    "    # Confusion Matrix\n",
    "    print('Confusion Matrix:')\n",
    "    disp = ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, display_labels=labels, xticks_rotation='vertical')\n",
    "    \n",
    "    \n",
    "compute_evaluation_metrics(ppn, X_test, y_test, unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff71e62",
   "metadata": {},
   "source": [
    "But, it will work that well on a dataset with no clear lineal decision boundary? Let's try for instance with Kidney cancer, where there are three subtypes of adenocarcinomas depending on the affected cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "33071dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dictionary to translate the Specimen IDs to the histological subtype\n",
    "subtype_tumortype_dict = dict(zip(histology_df.icgc_specimen_id, histology_df.histology_tier4))\n",
    "\n",
    "# Get the specimensID of blood cancers\n",
    "specimens_kidney = sample_df[sample_df['primary_location']=='Kidney']['icgc_specimen_id'].copy()\n",
    "\n",
    "# Intersect with the available columns to find the blood cancer specimens with available expression\n",
    "subset_specimens = list(set(expression_df.columns).intersection(set(specimens_kidney)))\n",
    "\n",
    "# Filter the data to do the PCA only with blood cancer specimens\n",
    "kidney_expression_df = expression_df[subset_specimens].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "73d561aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We preprocess the data with standarization\n",
    "scaler2 = StandardScaler()\n",
    "kidney_data = scaler2.fit_transform(kidney_expression_df.T)\n",
    "\n",
    "# We perform the PCA\n",
    "kidney_pca2D = PCA(n_components=2)\n",
    "kidney_proj_data = kidney_pca2D.fit_transform(kidney_data)\n",
    "\n",
    "# We can check the amount of variance explained by the two Principal components\n",
    "print(kidney_pca2D.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0e78fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tumor type for each sample (each column)\n",
    "labels_kidney = kidney_expression_df.columns.map(subtype_tumortype_dict)\n",
    "kidney_label_integers = plot_PCA_withlabels(kidney_proj_data, labels_kidney, 'Kidney_PCA.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0dbe3466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dependent and independent variables\n",
    "## The labels are the two types of kidney cancers we want to separate, encoded as 0s, 1s and 2s.\n",
    "y = pd.DataFrame(kidney_label_integers)\n",
    "## The two features are the two first PC, already on the 2D numpy array format (columns are the PC)\n",
    "X = pd.DataFrame(kidney_proj_data)\n",
    "\n",
    "# The names of the classes, not encoded as 0s, 1s and 2s for plot purposes\n",
    "unique_labels = list(labels_kidney.unique())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "ppn = Perceptron(eta0=0.1, random_state=42)\n",
    "ppn.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_regions(X.to_numpy(), y[0].to_numpy(), ppn, 0.1, X_test.index.to_numpy(), unique_labels)\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Save image\n",
    "plt.savefig(path.join('plots', 'Perceptron_kidney.png'))\n",
    "plt.show()\n",
    "\n",
    "# We check the metrics in this case\n",
    "compute_evaluation_metrics(ppn, X_test, y_test, unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd03de9",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Logistic Regression is a popular statistical method used for binary classification tasks, although, similarly to the perceptron, it is possible to implement it on multi-class taks by dividing the problem in multiple binary classifiers of one vs the rest or **OvR** (in this case we have a **multinomial logistic classification**).\n",
    "\n",
    "The logistic regression model works by transforming a linear combination of input features using a **logit function** (do not confuse with the **logistic or sigmoid function** which is the inverse) as follows:\n",
    "\n",
    "$$P(Y) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "where:\n",
    "- $P(Y)$ is the probability of the target (dependent) variable.\n",
    "- $e$ is the base of the natural logarithm.\n",
    "- $z$ is the linear combination of input features and their corresponding weights: $z = β₀ + β₁x₁ + β₂x₂ + ... + β_nx_n$.\n",
    "  \n",
    "Hence, the logit function maps any real-valued number to the range [0, 1], which is crucial for interpreting the output as a probability, and thus, assign each element to a given class based on the ratio of probabilities. Then, once the model is trained, we can use the inverse **logistic or sigmoid function** to predict the probability that a certain sample belongs to a particular class given the input features.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Logit function](https://i.stack.imgur.com/WY61Z.png)](https://en.wikipedia.org/wiki/Logit)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "On this setting, the linear combination of input features is related with the logarithm of the **odds-ratio** (also known as **log-odds**) since from the logistic regression formula we can derive that:\n",
    "\n",
    "$$logit(P(Y)) = ln(\\frac{P(Y)}{1 - P(Y)}) = β₀ + β₁x₁ + β₂x₂ + ... + β_nx_n$$\n",
    "\n",
    "The **odds-ratio**, expressed as $\\frac{P(Y)}{1 - P(Y)}$, is the relationship of the probability of one event with respect to the opposite one (on a binary situation) and can be used as the association strength between two events. From the relative probabilities of belonging to once class vs others, a **unit step function or quantizer** chooses the class with the highest probability to provide that as output.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Logistic classificator scheme](images/Logistic.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "\n",
    "The **learning rule** here is the maximization of a **likelihood function** (from a simplified perspective, the algorithm finds the proper weights by maximizing a function that provides the **probability of observing the training data given the parameters of the model**, if you need more information look at the bibliography).\n",
    "\n",
    "**Advantages:**\n",
    "- Logistic regression provides **interpretable** results since the coefficients can be directly interpreted in terms of changes in log odds: it provides with probabilities during the classification.\n",
    "- Training logistic regression models is computationally **efficient** and scales well to large datasets.\n",
    "\n",
    "**Disadvantages:**\n",
    "- A critical assumption is the **absence of extreme outliers** in the dataset, which distort the training (could be verified by calculating Cook’s distance (Di) to identify influential data points that may negatively affect the regression model).\n",
    "- Logistic regression **assumes a linear relationship between the log-odds and the independent variables**, which translated into a linear decision boundary. This might not be suitable for datasets with complex, non-linear relationships between features (better use support vector machines or decision tress and derivates).\n",
    "- Assumes little to no multicollinearity between explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0efe40",
   "metadata": {},
   "source": [
    "Let's see how it works for both blood and kidney cancer examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c3703572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the dependent and independent variables\n",
    "## The labels are the two types of blood cancers we want to separate, encoded as 0s and 1s.\n",
    "y = pd.DataFrame(blood_label_integers)\n",
    "## The two features are the two first PC, already on the 2D numpy array format (columns are the PC)\n",
    "X = pd.DataFrame(blood_proj_data)\n",
    "\n",
    "# The names of the classes, not encoded as 0s and 1s for plot purposes\n",
    "unique_labels = list(labels_blood.unique())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_regions(X.to_numpy(), y[0].to_numpy(), lr, 0.1, X_test.index.to_numpy(), unique_labels)\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Save image\n",
    "plt.savefig(path.join('plots', 'Logistic_blood.png'))\n",
    "plt.show()\n",
    "\n",
    "# We check the metrics in this case\n",
    "compute_evaluation_metrics(lr, X_test, y_test, unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8246242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dependent and independent variables\n",
    "## The labels are the two types of kidney cancers we want to separate, encoded as 0s, 1s and 2s.\n",
    "y = pd.DataFrame(kidney_label_integers)\n",
    "## The two features are the two first PC, already on the 2D numpy array format (columns are the PC)\n",
    "X = pd.DataFrame(kidney_proj_data)\n",
    "\n",
    "# The names of the classes, not encoded as 0s, 1s and 2s for plot purposes\n",
    "unique_labels = list(labels_kidney.unique())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_regions(X.to_numpy(), y[0].to_numpy(), lr, 0.1, X_test.index.to_numpy(), unique_labels)\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Save image\n",
    "plt.savefig(path.join('plots', 'Logistic_blood.png'))\n",
    "plt.show()\n",
    "\n",
    "# We check the metrics in this case\n",
    "compute_evaluation_metrics(lr, X_test, y_test, unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d135ae81",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)\n",
    "\n",
    "Support Vector Machines (SVM) are powerful supervised learning models used for classification and regression tasks. SVM aims to find the optimal hyperplane that separates data points of different classes in feature space. The hyperplane is defined by:\n",
    "\n",
    "$$f(x) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$$\n",
    "\n",
    "where:\n",
    "- $f(x)$ is the decision function.\n",
    "- $\\beta_0, \\beta_1, \\beta_2, ..., \\beta_n$ are the coefficients (weights) to be learned.\n",
    "- $x_1, x_2, ..., x_n$ are the input features.\n",
    "\n",
    "In contrast with the perceptron, where the objective was to minimized misclassification errors, in SVMs the optimization objective is to **maximize the margin** (the **learning rule** of SVM), defined as the distance between the separating hyperplane (the decision boundary) and the training samples that are closest to this hyperplane, which are the so-called **support vectors** (the data points that lie closest to the decision boundary). This is illustrated in the following figure:\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Margin concept](images/Margin.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "The rationale behind having decision boundaries with large margins is that they tend to have a lower generalization error (the decision boundaries are less influenced by the training dataset) whereas models such as the perceptron with small margins are more prone to fall into **overitting** issues. However, as you might understand, this concept of **maximizing the margins** is therorethically suitable if there is an hyperplane that is able to separate all the training groups. But what about cases where that is impossible? How can we build a good enough model in this situation where margins are not that clear?\n",
    "\n",
    "To assess that the algorithm takes into account a concept known as **soft-margin classification**, where an internal extra parameter called **slack variable** (see the bibliography for mathematical details) allows to relax the constrains for nonlinearly separable data to allow convergence of the optimization in the presence of misclassifications under the appropriate cost penalization. This is controlled by an **hyperparameter of the model C** where increasing values of C increases the bias and lowers the variance of the model to adjust for the **bias-variance** tradeoff:\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Regularization parameter C](images/ParamC.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "\n",
    "**Advantages:**\n",
    "- Effective in high-dimensional spaces.\n",
    "- Can handle **linear and non-linear relationships** using different kernel functions.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Non-linear relationships](images/Kernel.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "- **Robust to overfitting**, especially in high-dimensional spaces. Thanks to the hyperparameter C.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Training is more **computationally expensive** can be high for large datasets.\n",
    "- SVMs are **sensitive to noise** present on the data. It is important to play with the hyperparameter C to adjust for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b7c94880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the dependent and independent variables\n",
    "## The labels are the two types of blood cancers we want to separate, encoded as 0s and 1s.\n",
    "y = pd.DataFrame(blood_label_integers)\n",
    "## The two features are the two first PC, already on the 2D numpy array format (columns are the PC)\n",
    "X = pd.DataFrame(blood_proj_data)\n",
    "\n",
    "# The names of the classes, not encoded as 0s and 1s for plot purposes\n",
    "unique_labels = list(labels_blood.unique())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_regions(X.to_numpy(), y[0].to_numpy(), svm, 0.1, X_test.index.to_numpy(), unique_labels)\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Save image\n",
    "plt.savefig(path.join('plots', 'SVM_c1_blood.png'))\n",
    "plt.show()\n",
    "\n",
    "# We check the metrics in this case\n",
    "compute_evaluation_metrics(svm, X_test, y_test, unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c6b7fb",
   "metadata": {},
   "source": [
    "Here the classifier works in a similar way to the simple perceptron that we trained at the start of this session. However, notice that the **decision boundary** now is optimized to maximize the margin between the two training sets and avoid overfitting. If we apply the same settings to train a model for the kidney cancer, where there is no clear linear separation, the performance is less optimal and more similar to the one obtained with the **logistic model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "20fa6752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dependent and independent variables\n",
    "## The labels are the two types of kidney cancers we want to separate, encoded as 0s, 1s and 2s.\n",
    "y = pd.DataFrame(kidney_label_integers)\n",
    "## The two features are the two first PC, already on the 2D numpy array format (columns are the PC)\n",
    "X = pd.DataFrame(kidney_proj_data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# The names of the classes, not encoded as 0s, 1s and 2s for plot purposes\n",
    "unique_labels = list(labels_kidney.unique())\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_regions(X.to_numpy(), y[0].to_numpy(), svm, 0.1, X_test.index.to_numpy(), unique_labels)\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Save image\n",
    "plt.savefig(path.join('plots', 'SVM_linear_kidney.png'))\n",
    "plt.show()\n",
    "\n",
    "# We check the metrics in this case\n",
    "compute_evaluation_metrics(svm, X_test, y_test, unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b227e48",
   "metadata": {},
   "source": [
    "However, unlike the **logistic model** which cannot work with non-linear decision boundaries, on **SVM** we can use the kernel trick to solve non-linear classifications. This is the key aspect why SVM enjoy high popularity among machine learning practitioners.\n",
    "\n",
    "To simplify, kernels can be interpreted as a similarity function between pairs of samples so we can model an \"extra non-existent dimension\" to allow for non-linear boundaries. One of the most popular **kernel functions** is the **Radial Basis Function (RBF) or Gaussian kernel**, which is the one we are going to implement in our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5e8f262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's simply change the kernel from linear to rbf\n",
    "# We will also play with the C parameter to try to fit the data\n",
    "svm = SVC(kernel='rbf', C=80, random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_regions(X.to_numpy(), y[0].to_numpy(), svm, 0.1, X_test.index.to_numpy(), unique_labels)\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Save image\n",
    "plt.savefig(path.join('plots', 'SVM_rbf_kidney.png'))\n",
    "plt.show()\n",
    "\n",
    "# We check the metrics in this case\n",
    "compute_evaluation_metrics(svm, X_test, y_test, unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4564ab26",
   "metadata": {},
   "source": [
    "For instance with a C hyperparameter of 80 we are forcing the **SVM** to try to fit better the model that tries to classify class 2 (the most difficult one to separate) even if we are incurring into large levels of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f892e3a",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "To understand the Random Forest, first we need to understand the **Decision Trees**: a classic tree-like model used for classification based on multiple \"decisions\" based on several independent variables.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Decision trees](https://miro.medium.com/v2/resize:fit:1280/0*4QE-0kavxXfzF_bR.png)](https://en.wikipedia.org/wiki/Decision_tree)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Since each of these decision trees is considered a model per se, a **Random forest** which is a model arising from the combination of multiple decision trees, it is an **ensemble method**, that is, a machine learning model composed of multiple small models with the idea of outperforming the accuracy obtained from individual models alone.\n",
    "\n",
    "Therefore, the algorithm behind **Random forest** models involves the recursive splitting of the dataset based on the features that best separate the data into distinct classes or groups. The goal is to create decision rules that efficiently partition the data by randomly sampling of the features that best separate the data into distinct classes or groups. Therefore, the steps for a random forest classifier are:\n",
    "\n",
    "1. **Build Decision Trees:** Create multiple decision trees using bootstrapped samples and random subsets of features. Hence, each building block of the random forest is trained on a random subset of the training data and features, allowing for diversity.\n",
    "\n",
    "2. **Aggregate Predictions:** Combine the predictions of individual trees through voting or averaging.\n",
    "\n",
    "3. **Model Evaluation:** Assess the model's performance using metrics like accuracy or F1 score.\n",
    "\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Random forest classifier](https://www.freecodecamp.org/news/content/images/2020/08/how-random-forest-classifier-work.PNG)](https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Random forest **can handle both classification and regression taks**. The **learning rule** of this classifier is to maximize the \"discriminative power\" for classification taks and the error distance for regression tasks. This is commonly evaluated with the **Gini impurity** metric for classification tasks or the **mean squared error** for regression tasks, although there are many other metrics such as the **entropy level** or the **misclassification error**. Let's take a closer look at what the **Gini impurity** (and other metrics) reflect.\n",
    "\n",
    "The **Gini impurity** or **Gini index** is a measure of the uncertainty at each split points (the nodes) of a decision tree. That is, **measures the likelihood of misclassifying a randomly chosen element from the set**. Mathematically, it is calculated for the set $D$ to be splited as follows:\n",
    "\n",
    "$$Gini(D) = 1 - \\sum_{i=1}^{K} (p_i)^2$$\n",
    "\n",
    "where:\n",
    "- $K$ is the number of classes in the dataset.\n",
    "- $p_i$ is the probability of randomly picking an element of class $i$ from the set $D$.\n",
    "\n",
    "This measure ranges between 0 and 1, where lower values indicating a purer or less impure dataset. Notice that Gini impurity will be 0 if the set contains only elements of a single class (and hence, there is no need to discriminate), whereas 1 indicates that all classes are equally represented.\n",
    "\n",
    "In the context of decision trees, the algorithm uses the Gini impurity to find the splits that minimizes the Gini impurity across the resulting child nodes, that means, whose child nodes are pure of one of the classes we want to separate. Hence, the split that leads to the lowest Gini impurity is chosen as the best split through an iterative work until the decision tree is completed.\n",
    "\n",
    "**Advantages:**\n",
    "- Random Forests reduce are **robust to overfitting** by averaging the predictions of multiple trees.\n",
    "- Capable of **capturing complex, non-linear relationships** in the data.\n",
    "- Provides a measure of feature importance such as the **Gini impurity**.\n",
    "- There is no need to pre-process the data in terms of feature scaling such as standarization.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Random Forests **cannot be easily interpreted**, often considered as \"black-box\" models\n",
    "- Can be **computationally expensive** for large datasets and many trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e328c846",
   "metadata": {},
   "source": [
    "For the following examples instead of the our multiomics dataset, we will work with a mock dataset that is already present on the scikit-learn package, which will be better to exemplify the largest power of decision trees: working with high number of features and evaluate the importance of each of them in the classification power. This is the dataset that will be used for the first deadline of delivery exercises. First we will build a simple decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "51b555c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "# Continous independent variables\n",
    "X = pd.DataFrame(iris['data'])\n",
    "# Labels or dependent variable (discrete classes)\n",
    "y = pd.DataFrame(iris['target'])\n",
    "\n",
    "print(iris.keys())\n",
    "print('')\n",
    "print('feature names:', iris['feature_names'])\n",
    "print('target names:', iris['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b8188",
   "metadata": {},
   "source": [
    "Until here, we have seen many machine learning algorithms that require to preprocess the data (we have been using **standarization** if our original data had features that were not already scaled), however, one of the main advantatges of decision trees and random forests is that, as non-parametric methods, we do not need to worry about scaling features.\n",
    "\n",
    "We are going to train a decision tree of maximum of three nodes (parameter max_depth on 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5a291e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# We can skip the standarization step\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plot_tree(decision_tree, fontsize=12, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd036b",
   "metadata": {},
   "source": [
    "The plot tree function provides with an encoded decision tree, which is translated into an understandable tree plot. As we can see, the first decision, that the petal length (in cm) is less or equal than 2.45 (x[2] means the third variable, remember that python indexes start at 0) is able to separate all the 40 setosa samples from the other 41 versicolor and 39 virginica. Note how the gini impurity index is lower on the next nodes, since they are less discriminative that the nodes above. Sadly, we only three nodes we are not able to fully separate all the remaining versicolor and virginica flowers.\n",
    "\n",
    "We can display the decision boundaries, however, since now we are working with 4 features, we will need to use another function than the one generated here. If you want more information on that, please see nthis scikit-learn documentation webpage (https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html).\n",
    "\n",
    "What we can plot is the confusion matrix and the metrics for our three classes classificator with the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9c0a857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check the metrics in this case\n",
    "compute_evaluation_metrics(decision_tree, X_test, y_test, iris['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c163a4e",
   "metadata": {},
   "source": [
    "Interestingly, although for the training set our classifier is unable to classify all samples correctly, it does a perfect classification for the test dataset (hence, the \n",
    "\n",
    "Okay, now instead of an individual decision tree we will train a random forest classifier with the same dataset. As a non-parametric method, we do not need to adjust the hyperparameters as much as when dealing with **SVM**. In fact, **scikit-learn** already optimizes the size **n** (chosen to be equal to the number of samples in the original training set) of the bootstrap sample and the number of features **d** (**scikit-learn** chooses $d=\\sqrt{m}$ where $m$ is the number of features at the training set) that is randomly chosen for each iteration.\n",
    "\n",
    "Through **n** we control the bias-variance tradeoff of the random forest:\n",
    "- For larger values for **n** we decrease the randomness and thus the forest is more likely to overfit whereas we can reduce the degree of overitting by choosing smaller **n** values at the expense of the model performance.\n",
    "- The optimal for **d** is just a smaller value than the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "86ac4cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# We check the metrics in this case\n",
    "compute_evaluation_metrics(decision_tree, X_test, y_test, iris['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d0f9b0",
   "metadata": {},
   "source": [
    "In the same way as the decision tree, the random forest that we trained is able to properly separate the three flower classes.\n",
    "\n",
    "One of the interesting features of the random forest classifier is that, as an **embedded method** (remember the **feature selection** theory), by building random decision trees from the data the ensemble model is able to better grasp the relative importance of each of the features in the classification of the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "16cf5884",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris['feature_names'])\n",
    "print('feature importances:', random_forest.feature_importances_)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.bar(iris['feature_names'], random_forest.feature_importances_)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel('Relative feature importance', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b209ca",
   "metadata": {},
   "source": [
    "We can see that to classify flowers the features with the highest relative importance is the petal lengh and width (the two features of the peatals, rather than the information from the sepals)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325bd95",
   "metadata": {},
   "source": [
    "# Advanced supervised methods: neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e70a4c",
   "metadata": {},
   "source": [
    "Neural networks are powerful group of supervised machine learning models that inspired by the human brain's neural structure.\n",
    "\n",
    "Within the core of Neural Networks lies the **perceptron**, which serves as their basic building block. As we have seen, the perceptron is a binary classifier that takes multiple input features, applies weights, sums them up, and passes the result through an activation function to produce an output. However, we have seen that perceptrons have severe limitations, especially when it comes to handling non-linear relationships in data.\n",
    "\n",
    "To address the limitations of perceptrons, the **multi-layer perceptron (MLP)** was introduced. MLP consists of multiple layers of interconnected perceptrons, including an input layer, one hidden layer and an output layer. Each layer introduces non-linearity through activation functions, allowing the model to learn complex mappings between inputs and outputs.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Multi-layered perceptron](https://www.allaboutcircuits.com/uploads/articles/an-introduction-to-training-theory-for-neural-networks_rk_aac_image2.jpg)](https://www.allaboutcircuits.com/technical-articles/how-to-train-a-multilayer-perceptron-neural-network/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "How MLP are able to solve non-linearly separable problems is perfectly exemplified by the **XOR problem**, a classic challenge in artificial intelligence and machine learning. The XOR gate, or exclusive OR, is a logical operation that returns true (1) only when the number of true inputs is odd, and false (0) when the inputs are the same:\n",
    "\n",
    "- $0 \\oplus 0 = 0$\n",
    "- $0 \\oplus 1 = 1$\n",
    "- $1 \\oplus 0 = 1$\n",
    "- $1 \\oplus 1 = 0$\n",
    "\n",
    "The difficulty with the XOR problem lies in finding a linear decision boundary that separates the two classes (true and false) in a 2D space. A single-layer perceptron, being a linear classifier, is unable to solve the XOR problem as it can only create linear decision boundaries. Hence, to successfully solve the XOR problem, a more complex model with non-linear activation functions and multiple layers, such as a MLP or other types of neural networks are required, where the hidden layers allow to capture the non-linear relationships between input features.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![XOR problem](https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/04/bitwise_datasets.png)](https://pyimagesearch.com/2021/05/06/implementing-the-perceptron-neural-network-with-python/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "If you want a mathematically detailed explanation on how a simple MPL can solve this problem, see this video: https://youtu.be/dM_8Y41EgsY?si=BmcvwLx-qQCvO8Fd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415b2975",
   "metadata": {},
   "source": [
    "Beyond the simple **MLP** explained before there are other types of more complex artificial neural networks, which include **more than one hidden layer**: the so-called **deep neural networks (DNNs)** that are studied within the **deep learning subfield of machine learning**. There are basically two types of neural networks depending on the direction of information propagation: **feed-forward neural networks (FNNs)**, where the information is propagated unidirectionally, and **Recurrent Neural Networks (RNNs)** where the information flows bidirectionally across its layers.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Types of DNNs](https://www.researchgate.net/profile/Engin-Pekel/publication/315111480/figure/fig1/AS:472548166115333@1489675670530/Feed-forward-and-recurrent-ANN-architecture.png)](https://www.researchgate.net/figure/Feed-forward-and-recurrent-ANN-architecture_fig1_315111480)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "This distiction is relevant in terms of how to the DNNs are trained. For instance, the most common method for training FNNs is **backpropagation** and its modifications (RNNs, on the other hand, use an slightly modified algorithm that fits the particularities of these type of neural networks). In general terms, the iterative process involves three steps:\n",
    "\n",
    "1. **Forward Pass**: input data is passed through the network layer by layer, with each layer performing a computation based on the inputs it receives and passing the result to the next layer.\n",
    "\n",
    "    - Input values from the training dataset are fed into the neural network through the **input layer**, where each feature corresponds to a node. Hence, the number of neurons in the input layer is determined by the dimensionality of the input data.\n",
    "    - This input layer is then feeded to the **first hidden layer**, which process the input information as a weighted sum of inputs from the previous layer and then applies an activation function (remember that each neuron is in fact a perceptron) to generate the output of this layer, which is in turn passed to the next hidden layer (the **second hidden layer**) until the **final hidden layer** (it can also be the second if the DNN only has two hidden layers). \n",
    "    - The output of the final hidden layer is then passed into the **output layer**. These neurons also apply an activation function to the weighted sum of inputs (commonly a **softmax activation function** for classification tasks) to obtain the **predicted output**. At the end, the predicted output is compared with the actual values (the labels, remember that we are training a supervised method) through modelling of a **loss function**: a mathematical relationship that allows to model the error of the prediction, for example the **root of mean squared errors (RMSE)** for regression tasks.  \n",
    "\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "2. **Backward Pass (*stricto sensu* Backpropagation)**: By means of the **loss function**, and moving backwards from the output layer, the gradients of the loss (\"the change in the error\") is computed for each neuron in the hidden layers. This is performed though the **chain rule of calculus**, which allows to decomposes the gradient of the overall loss with respect to the weights for each layer.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "3. **Weight Update (Gradient Descent)**: The gradients are used to update the weights in the direction that reduces the loss. The **learning rule** here is to minimize the **loss function** by exploring the space of weights. As we saw for the perceptron, the **learning rate** is a key hyperparameter that will determine the size of the steps taken each iteration.\n",
    "\n",
    "This process of subsequent forward and backward passes is performed for a specified number of iterations until the algorithm reaches convergence or until the user stops the training.\n",
    "\n",
    "![Backpropagation](https://i.gifer.com/6fyP.gif)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "        \n",
    "There are a few concepts mentioned on the explanation of the algorithm that need extra discussion, some of them covered in less depth while introducing the perceptron:\n",
    "\n",
    "- **Activation functions**: we introduced the **unit or binary step** for our perceptron example, however there are multiple activation functions that could be used to learn specific complex patterns, usually combined across layers of a neural network arquitecture. Common activation functions include the **sigmoid**, **tanh (hyperbolic tangent)**, **ReLU (Rectified Linear Unit)**, and the **softmax**.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Activation functions](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/627d12431fbd5e61913b7423_60be4975a399c635d06ea853_hero_image_activation_func_dark.png)](https://www.v7labs.com/blog/neural-networks-activation-functions)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "- **Loss Function**: The choice of the loss function will depend on the task at hand (e.g., mean squared error for regression, cross-entropy for classification,...), however, all of them quantify the difference between predicted and actual values and allows to find the gradient of the loss at each neuron through the **chain rule of calculus**.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "- **Learning Rate**: The learning rate, as the hyperparameter that controls the step size during weight updates, is key on the speed and stability of the training process. \n",
    "\n",
    "\n",
    "*Small learning rates have the advantatge of approximating the minimum of the loss function better than large learning rates which could fail to closely approximate the minimum.*\n",
    "\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Small learning rate](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*JbiGghtQBtiaHYVdP_hprQ.gif)](https://medium.com/codex/introduction-to-how-an-multilayer-perceptron-works-but-without-complicated-math-a423979897ac)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Large learning rate](https://miro.medium.com/v2/resize:fit:640/format:webp/1*P_Ll2G6WSFy-1STbYPLTBQ.gif)](https://medium.com/codex/introduction-to-how-an-multilayer-perceptron-works-but-without-complicated-math-a423979897ac)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "*However, small learning rates can also get stuck in local mimima. That is why it is interesting to combine large learning rates to approximately scan the loss function lansdcape and then use small learning rates to finally find the global minimum.*\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Local minima problem](https://miro.medium.com/v2/resize:fit:720/format:webp/1*Rh387pxwuU9Owa0QCuN4Yw.gif)](https://medium.com/codex/introduction-to-how-an-multilayer-perceptron-works-but-without-complicated-math-a423979897ac)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    " - **Mini-Batch Gradient Descent**: Commonly, and due to computational efficiency, instead of updating weights after processing the entire dataset (a batch) on each iteration, weights are updated after processing a small subset (mini-batch) of the data. Here it is important to introduce the concept of **epoch**: one complete pass of the entire training dataset through the learning algorithm.\n",
    " \n",
    "*If we decide to pass the entire dataset, an epoch corresponds to one interation. This is computationally inneficient but the algorithm can find an stay on a loss minimum.*\n",
    " \n",
    " <!-- Add an empty line here -->\n",
    "\n",
    "[![Entire batch](https://miro.medium.com/v2/resize:fit:720/format:webp/1*fbESYSVwSqcGFvRJWGPZMQ.png)](https://medium.com/codex/introduction-to-how-an-multilayer-perceptron-works-but-without-complicated-math-a423979897ac)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    " \n",
    "*However, if we pass a Mini-Batch of, for instance 25% of the entire dataset, we will need 4 iterations of the algorithm for an entire epoch. It will be computationally more efficient but, since each mini-batch is a different data subset, the algorithm doesn’t settle down to a minimum point.*\n",
    "\n",
    " <!-- Add an empty line here -->\n",
    "\n",
    "[![Mini-batch](https://miro.medium.com/v2/resize:fit:720/format:webp/1*25NS5rOg8kESmH-xy78Cvg.png)](https://medium.com/codex/introduction-to-how-an-multilayer-perceptron-works-but-without-complicated-math-a423979897ac)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "\n",
    "After this review on general concepts, we can further explore types of DNNs based on the architecture, the interconexion of the different layers, and find at which kind of problem they excel. Common architectures are **Dense or fully connected Neural Networks** and **Convolutional Neural Networks (CNNs)**, with respect to the FNNs, or the different subtypes of **Recurrent Neural Networks (RNNs)**. On this course we will only explore Dense Neural Networks and CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcecbd8",
   "metadata": {},
   "source": [
    "## Dense Neural Networks\n",
    "\n",
    "The simplest form of a deep neural networks with information flowing in one direction is the one with fully connected hidden layers: a **dense neural network**. They are usually employed for basic classification and regression tasks.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Multi-layered perceptron](https://miro.medium.com/v2/resize:fit:640/format:webp/1*4_BDTvgB6WoYVXyxO8lDGA.png)](https://medium.com/codex/introduction-to-how-an-multilayer-perceptron-works-but-without-complicated-math-a423979897ac)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "\n",
    "**Advantages:**\n",
    "   - Dense neural networks are very **versatile** and can be applied to a wide range of data and tasks, including classification and regression, with a straightforward architecture of input, hidden, and output layers.\n",
    "\n",
    "**Disadvantages:**\n",
    " - Dense neural networks do **not inherently understand spatial relationships** in data. That is why they are less effective for tasks where spatial information is crucial, such as image processing.\n",
    "- Moreover, FNNs are prone to **overfitting**, especially with large numbers of parameters. Hence, regularization techniques may be required to mitigate this effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72780977",
   "metadata": {},
   "source": [
    "We are going to train a dense neural network as our first hands-on exercise. For that purpose we are going to use the **keras** package, which is an open-source and user-friendly high-level neural networks API written in Python.\n",
    "\n",
    "The advantatge of this package is that is compatible with various backends (it can run control different numerical computation libraries), for instance, **TensorFlow** where it is integrated as its official high-level API. **TensorFlow** (developed by *Google*) is, together with **PyTorch** (developed by *Meta*) the most popular software for deep learning.\n",
    "\n",
    "If you are interested, you can interactively play online with it at https://playground.tensorflow.org for simple classification tasks to understand the basic ideas behind deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3abe2e",
   "metadata": {},
   "source": [
    "We will try to classify all the different types of primary tumors based on transcriptomic information. Remember that we were only able to separate the two blood types of cancers and roughly some types of kidney tumors while using simple supervised methods on each primary cancer type location alone!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "56056af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "# To ignore some plot warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We load the expression data which will be the features X array (needs to be transposed)\n",
    "expression_df = pd.read_csv(path.join('data', 'gene_expression.tsv.gz'),\n",
    "                                                        sep=\"\\t\", header='infer', index_col=0, compression='gzip')\n",
    "X = expression_df.to_numpy().T\n",
    "\n",
    "# Get sample dataframe with the information\n",
    "sample_df = pd.read_csv(path.join('data', 'sample_df.tsv'), sep=\"\\t\", header='infer')\n",
    "\n",
    "# Finally we load directly from the ICGC database the histology excel file\n",
    "histology_df = pd.read_excel('https://dcc.icgc.org/api/v1/download?fn=/PCAWG/clinical_and_histology/pcawg_specimen_histology_August2016_v9.xlsx')\n",
    "histology_df.columns = ['icgc_specimen_id'] + list(histology_df.columns[1:])\n",
    "\n",
    "# Generate a dictionary to translate the Specimen IDs to the histological subtype and get the labels y\n",
    "tumortype_dict = dict(zip(histology_df.icgc_specimen_id, histology_df.histology_abbreviation))\n",
    "cancer_types = list(map(lambda x: tumortype_dict[x], expression_df.columns))\n",
    "labels_dict = {gene_id: i for i, gene_id in enumerate(set(cancer_types))}\n",
    "y = np.array(list(map(lambda x: labels_dict[x], cancer_types)))\n",
    "labels_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c4f407-8cd8-4c53-baa0-c869a12a08da",
   "metadata": {},
   "source": [
    "We will do the train/test split with 25% of the samples in the test split. Notice that we sample the train and test dataset with stratification based on the labels (the different tumor histological types) to ensure that both datasets have the same distribution across classes.\n",
    "\n",
    "We will need to encode the labels into dummy vectors (known as **one-hot encoding**).\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![One-hot encoder](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*ggtP4a5YaRx6l09KQaYOnw.png)](https://towardsdatascience.com/building-a-one-hot-encoding-layer-with-tensorflow-f907d686bf39)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "\n",
    "**Keras** has a utility that allows to easily convert the histological labels into vectors that encode the output category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "07c1bc1a-78eb-4a37-b3b1-dbd1a9bb7b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y)\n",
    "\n",
    "# Standarization step (note that we scale with the same fit of the train to the train and the test).\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc6622a-ec33-4a44-bf26-a98da622ed34",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will use a PCA as a way to reduce the dimensionality and select less features: we will take the first 30 principal components as input features, which are linear combinations of the original expression of more than 20,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "15622bc1-f5db-4c60-87b9-7b5df68d2fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# We fit the PCA using the training set, and then we project both training and test set\n",
    "pca = PCA(n_components = 30)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58ce56-4c13-4ac0-a723-b1fe262eb9ce",
   "metadata": {},
   "source": [
    "Now we can proceed to specify with the **keras** interface the architecture of our fully connected neural network. As the activation function on the hidden layers we will use the **sigmoid function**:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(x) = \\frac{1}{1 + \\exp{-x}}\n",
    "\\end{equation}\n",
    "\n",
    "One of the advantatges of this function is that the derivative is very convenient, hence its simplicity when applying the **chain rule of calculus** during backpropagation.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d\\sigma(x)}{dx} = (1 - \\sigma(x))·\\sigma(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927f6887",
   "metadata": {},
   "source": [
    "The activator function that we are going to use on the output layers is the **softmax function**. It converts a vector of raw numbers or scores that are inputed from the hiddern layers into a vector of probabilities that are proportional of the relative scale of each value in the input vector. Given a vector $z = [z_1, z_2, ..., z_C]$ representing the raw scores for $C$ classes, the softmax function is defined as:\n",
    "\n",
    "$$\\text{Softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}$$\n",
    "\n",
    "for each element $i$ in the range from 1 to $C$. Mathematically, this function exponentiates the raw input scores and normalizes them by the sum of the exponentiated scores thus ensuring that the resulting values lie in the range [0, 1] and sum to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36947d",
   "metadata": {},
   "source": [
    "With respect to the **loss function**, given the classification task at hand, we will use the **cross-entropy function** (a.k.a **log-loss function**) for the loss modelling. For a single training with $C$ classes the formula is:\n",
    "\n",
    "$$H(y, p) = - \\sum_{i=1}^{C} y_i \\cdot \\log(p_i)$$\n",
    "\n",
    "where:\n",
    "- $y_i$ is the true label (ground truth) for class $i$.\n",
    "- $p_i$ is the predicted probability for class $i$, obtained from the softmax fucntion activation in the output layer of the neural network.\n",
    "\n",
    "Note that this mathematical formula ensures the following properties:\n",
    "\n",
    "- The contribution to the loss is determined by the negative logarithm of the predicted probability (\\(p_i\\)) for the considered class.\n",
    "- The loss is minimized when the predicted probabilities align with the true distribution of class labels.\n",
    "- Probability predictions that diverge more from the true label are more heavily penalized than predictions that are close to the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "56d0f977-06eb-4d8e-ad13-045aefec7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# The shape of the input (the 30 Principal components, features that we are going to input)\n",
    "input_shape = X_train.shape[-1]\n",
    "# The amount of intermediate neurons we want on the hidden layers\n",
    "n_intermediate = 64\n",
    "# And the size of the vector we want as output (the probabilities of belonging to the 27 histological types)\n",
    "n_classes = y_train.shape[-1]\n",
    "\n",
    "# We define a networks made of sequential layers of neurons\n",
    "model = Sequential()\n",
    "## Two hidden layers densely connected of (the first one with an input of the shape of the 30 PCA values)\n",
    "## Note that we use the sigmoid activation function\n",
    "model.add(Dense(n_intermediate, activation='sigmoid', input_shape=(X_train.shape[-1],)))\n",
    "model.add(Dense(n_intermediate, activation='sigmoid'))\n",
    "## An output layer with the softmax function\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# Show a summary of the model with all the parameters (weights) to train\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dde599d6-9574-4164-b2bb-8e4c781ab15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "# Compile the model with categorical crossentropy as the loss function and with a 5% of learning rate\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    # As the numerical method to minimize the loss function we will use the Stochastic gradient descent (SGD)\n",
    "    optimizer=SGD(learning_rate=0.05),\n",
    "    ## Accuracy, the ratio of correctly predicted observations to the total, as the evaluation metric\n",
    "    metrics=[\"accuracy\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7deb0ae5-4332-4bcb-8bd3-b75d9c8e3688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we train the model\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    # 500 epoch, i.e. 500 times we pass the entire training dataset\n",
    "    epochs=500,\n",
    "    # We will pass the input on each iteration in batches of 50 samples\n",
    "    batch_size=50, \n",
    "    verbose=1,\n",
    "    # The test dataset will be used for evaluation purposes\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb8cc8a",
   "metadata": {},
   "source": [
    "Ok, now it is time to interpret the output of the training process. Basically, there has been an update of the weights of all the connections at our neural network for **500 epochs** (500 times that the training dataset has been passed), which corresponds to **19 training iterations/epoch** given the batch size of **50 samples per iteration**. For each epoch, the **loss value** (the measure of the error on the classification) is computed together with the **accuracy** of the method. These are measures using the training dataset, while **val_loss** and **val_acc** are being computed while passing the **test_dataset** at the current training state of the neural network. \n",
    "\n",
    "Since it is hard to see how all the metrics change with time with a flat text, we can define a function to plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1d4ad985-8745-473c-b684-7fcf66f09b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_curves(history):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    epochs = range(len(history.history['loss']))\n",
    "\n",
    "    plt.plot(epochs, loss, label='training_loss')\n",
    "    plt.plot(epochs, val_loss, label='val_loss')\n",
    "    plt.title('loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, accuracy, label='training_accuracy')\n",
    "    plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2a8e1b90-a3ce-48e9-a973-473bc27329ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bab1d4",
   "metadata": {},
   "source": [
    "What can we observe? Note that:\n",
    "\n",
    "- At the start both training and validation loss values are high but they both shrink as the training continues. The test loss finnally reaches a value and becomes stable while the one from the training dataset keeps being minimized although slowly.\n",
    "\n",
    "- Analogously, at the start both training and validation accuracies are close to what we expect from a random classifier (probability of 1/27 classes) but they both improves greatly as the training continues. The training accuracy at some point keeps improving slowly while the one at the test dataset reaches a plateau.\n",
    "\n",
    "What can we say about the training process from these two plots? Remember that:\n",
    "\n",
    "- **Underfitting** will be recognizable if training and validation losses remained high during the training process, indicating the model fails to capture patterns in the training data and, hence, also performs poorly on unseen data.\n",
    "\n",
    "- **Overfitting** is observed when low training loss are achieved but validation loss remains high, which suggests that the model has learned the training data too well, including its noise, and fails to generalize to new data.\n",
    "\n",
    "- The desired scenario, a **properly fitted model** on the best bias-variance trade-off, is observed when both training and validation losses decrease and converge, indicating a model that generalizes well to new data.\n",
    "\n",
    "\n",
    "Considering that training loss continues to increase (same with accuracy on decrease) while validation loss and accuracy plateaus, we can see that beyond 100 and few epochs the model starts to enter into the overfitting dynamic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a1fc5",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs are designed to exploit the spatial structure of data through extracting local connectivity patterns and detecting patterns independently of their location in the input data. That is why they are mostly used for working with structured data such as images (or even continous stream of images such as videos); from a computational point of view images are 2D matrixes of values for black and white images or 3D matrixes in the case of colour images, encoded for the **Red, Green and Blue (RGB)** codes.\n",
    "   \n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![MNIST image](https://www.researchgate.net/profile/Soha-Boroojerdi/publication/361444345/figure/fig1/AS:11431281091155908@1666326170916/Representation-of-value-three-in-the-MNIST-dataset-and-its-equivalent-matrix.ppm)](https://www.researchgate.net/figure/Representation-of-value-three-in-the-MNIST-dataset-and-its-equivalent-matrix_fig1_361444345)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![RGB encoding](https://e2eml.school/images/image_processing/three_d_array.png)](https://e2eml.school/convert_rgb_to_grayscale.html)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Perhaps the best way to understand how CNNs work is to summarize some concepts on photography edition. Let's download an image and apply some photographic **filters**. From a computational point of view, applying filters to images involves multiplying the image matrixes with other matrixes, for instance, to remove the red colours.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Filter](https://i0.wp.com/cdn.makezine.com/uploads/2011/03/filter_example.png)\n",
    "\n",
    "<!-- Add an empty line here -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0bcfb087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# These are python libraries to download and process images\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# URL of the image\n",
    "image_url = \"https://www.uvic.cat/sites/default/files/styles/image_960x650/public/sites/all/images/uvic-_retorn_2021_0.jpg\"\n",
    "\n",
    "# Download the image and get the underlying array\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "image_array = np.array(image)\n",
    "\n",
    "# This image can be ploted with the imshow method of matplotlib\n",
    "plt.imshow(image_array)\n",
    "plt.title(\"Uvic image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7088cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can apply several filters\n",
    "\n",
    "## Remove red\n",
    "remove_red_filter = np.array([[0,0,0],\n",
    "                              [0,1,0],\n",
    "                              [0,0,1]])\n",
    "\n",
    "### Apply the filter to the image\n",
    "no_red_image = np.dot(image_array, remove_red_filter)\n",
    "\n",
    "plt.imshow(no_red_image)\n",
    "plt.title(\"Uvic image without red\")\n",
    "plt.show()\n",
    "\n",
    "## Get only green channel\n",
    "green_only_filter = np.array([[0,0,0],\n",
    "                              [0,1,0],\n",
    "                              [0,0,0]])\n",
    "\n",
    "### Apply the filter to the image\n",
    "green_only_image = np.dot(image_array, green_only_filter)\n",
    "\n",
    "plt.imshow(green_only_image)\n",
    "plt.title(\"Uvic image only green\")\n",
    "plt.show()\n",
    "\n",
    "## Sepia filter\n",
    "sepia_filter = np.array([[.393, .769, .189],\n",
    "                         [.349, .686, .168],\n",
    "                         [.272, .534, .131]])\n",
    "\n",
    "### Apply the filter to the image\n",
    "sepia_image = np.dot(image_array, sepia_filter.T)\n",
    "\n",
    "### Unfortunately the filter lines do not have unit sum, so we need to rescale\n",
    "sepia_image /= sepia_image.max()\n",
    "\n",
    "plt.imshow(sepia_image)\n",
    "plt.title(\"Uvic image but old\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c091f870",
   "metadata": {},
   "source": [
    "The basic concept behind CNNs are convolution operations, which in fact are matrix multiplications applied not once to the entire matrix but as sliding window across the image and the result is summed an outputed as an element of a new but smaller matrix.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Convolution](https://dennybritz.com/img/Convolution_schematic.gif)](https://dennybritz.com/posts/wildml/understanding-convolutional-neural-networks-for-nlp/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "The matrix that is applied on the sliding window is called a **kernel, filter, or feature detector** and performs a task of extracting information of the matrix and summarizes it on a new matrix. Let's try two convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "951fd505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this package from scipy\n",
    "from scipy.signal import convolve\n",
    "\n",
    "# Extract edges\n",
    "edges_kernel = np.array([[0,1,0],\n",
    "                        [1,-4,1],\n",
    "                        [0,1,0]])\n",
    "# Make it a 3D image\n",
    "edges_kernel = edges_kernel[:, :, None]\n",
    "\n",
    "### Apply the kernel to the image\n",
    "edges_image = convolve(image_array, edges_kernel)\n",
    "\n",
    "plt.imshow(edges_image)\n",
    "plt.title(\"UVic image but edges\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d4b1d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "\n",
    "# We will need to define some functions before\n",
    "def multi_convolver(image, kernel, iterations):\n",
    "    for i in range(iterations):\n",
    "        image = convolve2d(image, kernel, 'same', boundary = 'fill',\n",
    "                           fillvalue = 0)\n",
    "    return image\n",
    "\n",
    "def convolver_rgb(image, kernel, iterations=1):\n",
    "    convolved_image_r = multi_convolver(image[:,:,0], kernel,\n",
    "                                        iterations)\n",
    "    convolved_image_g = multi_convolver(image[:,:,1], kernel, \n",
    "                                        iterations)\n",
    "    convolved_image_b  = multi_convolver(image[:,:,2], kernel, \n",
    "                                         iterations)\n",
    "    \n",
    "    reformed_image = np.dstack((np.rint(abs(convolved_image_r)), \n",
    "                                np.rint(abs(convolved_image_g)), np.rint(abs(convolved_image_b)))) / 255\n",
    "   \n",
    "\n",
    "    return reformed_image\n",
    "\n",
    "\n",
    "\n",
    "## We will apply this blur kernel\n",
    "blur_kernel = (1 / 16.0) * np.array([[1., 2., 1.],\n",
    "                                  [2., 4., 2.],\n",
    "                                  [1., 2., 1.]])\n",
    "\n",
    "# We can apply the kernel iteratively to get an extra effect\n",
    "blur_output = convolver_rgb(image_array, blur_kernel, iterations = 1)\n",
    "plt.imshow(blur_output)\n",
    "plt.title(\"UVic image but slightly blur\")\n",
    "plt.show()\n",
    "\n",
    "blur_output2 = convolver_rgb(image_array, blur_kernel, iterations = 10)\n",
    "plt.imshow(blur_output2)\n",
    "plt.title(\"UVic image but more blur\")\n",
    "plt.show()\n",
    "\n",
    "blur_output3 = convolver_rgb(image_array, blur_kernel, iterations = 100)\n",
    "plt.imshow(blur_output3)\n",
    "plt.title(\"UVic image but super blur\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d4a301",
   "metadata": {},
   "source": [
    "Based on the same principle, CNNs apply convolutions and other types of operations combined into an architecture of layers with two main purposes:\n",
    "\n",
    "1. **Learn the features** of the images in a hierarchical way.\n",
    "\n",
    "    - This is achieved by the combination of **convolutional layers** that apply a given **kernel** and generate as output feature maps that summarize the large (and assumed as important) structural elements of the image. There are two **hyperparameters** associated with this procedure:\n",
    " \n",
    "\n",
    "        - **Padding** is the process of adding extra pixels (usually zero-valued) around the input image or feature map before applying convolutional operations in order to ensure that the size of the output image is the same as the input.\n",
    "\n",
    "        - **Stride** refers to the step size with which the convolutional filter moves across the input image (or feature map on subsequent layer) during the convolution operation. Hence, this hyperparameter controls how much reduction in the spatial dimensions of the feature map happens on each convolution.\n",
    "    \n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Convolution](https://miro.medium.com/v2/resize:fit:720/format:webp/1*O06nY1U7zoP4vE5AZEnxKA.gif)](\n",
    "https://medium.com/@Suraj_Yadav/in-depth-knowledge-of-convolutional-neural-networks-b4bfff8145ab)\n",
    "    \n",
    "<!-- Add an empty line here -->\n",
    "    \n",
    "   - Once the feature map is learned, this is passed into **pooling layers** that **downsample the feature maps** and reduce the size of the feature map by selecting the most relevant extracted on the previous layer and pass them to the next layer.\n",
    " \n",
    "<!-- Add an empty line here -->\n",
    " \n",
    "[![Pooling](https://miro.medium.com/v2/resize:fit:720/format:webp/1*vOxthD0FpBR6fJcpPxq6Hg.gif)](\n",
    "https://medium.com/@Suraj_Yadav/in-depth-knowledge-of-convolutional-neural-networks-b4bfff8145ab)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "   - Through diffent rounds of convolutional and pooling layers, the network can move from learning simple and low-level features such as edges and textures at the start to more complex and high-level features at the end.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "2. **Final classification** based on the learned features.\n",
    "\n",
    "    - Once the relevant information is extracted, a **dense or fully connected neuron layer** allows for the classification based on the information preproccessed in the previous layers.\n",
    "  \n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![CNN general structure](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*vkQ0hXDaQv57sALXAJquxA.jpeg)](\n",
    "https://medium.com/@Suraj_Yadav/in-depth-knowledge-of-convolutional-neural-networks-b4bfff8145ab)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "   \n",
    "\n",
    "**Advantages:**\n",
    "- CNNs are designed to **capture spatial hierarchies** through the different rounds of convolutional layers.\n",
    "- **Computationally efficient** due to the reduced number of parameters compared to a dense architecture.\n",
    "\n",
    "**Disadvantages:**\n",
    "- CNNs have a more **complex architecture** which implies a **difficult interpretation** of what are processing the last layers.\n",
    "- CNNs usually **require a large amount of labeled data** for training, especially for deep architectures, so data scarcity might lead to suboptimal performance.\n",
    "- CNNs may struggle to capture long-range dependencies in data as they focus on local structures, hence, they are **less effective for tasks where global context is crucial**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39292b1c-ef0e-4819-a2f6-eb1b967fccaf",
   "metadata": {},
   "source": [
    "For the hands-on exercise we will use a dataset of breast histology images that we will try to classify on having cancerous cells or not. Go to https://www.kaggle.com/code/paultimothymooney/predicting-idc-in-breast-cancer-histology-images and download the dataset (on the input tab). The link includes a description explaining the context and content of the dataset together with the code that the author provides to do an analysis (I deeply encourage you to look and play with the author's code, since he applies multiple types of classifiers that we have seen here and other news).\n",
    "\n",
    "We will use part of its code for the processing of the data, but we will build our own CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "008b8d71-b6d3-4ffe-a692-15732e15cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the array of images that are already in numpy format\n",
    "X = np.load(path.join('data', 'X.npy')) # images (features)\n",
    "Y = np.load(path.join('data', 'Y.npy')) # labels associated to images (0 = no IDC, 1 = IDC)\n",
    "\n",
    "# A summary of the dataset\n",
    "print('Total number of images: {}'.format(len(X)))\n",
    "print('Number of IDC(-) Images: {}'.format(np.sum(Y==0)))\n",
    "print('Number of IDC(+) Images: {}'.format(np.sum(Y==1)))\n",
    "print('Percentage of positive images: {:.2f}%'.format(100*np.mean(Y)))\n",
    "print('Image shape (Width, Height, Channels): {}'.format(X[0].shape)) # The images consist of 50 to 50 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d50f9b",
   "metadata": {},
   "source": [
    "Note that the images are small (50x50 pixels) since the dataset is already highly preprocessed (see documentation). This will help with the training on this hands-on exercise (it will largely reduce the computational time) but on a real situation multiple processing steps might apply.\n",
    "\n",
    "On the theoretical part we saw the general structure of CNNs, however, the amount of possible architectures is virtually infinite. There is no architecture that is optimal for all kind of problems and which specific architecture is best to use depends on the problem at hand. Moreover, this kind of decision usually involves **trial and error** with multiple candidates, as happens at the original implementation on Kaggle.\n",
    "\n",
    "Here we are going to try a very well-known architechture known as **LeNet-5**, which was already descibed by Yann Lecun in 1998. This CNN network has 5 layers (hence its name): 3 sets of convolution layers with a combination of average pooling, followed by 2 fully connected layers with a Softmax classifier. Our implementation is going to be more simple from the ones at Kaggle, as we will not use the RGB information and we will just work with greyscale images (we will loose part of the information from the tincture at the histology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d418effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the training and test dataset (20% on testing)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Note that the training dataset consists of 4437 images RGB of 50 x 50 pixels\n",
    "print(np.shape(X_train))\n",
    "# And the test of 1110 images\n",
    "print(np.shape(X_test))\n",
    "\n",
    "# We can print the first image of the training set\n",
    "plt.imshow(X_train[0])\n",
    "plt.show()\n",
    "\n",
    "# We reduce the dimensionality by working with greyscale images. The transformation is the following for our eyes:\n",
    "## Gray code = 0.299 * red + 0.5870 * green + 0.1140 * blue\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "    return gray\n",
    "\n",
    "# We apply the function to each image in a vectorized way\n",
    "## Create an empty array for grayscale images\n",
    "X_train_gray = np.zeros((np.shape(X_train)[0], np.shape(X_train)[1], np.shape(X_train)[2], 1))\n",
    "X_test_gray = np.zeros((np.shape(X_test)[0], np.shape(X_test)[1], np.shape(X_test)[2], 1))\n",
    "\n",
    "# Apply rgb2gray function to each image for the training set\n",
    "for i in range(np.shape(X_train)[0]):\n",
    "    X_train_gray[i, :, :, 0] = rgb2gray(X_train[i, :, :, :])\n",
    "    \n",
    "# Apply rgb2gray function to each image for conversion for the test set\n",
    "for i in range(np.shape(X_test)[0]):\n",
    "    X_test_gray[i, :, :, 0] = rgb2gray(X_test[i, :, :, :])\n",
    "    \n",
    "# We can print the first image to see the conversion into grey scale as how our human eyes will see\n",
    "plt.imshow(X_train_gray[0], cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e79e7f",
   "metadata": {},
   "source": [
    "Now let's decode the operations we will be performing in each layer:\n",
    "\n",
    "1. **Convolutional Layer (CONV1)**:\n",
    "\n",
    "    Parameters: Input (N) = 50, Padding (P) = 2, Kernel (K) = 5 x 5, Stride (S) = 1\n",
    "    Convolutional Operation: ((N + 2P - K) / S) + 1 = ((50 + 4 - 5) / 1) + 1 = 50 x 50\n",
    "    We will apply 12 filters / kernels so we will get a 50 x 50 x 12 dimensional output\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "2. **Average Pooling Layer (POOL1)**:\n",
    "\n",
    "    Parameters: Input (N) = 50, Padding (P) = 0, Kernel (K) = 2 x 2, Stride (S) = 2\n",
    "    AVG Pooling Operation: ((N - K) / S) + 1 = ((50 - 2) / 2) + 1 = 25 x 25\n",
    "    We will have a 25 x 25 x 12 dimensional output at the end of this pooling\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "3. **Convolutional Layer (CONV2)**:\n",
    "\n",
    "    Parameters: Input (N) = 25, Padding (P) = 2, Kernel (K) = 5 x 5, Stride (S) = 2\n",
    "    Convolutional Operation: ((N + 2P - K) / S) + 1 = ((25 + 4 - 5) / 2) + 1 = 13 x 13\n",
    "    We will apply 20 filters / kernels so we will get a 13 x 13 x 20 dimensional output\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "4. **Average Pooling Layer (POOL2)**:\n",
    "\n",
    "    Parameters: Input (N) = 13, Padding (P) = 0, Kernel (K) = 3 x 3, Stride (S) = 2\n",
    "    AVG Pooling Operation: ((N + 2P - K) / S) + 1 = ((13 - 3) / 2) + 1 = 6 x 6\n",
    "    We will have a 6 x 6 x 20 dimensional output at the end of this pooling\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "5. **Fully Connected layer (FC1)**:\n",
    "\n",
    "    Parameters: W: 720 * 100, b: 100\n",
    "    We will have an output of 100 x 1 dimension\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "6. **Fully Connected layer (FC2)**:\n",
    "\n",
    "    Parameters: W: 128 * 40, b: 40\n",
    "    We will have an output of 40 x 1 dimension\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "7. **Output layer (Softmax)**:\n",
    "\n",
    "    Parameters: W: 40 * 2, b: 2\n",
    "    We will get an output of 2 x 1 dimension\n",
    "    \n",
    "Moreover, we will have to normalize the pixel values, from [0, 255] range to [0,1] values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "21bcf62f-6640-409c-b178-a372f129a0ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPool2D, Flatten\n",
    "\n",
    "# We define some starting parameters\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 50\n",
    "img_rows, img_cols = X_train_gray.shape[1], X_train_gray.shape[2]\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# We one-hot encode the training and test labels\n",
    "Y_train_one_hot = to_categorical(Y_train, num_classes=num_classes)\n",
    "Y_test_one_hot = to_categorical(Y_test, num_classes=num_classes)\n",
    "\n",
    "# Assuming X_train_gray and X_test_gray are your input images\n",
    "X_train_gray = X_train_gray / 255.0\n",
    "X_test_gray = X_test_gray / 255.0\n",
    "\n",
    "# Let's define the LeNet-5 model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=12, kernel_size=(5,5), padding='same', strides=1, activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPool2D(strides=2))\n",
    "model.add(Conv2D(filters=20, kernel_size=(5,5), padding='same', strides=2, activation='relu'))\n",
    "model.add(MaxPool2D(strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax')) # Two values as output\n",
    "\n",
    "# Compile the model with categorical crossentropy as the loss function and with a 5% of learning rate\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    # As the numerical method to minimize the loss function we will use the Stochastic gradient descent (SGD)\n",
    "    optimizer=SGD(learning_rate=0.05),\n",
    "    ## Accuracy, the ratio of correctly predicted observations to the total, as the evaluation metric\n",
    "    metrics=[\"accuracy\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf01e459",
   "metadata": {},
   "source": [
    "Finally we train the model. Note that it will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c3b146cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we train it\n",
    "history = model.fit(X_train_gray, Y_train_one_hot,\n",
    "          batch_size=batch_size,\n",
    "          verbose=1,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test_gray, Y_test_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "039183cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abc4409",
   "metadata": {},
   "source": [
    "What can we infer from the profile? This simple architecture does not seem to capture the relevant information and build an accurate model, since the accuracy on the test dataset plateaus at around 70% of accuracy (remember that for a binary classifier, we expect 50% of accuracy of a random untrained model).\n",
    "\n",
    "Moreover, by comparing the training and testing dataset loss and accuracy, we can infer that a little earlier of the 20 epochs the model cannot improve more since the metrics on the test dataset get stuck (it fluctuates or even decays) while the training metrics keep improving, suggesting that the model enters into an overfitting dynamic.\n",
    "\n",
    "This was expected, since LeNet-5 is a very simple CNN whose main application is to distinguish digits (https://en.wikipedia.org/wiki/LeNet). Note that the CNNs proposed by the author of the dataset they have far more hidden layers and complexity than the one explained here. This allows to reach 0.75 levels of accuracy after very few epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda6c664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MScOmicsVic",
   "language": "python",
   "name": "mscomicsvic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "308px",
    "width": "346px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
