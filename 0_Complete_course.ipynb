{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ee1674-e0a1-47b8-8a23-7a67373279cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "**In this practical session you will:**\n",
    "\n",
    "   - Learn the essential idea behind Machine Learning including several statistical concepts and the implementation steps under the point of view of the Data Science cycle.\n",
    "   - Download, explore and implement the preliminary processing of a multi-omics cancer dataset that will be used throughout the course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aa72b7",
   "metadata": {},
   "source": [
    "## Definition:\n",
    "\n",
    "Machine learning is a subfield of artificial intelligence (AI) that focuses on developing algorithms and models that allow computers to learn patterns and make predictions or decisions without being explicitly programmed. In the context of data science and mathematical modeling, machine learning plays a crucial role in building models that represent real-world systems using mathematical concepts and language. A subfield of Machine learning is Deep learning, which uses a type of models called neural networks that are inspired in the architechture of human brains.\n",
    "\n",
    "[![AI diagram](http://danieljhand.com/images/AI_ML_DL_circles.jpeg)](http://danieljhand.com/the-relationship-between-artificial-intelligence-ai-machine-learning-ml-and-deep-learning-dl.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf8763",
   "metadata": {},
   "source": [
    "## Characteristics of Machine Learning:\n",
    "\n",
    "1. **Learning from Data:**\n",
    "   - Machine learning systems learn from data rather than relying on explicit programming by using some statistical techniques.\n",
    "   - Algorithms use available data to identify patterns, relationships, and trends.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "2. **Model Development:**\n",
    "   - Machine learning involves creating models that can generalize patterns from the training data to make predictions or decisions on new, unseen data.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "3. **Adaptability:**\n",
    "   - Machine learning models can adapt and evolve as new data becomes available, making them suitable for dynamic and changing environments.\n",
    "\n",
    "<!-- Add an empty line here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d915a3e",
   "metadata": {},
   "source": [
    "## Data Science Lifecycle:\n",
    "\n",
    "The data science lifecycle involves several key steps that machine learning implementations follows:\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "1. **Identification of the problem:**\n",
    "   - Allows the decision on the suitable model and algorithm and definition of training and test datasets.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "2. **Data Collection:**\n",
    "   - Gather relevant data from various sources, ensuring it is representative and suitable for the problem at hand.\n",
    "   - It is highly important to perform exploratory analysis to evaluate the quality of the data and, if suitable, define the subsequent necessary processing steps.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "3. **Data Processing:** This is probably the most relevant step: independently of a succesful implementation of the previous steps, if the data does not contain the information relevant to solve the problem and or present in an inadequate state for the algorithm to learn from, the resulting model will be useless (garbage-in -> garbage-out). It mainly consists of two steps.\n",
    "\n",
    "    3.1. **Data Pre-processing:**\n",
    "    - Clean and preprocess the data to handle missing values, outliers, and format issues.\n",
    "\n",
    "    <!-- Add an empty line here -->\n",
    "\n",
    "    3.2. **Feature Engineering:**\n",
    "    - Necessary in some cases but optional in others.\n",
    "    - Select or create features that are relevant and informative for the machine learning model.\n",
    "    - Common approaches are grouped into *Filter-based*, *Wrapper-based* and *Embedded-based* categories.\n",
    "   \n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "4. **Data modelling:** During this iterative process, each model's performance is assessed using different metrics depending if the algorithm works with categorical or continous variables.\n",
    "\n",
    "    <!-- Add an empty line here -->\n",
    "\n",
    "    4.1. **Model Training:**\n",
    "    - Use a learning algorithm to train the model on a labeled dataset, allowing it to learn patterns and relationships.\n",
    "\n",
    "    <!-- Add an empty line here -->\n",
    "\n",
    "    4.2. **Model Optimization:**\n",
    "    - Adjust model parameters and features to improve performance, often involving techniques like hyperparameter tuning. Within this step it is important to avoid overfitting (the model could be generalized to datasets beyond the training ones).\n",
    "\n",
    "    <!-- Add an empty line here -->\n",
    "\n",
    "    4.3. **Model Testing:**\n",
    "    - Validate the model on new, unseen data to ensure it generalizes well (without overfitting) and provides accurate predictions (without underfitting).\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "5. **Deployment:**\n",
    "   - Deploy the model into a real-world environment, integrating it into decision-making processes.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Data Science LyfeCycle](https://www.onlinemanipal.com/wp-content/uploads/2022/09/Data-Science-Life-cycle-768x767.png.webp)](https://www.onlinemanipal.com/blogs/data-science-lifecycle-explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3293b",
   "metadata": {},
   "source": [
    "## Types of Machine Learning\n",
    "\n",
    "Machine learning is broadly categorized into several types, each serving different purposes and solving distinct problems. Here are the main types:\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![AI diagram](https://www.freecodecamp.org/news/content/images/2020/08/ml-1.png)](https://www.freecodecamp.org/news/machine-learning-for-managers-what-you-need-to-know/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "In supervised learning, the algorithm is trained on a **labeled** dataset, where each input is paired with the corresponding output. The goal is to learn a mapping from inputs to outputs, and hence, **predict an output based on input**.\n",
    "\n",
    "The usefulness of these models is evaluated immediately since both the input and corresponding correct outputs are provided in the testing dataset.\n",
    "\n",
    "\n",
    "**a. Regression:**\n",
    "   - **Objective:** Predict a continuous target variable.\n",
    "   - **Examples:** Linear Regression, Polynomial Regression.\n",
    "\n",
    "**b. Classification:**\n",
    "   - **Objective:** Predict a discrete target variable (class labels).\n",
    "   - **Examples:** Logistic Regression, Decision Trees or Random Forest and Support Vector Machines.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "Unsupervised learning involves training on **unlabeled** data, and the algorithm tries to **discover patterns or relationships in the data** without explicit guidance on the output.\n",
    "\n",
    "Since the output is unknown in the training data, the usefulness is implicitly derived from the structure and relationships discovered in the data.\n",
    "\n",
    "**a. Clustering:**\n",
    "   - **Objective:** Group similar data points together.\n",
    "   - **Examples:** K-Means Clustering, Hierarchical Clustering.\n",
    "\n",
    "**b. Dimensionality Reduction:**\n",
    "   - **Objective:** Reduce the number of input features while preserving important information. It is also commonly used as a pre-processing step for feature extraction.\n",
    "   - **Examples:** Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE).\n",
    "\n",
    "**c. Association Rule Learning:** It will not be covered on this course.\n",
    "   - **Objective:** Discover interesting relationships between variables in large datasets.\n",
    "   - **Examples:** Apriori Algorithm, Eclat Algorithm.\n",
    "\n",
    "\n",
    "### Reinforcement Learning\n",
    "\n",
    "Reinforcement learning involves an **agent interacting with an environment**, learning to make decisions by receiving feedback in the form of rewards or penalties (mimics the human trial and error behaviour). Hence, the objective is to **learn a policy to make decisions achieving the most optimal result**.\n",
    "\n",
    "On this type of algorithms, the performance depends on the environment provided by the agent (reward or penalty) after each action, guiding it towards learning a successful policy for optimization (https://www.youtube.com/@aiwarehouse). It is usually employed for training AI for videogames rather than on -omics data analysis, so it won't be covered on this course.\n",
    "\n",
    "**a. Model-Based Reinforcement Learning:**\n",
    "\n",
    "   - **Objective:** Build an explicit model of the environment to make decisions.\n",
    "   - **Examples:** Monte Carlo Tree Search.\n",
    "\n",
    "**b. Model-Free Reinforcement Learning:**\n",
    "\n",
    "   - **Objective:** Learn to make decisions without an explicit model of the environment.\n",
    "   - **Examples:** Q-Learning, Deep Q Network (DQN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb21391",
   "metadata": {},
   "source": [
    "## Relationship with Statistical Concepts\n",
    "\n",
    "1. **Pattern Recognition:**\n",
    "   - Machine learning involves finding patterns in data, a concept deeply rooted in statistics.\n",
    "   - Depeding on the types of problem, and hence, the employed algorithm, different kinds of patterns can be extracted from data.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Pattern types](https://www.researchgate.net/profile/Gordon-Elger/publication/352727978/figure/fig2/AS:1153327744192512@1651986170131/Machine-learning-tasks-most-relevant-for-PdM.png)](https://www.researchgate.net/figure/Machine-learning-tasks-most-relevant-for-PdM_fig2_352727978)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "2. **Cross-Validation:** A key concept for supervised models when the available dataset is smaller than the optimal for the validation purposes.\n",
    "   - To assess a supervised model's generalization ability, cross-validation techniques are used to evaluate performance on multiple subsets of the data.\n",
    "   - There are multiple methodologies (https://www.turing.com/kb/different-types-of-cross-validations-in-machine-learning-and-their-explanations) although the most common is the **K-fold cross-validation** which involves partitioning the entire dataset into k number of random subsets, where k-1 are used for training and 1 for testing purposes. This is repeated for a number of iterations and the model is evaluated through the metrics obtained across interations.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Bias and Variance](https://d2mk45aasx86xg.cloudfront.net/image5_11zon_af97fe4b03.webp)](https://www.turing.com/kb/different-types-of-cross-validations-in-machine-learning-and-their-explanations) \n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "3. **Bias-Variance Tradeoff:** Also a key concept when dealing with supervised models.\n",
    "   - In statistics, the bias of an estimator is the difference between this estimator’s expected value and the true value of the parameter being estimated. On the other hand, the variance of an estimator measures how much the estimates from the estimator are likely to vary or spread out around the true, unknown parameter, through repeated sampling.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "   [![Bias and Variance](https://nvsyashwanth.github.io/machinelearningmaster/assets/images/bias_variance.jpg)](https://nvsyashwanth.github.io/machinelearningmaster/bias-variance/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "   - If we consider Machine learning predictions as estimations these two concepts acquire the following meaning in this context. \n",
    "       \n",
    "       - **Variance** is the consistency of the model predictions for a particular sample instance (for instance applying the model multiple times on subsets of the training dataset). In other words, is the sensitivity of the model to the randomness of the training dataset.\n",
    "       \n",
    "       - In contrast, **Bias** could be seen as the measure of the distance between predictions and the correct values (the labels) if we rebuild the model multiple times with different training datasets. Therefore, is the measure of the systematic error not due to randomness in the training data.\n",
    "             \n",
    "   - These two concepts are intrinsically related, and therefore, the bias-variance tradeoff is a fundamental concept in machine learning: there is an optimal model complexity that allows for good performance on the training data but still keeping the ability to generalize to new data. In complex omics datasets, we often face the **Curse of Dimensionality**.\n",
    "\n",
    "      * **High Bias (Underfitting):** The model is too simple (e.g., linear regression on non-linear biological data).\n",
    "      * **High Variance (Overfitting):** The model captures noise (e.g., a decision tree with infinite depth memorizing patient IDs).\n",
    "   \n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "   [![Underfitting and overfitting](https://www.endtoend.ai/assets/blog/misc/bias-variance-tradeoff-in-reinforcement-learning/underfit_right_overfit.png)](https://www.endtoend.ai/blog/bias-variance-tradeoff-in-reinforcement-learning/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "4. **Statistical Metrics:**\n",
    "   - Various statistical metrics are used to quantify the performance of both unsupervised, and mostly, supervised machine learning models.\n",
    "   - The type of metric used is related with the type of problem/algorithm used.\n",
    "   \n",
    "   <!-- Add an empty line here -->\n",
    "   \n",
    "   [![Supervised metrics](https://www.kdnuggets.com/wp-content/uploads/anello_machine_learning_evaluation_metrics_theory_overview_11.png)](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd29ed",
   "metadata": {},
   "source": [
    "## Case of use: Cancer genomics\n",
    "\n",
    "Cancer is a group of diseases involving abnormal cell growth with the potential to invade or spread to other parts of the body. At the very core of the etiology of cancer is somatic mutations: permanent alterations in the genetic material (either resulting from spontaneous errors during the DNA replication or as a result of DNA damage) originated throughout the somatic development (from the very first mitotic divisions of the Zygot to the human adult tissues).\n",
    "\n",
    "As sequencing technologies advanced in the past decade, the number of available tumoral whole genomes have increased exponentially, revealing that different tumors accumulate mutations with a variability of up to three orders of magnitude.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![ICGC TMB](images/ICGC_muts.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Not only the total number of mutation varies, but also the composition. The endogenous mutational processes active in a tissue as well as the mutagens a person has been exposed during their lifetime, e.g ultraviolet (UV)-light or tobacco smoking, define a set of probabilities for each nucleotide to mutate provided of its neighboring\n",
    "sequence. These probabilities can be inferred can be decomposed from the observed data into several\n",
    "components that roughly reflect the individual mutational processes affecting the cell, the so-called ‘mutation signatures’, some linked to specific mechanisms.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "**Tobacco-related signature of single base substitutions (SBS) 4**\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Tobacco Signature](https://cog.sanger.ac.uk/cosmic-signatures-production/images/v2_signature_profile_4.original.png)](https://cancer.sanger.ac.uk/signatures/signatures_v2/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "**Ultraviolet light-related signature of single base substitutions (SBS) 7**\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Tobacco Signature](https://cog.sanger.ac.uk/cosmic-signatures-production/images/v2_signature_profile_7.original.png)](https://cancer.sanger.ac.uk/signatures/signatures_v2/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Hence, the study of mutations within the Cancer Genomics field, integrated with other -omic data such as transcriptomics or epigenomics as well as clinical data has paved the latest advances in Cancer Research.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e1084",
   "metadata": {},
   "source": [
    "Several international consortium have generated multi-omic cancer datasets. One of them, enmarked within The Cancer Genome Atlas (TCGA) is the Pan Cancer Analysis of Whole Genomes (PCAWG) initiative. Public available data is stored at the International Cancer Genome Consortium (ICGC) database: https://dcc.icgc.org/releases/PCAWG\n",
    "\n",
    "UPDATE: The data from the PCAWG, which was available at the ICGC portal, is no longer of public access due to the closure in June 24 of 2024. The large files that cannot be stored on GitHub directly are on the Google Drive link of the documentation.\n",
    "\n",
    "Some files are particularly interesting for analysis with Machine Learning techniques:\n",
    "\n",
    "* **Clinical:** `pcawg_donor_clinical_August2016_v9.xlsx`\n",
    "* **Sample Sheet:** `pcawg_sample_sheet.tsv`\n",
    "* **RNA-Seq:** `pcawg.rnaseq.transcript.expr.tpm.tsv.gz`\n",
    "* **Mutational Signatures:** `SignatureAnalyzer_COMPOSITE.SBS.txt`\n",
    "\n",
    "All files are available in the data folder already, with the exception of **pcawg.rnaseq.transcript.expr.tpm.tsv.gz** which needs to be downloaded from https://drive.google.com/file/d/1V2_deNxYowAG2mvb9OqMCg0WC2uPX3LM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16235d21",
   "metadata": {},
   "source": [
    "### Exploratory analysis: Clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "clinical_df = pd.read_excel('data/pcawg_donor_clinical_August2016_v9.xlsx')\n",
    "clinical_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e419ea2",
   "metadata": {},
   "source": [
    "Notice that now except for the sex and vital status features, the NA category nan from numpy is the most common category across the information on the patients. This is going to complicate analysis using this clinical phenotypical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e374e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values for each column\n",
    "na_counts = clinical_df.isna().sum() / len(clinical_df) * 100\n",
    "na_df = na_counts.reset_index()\n",
    "na_df.columns = ['Column', 'Missing_Percentage']\n",
    "na_df = na_df.sort_values(by='Missing_Percentage', ascending=True)\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "fig = px.bar(\n",
    "    na_df,\n",
    "    x='Missing_Percentage',\n",
    "    y='Column',\n",
    "    orientation='h',\n",
    "    title=\"<b>Missing Data (NA) Prevalence</b>\",\n",
    "    labels={'Missing_Percentage': 'Percentage of Missing Values (%)', 'Column': 'Feature'},\n",
    "    text_auto='.1f',  # Show values on bars\n",
    "    template='plotly_white',\n",
    "    color='Missing_Percentage',\n",
    "    color_continuous_scale='Reds' # Red highlights the \"danger\" of missing data\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c456b5f",
   "metadata": {},
   "source": [
    "We will use some clinical data as predictors for regression (predicting mutations from age at diagnosis), hence we need to check if Age is normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e4cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_age = px.histogram(\n",
    "    clinical_df, \n",
    "    x=\"donor_age_at_diagnosis\", \n",
    "    color=\"donor_vital_status\",\n",
    "    nbins=50,\n",
    "    title=\"<b>Distribution of Donor Age</b>\",\n",
    "    labels={\"donor_age_at_diagnosis\": \"Age at Diagnosis\", \"count\": \"Number of Patients\"},\n",
    "    opacity=0.7,\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "fig_age.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47fa1bb",
   "metadata": {},
   "source": [
    "### Exploratory analysis: samples\n",
    "\n",
    "Now we can check the file with relation of specimens and samples extracted from each donor. It is just a file that helps connect by IDs other files, so let's have a quick look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea61e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It is a tabular separated file, so we read with the read_csv function specifying the tabulator \\t as the separator character\n",
    "## Moreover, we indicate that the file has a header that should be inferred as the column names.\n",
    "sample_df = pd.read_csv('data/pcawg_sample_sheet.tsv', sep='\\t', header='infer')\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5098d09",
   "metadata": {},
   "source": [
    "Note that for the same donor, several specimens are extracted. Usually, one from a normal tissue and another from a primary tumor (to find mutations through WGS it is necessary to remove germline mutations that are present on both normal and tumoral tissue, that is why the mutations found on normal tissues are usually substracted from the tumor mutation calls)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_specimen_counts = sample_df.groupby('donor_unique_id')['icgc_specimen_id'].nunique().reset_index()\n",
    "donor_specimen_counts.columns = ['Donor_ID', 'Specimen_Count']\n",
    "\n",
    "fig_count = px.histogram(\n",
    "    donor_specimen_counts,\n",
    "    x=\"Specimen_Count\",\n",
    "    title=\"<b>Distribution of Specimens per Donor</b>\",\n",
    "    labels={'Specimen_Count': 'Number of Specimens Collected', 'count': 'Number of Donors'},\n",
    "    text_auto=True,  # Show the exact count on top of the bars\n",
    "    template=\"plotly_white\",\n",
    "    color_discrete_sequence=['teal']\n",
    ")\n",
    "\n",
    "# Force the X-axis to show integers (1, 2, 3...) rather than a range\n",
    "fig_count.update_layout(xaxis=dict(dtick=1))\n",
    "fig_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d405f7",
   "metadata": {},
   "source": [
    "In some projects, some donors have more than the Normal and tumoral pair, and more specimens are collected. Moreover, notice that from the same tumoral specimen several samples might be extracted to extract information with different techniques: in this case for WGS or for RNA-seq. This will be relevant to map multiomic information across samples from the same tumoral specimen from a given donor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c3da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Project -> Specimen Type -> Library Strategy\n",
    "# This effectively tracks the \"Sample\" level (e.g., is this sample WGS or RNA-Seq?)\n",
    "hierarchy_counts_detailed = sample_df.groupby(\n",
    "    ['dcc_project_code', 'dcc_specimen_type', 'library_strategy']\n",
    ").size().reset_index(name='count')\n",
    "\n",
    "# Create the 3-Level Sunburst Plot\n",
    "fig = px.sunburst(\n",
    "    hierarchy_counts_detailed,\n",
    "    path=['dcc_project_code', 'dcc_specimen_type', 'library_strategy'], # Added 3rd level\n",
    "    values='count',\n",
    "    title=\"<b>Multi-Omics Data Hierarchy</b>: Project > Specimen > Strategy\",\n",
    "    color='count', \n",
    "    color_continuous_scale='ice', # 'ice' scale is distinct and clean\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.update_layout(margin=dict(t=50, l=0, r=0, b=0))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc3ca7b",
   "metadata": {},
   "source": [
    "We can finally get the primary location label for each project and a sample dataframe ready for analyses is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "# Merge it with a file provided in the data folder to extract the primary site\n",
    "project_info = pd.read_csv(path.join('data', 'projects_PCAWG_info.txt'), sep='\\t', header='infer')\n",
    "\n",
    "primary_location_dict = dict(zip(project_info.project, project_info.primary_location))\n",
    "\n",
    "sample_df['primary_location'] = sample_df['dcc_project_code'].map(primary_location_dict)\n",
    "\n",
    "# Save it on the data folder for later uses\n",
    "sample_df.to_csv(path.join('data', 'sample_df.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416cbc4",
   "metadata": {},
   "source": [
    "### Exploratory analysis: RNA-seq\n",
    "\n",
    "Next, we can explore the file with the expression data. Remember you should **download** it at https://drive.google.com/file/d/1V2_deNxYowAG2mvb9OqMCg0WC2uPX3LM and **save it into the data folder**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d44eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the expression data\n",
    "expression_df = pd.read_csv(path.join('data', 'pcawg.rnaseq.transcript.expr.tpm.tsv.gz'), sep='\\t', header='infer', compression='gzip')\n",
    "expression_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12ee5d6",
   "metadata": {},
   "source": [
    "Here the first column shows a the Ensembl Transcript ID and the rest of the columns, whose name is the aliquot ID (present at the **pcawg_sample_sheet.tsv** file).\n",
    "\n",
    "If we want to add gene IDs or Symbols instead of transcript IDs, we will need to process the file that the PCAWG consortium uses for annotation. The file is already present on your data folder, but it will need some preprocessing.\n",
    "\n",
    "We will perform the preprocessing with a bash script. In Jupyter notebooks, you can use other interpreters rather than python. The script below is able to download and process the necessary file (If you don't have a linux-based operative system, skip this step. The final processed file is already provided in the data folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d81d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# The %% above indicates to the jupyter notebook to use bash as interpreter\n",
    "\n",
    "# Change into the data directory\n",
    "cd data\n",
    "\n",
    "# Process the file using a bash code\n",
    "zcat rnaseq.gc19_extNc.gtf.tar.gz | cut -f9 | cut -d';' -f2 | sed 's/.*gencode::\\([^:]*\\)::tc_\\([^._]*\\)[^:]*::\\([^._]*\\)[^:]*.*/\\1\\t\\2\\t\\3/' | sort | uniq | tail -n +3 | gzip > gencode_transcript.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cfa031",
   "metadata": {},
   "source": [
    "Summarizing until here we have:\n",
    "\n",
    "**pcawg.rnaseq.transcript.expr.tpm.tsv.gz**: A large file with the first column being the ensembl Transcript ID and the rest of the columns with an aliquot ID.\n",
    "\n",
    "**sample_df.tsv**: A file that contains the relationship between donors, specimens and samples. The aliquotID is also a column of this file.\n",
    "\n",
    "**gencode_transcript.tsv.gz**: A file that contains the transcript information.\n",
    "\n",
    "For the analysis at the following sessions we will need to process the expression data, specifically:\n",
    "\n",
    "- A) Get only the expression for tumoral specimens (and that will not cover all the tumoral specimens of the PCAWG).\n",
    "\n",
    "- B) Get the information on a gene, instead than on a transcript level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da42517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open expression matrix\n",
    "expression_matrix = pd.read_csv(path.join('data', 'pcawg.rnaseq.transcript.expr.tpm.tsv.gz'),\n",
    "                                                                    sep=\"\\t\", header='infer', compression='gzip')\n",
    "\n",
    "# Remove version\n",
    "expression_matrix['Transcript'] = (\n",
    "    expression_matrix['Transcript']\n",
    "    .str.extract(r'^(\\w+)\\.\\w+$')\n",
    ")\n",
    "\n",
    "expression_matrix = expression_matrix.set_index('Transcript', drop=True)\n",
    "expression_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a591b700",
   "metadata": {},
   "source": [
    "#### A) Select tumoral specimens\n",
    "\n",
    "Using **sample_df.tsv** for mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08295a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specimen information PCAWG\n",
    "sample_df = pd.read_csv(path.join('data', 'sample_df.tsv'), sep=\"\\t\", header='infer')\n",
    "\n",
    "# Get an aliquot to specimen ID dictionary\n",
    "specimen_dict = dict(zip(sample_df.aliquot_id, sample_df.icgc_specimen_id))\n",
    "\n",
    "# Let's translate the columns into the specimen ID\n",
    "translated_columns = []\n",
    "aliq_ID_not_found_on_files = []\n",
    "for aliqID in expression_matrix.columns:\n",
    "    try:\n",
    "        translated_columns.append(specimen_dict[aliqID])\n",
    "    except:\n",
    "        aliq_ID_not_found_on_files.append(aliqID)\n",
    "        \n",
    "print('Total number of aliquots with expression data: ' + str(len(expression_matrix.columns)))\n",
    "print('Aliquot that could be translated into specimenID: ' + str(len(translated_columns)))\n",
    "print('Dropped samples because of unknown translation of IDs: ' + str(len(aliq_ID_not_found_on_files)))\n",
    "\n",
    "# Extract the columns\n",
    "print(expression_matrix.shape[1])\n",
    "expression_matrix = expression_matrix.drop(aliq_ID_not_found_on_files, axis=1)\n",
    "print(expression_matrix.shape[1])\n",
    "\n",
    "expression_matrix.columns = translated_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0082d4d0",
   "metadata": {},
   "source": [
    "Double checking if all aliquotIDs can be translated into SpecimenIDs is key. However, how many of these specimens that were RNA-Sequenced are from tumoral samples? We do not want on the next analysis to include non-tumoral tissues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca3a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rom the Specimen IDs that we could obtain using the RNA-Seq, library strategy, are all specimen types from tumoral samples?\n",
    "category_series = sample_df[(sample_df['icgc_specimen_id'].isin(translated_columns))&(sample_df['library_strategy']=='RNA-Seq')]['dcc_specimen_type'].value_counts()\n",
    "category_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a20af",
   "metadata": {},
   "source": [
    "Most of the expression data come from specimens of primary solid tumors. Other specimens are from lymph nodes or blood (not solid primary tumors) or even metastasis. However, a non-negligible proportion comes from either Normal tissue adjacent to the primary tumor or just regular healthy tissues. **Hence we need to remove them from the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f069269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get specimens that do not come form normal healthy tissues (do not contain the normal word)\n",
    "clean_sample_df = sample_df[(sample_df['icgc_specimen_id'].isin(translated_columns))&(sample_df['library_strategy']=='RNA-Seq')&(~sample_df['dcc_specimen_type'].str.startswith('Normal'))].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c50da",
   "metadata": {},
   "source": [
    "B) Get information at gene, not at transcript level\n",
    "\n",
    "Finally, we need to process the matrix to get expression information at the gene level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ba797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The annotation file does not have a header, so the column names are specified\n",
    "annotation_df = pd.read_csv(path.join('data', 'gencode_transcript.tsv.gz'), \n",
    "                                sep=\"\\t\", header=None, names=['Symbol', 'Gene', 'Transcript'], compression='gzip')\n",
    "annotation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf148be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To group the transcripts and sum their expression by gene IDs we have to do the following steps\n",
    "\n",
    "## Merge the expression_matrix with annotation_df on the 'Transcript' column.\n",
    "## An Inner join is done to work with the Transcript IDs that are on both dataframes\n",
    "merged_df = pd.merge(expression_matrix.reset_index(), annotation_df , left_on='Transcript', right_on='Transcript', how='inner')\n",
    "print(\"Expression available for \" + str(len(merged_df)) + \" transcripts.\")\n",
    "\n",
    "## Group by 'Gene' and sum the values for each gene\n",
    "collapsed_df = merged_df.groupby('Gene').sum()\n",
    "\n",
    "## Drop unnecessary columns\n",
    "collapsed_df = collapsed_df.drop(columns=['Transcript', 'Symbol']).reset_index()\n",
    "\n",
    "print(\"After merging, expression for \" + str(len(collapsed_df)) + \" genes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34003fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it on the data folder for later uses.\n",
    "collapsed_df.to_csv(path.join('data', 'gene_expression.tsv.gz'), sep='\\t', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb131e",
   "metadata": {},
   "source": [
    "### Exploratory analysis: mutation signatures\n",
    "\n",
    "Finally, we can explore the signature number of attributed mutations for each specimen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ecb0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "signatures_df = pd.read_csv('data/SA_COMPOSITE_SNV.activity.FULL_SET.031918.txt', sep='\\t', header='infer')\n",
    "signatures_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9ca8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the tumor mutation burden to do some exploratory analysis\n",
    "TMB_proxy = signatures_df.iloc[:, 1:].sum(axis=0)\n",
    "TMB_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first column to extract SBS code\n",
    "signatures_df['Unnamed: 0'] = signatures_df['Unnamed: 0'].str.extract(r'_(SBS\\w+)_')\n",
    "\n",
    "# Change column names: the first is signature and the rest are the specimenID\n",
    "signatures_df.columns = ['signature'] + [col.split('__')[-1] for col in signatures_df.columns[1:]]\n",
    "\n",
    "# Save the information for later uses\n",
    "signatures_df.to_csv(path.join('data' , 'signatures.tsv.gz'), sep='\\t', index=False, compression='gzip')\n",
    "signatures_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95584bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, going back to the TMB value\n",
    "specimen_IDs = [col.split('__')[-1] for col in TMB_proxy.index]\n",
    "Histological_type = [col.split('__')[0] for col in TMB_proxy.index]\n",
    "\n",
    "# Generate de novo pandas dataframe with the info\n",
    "TMB_df = pd.DataFrame({'specimenID': specimen_IDs, 'hist_type': Histological_type, 'TMB_proxy': TMB_proxy.values})\n",
    "TMB_df\n",
    "\n",
    "TMB_df.to_csv(path.join('data' , 'TMB.tsv.gz'), sep='\\t', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b244de7",
   "metadata": {},
   "source": [
    "It might be interesting to explore the data with a plot. For that we will generate a plot similar to the one showed when the case of use was introduced: a complex plot with two panels, one showing the distribution of total number of mutations for each histological class in logarithmic scale and one showing the proportion of attribution of mutations to the different signatures, across samples throughout different histological classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# First set the signature name as the index (row name)\n",
    "signatures_df = signatures_df.set_index('signature')\n",
    "\n",
    "# Normalize the values in each column to generate the proportions of each signature\n",
    "signatures_df = signatures_df.div(signatures_df.sum(axis=0), axis=1)\n",
    "\n",
    "# Traspose and reorganize index to have as columns (independent variables) each signature\n",
    "signatures_df = signatures_df.transpose().reset_index()\n",
    "\n",
    "# Some signatures that were extracted at the start of the cancer genomics field were subdivided into more components\n",
    "# (7 was subdivided into 7a, 7b and 7c while 17 into 17a and 17b). To simplify we will merge into one component.\n",
    "# Create new columns by summing the specified columns\n",
    "signatures_df['SBS7a'] = signatures_df[['SBS7a', 'SBS7b', 'SBS7c']].sum(axis=1)\n",
    "signatures_df['SBS17a'] = signatures_df[['SBS17a', 'SBS17b']].sum(axis=1)\n",
    "# Rename the columns ('index' column to 'specimenID' and the others)\n",
    "signatures_df = signatures_df.rename(columns={'index': 'specimenID', \n",
    "                                              'SBS7a': 'SBS7', \n",
    "                                              'SBS17a': 'SBS17',\n",
    "                                              'SBS10a': 'SBS10'})\n",
    "# Drop the original columns\n",
    "signatures_df = signatures_df.drop(['SBS7b', 'SBS7c', 'SBS17b'], axis=1)\n",
    "\n",
    "# Drop signatures with no contribution across specimens \n",
    "sum_over = signatures_df[signatures_df.columns[1:]].sum(axis=0)\n",
    "signatures_df = signatures_df.drop(columns=list(sum_over[sum_over==0].index))\n",
    "\n",
    "# Convert TMB_proxy to logarithmic scale\n",
    "TMB_df['log_TMB_proxy'] = np.log10(TMB_df['TMB_proxy'])\n",
    "\n",
    "# Include the total number of elements in the hist_type label for the plots\n",
    "TMB_df['hist_type'] = TMB_df['hist_type'] + ' (n=' + TMB_df.groupby('hist_type').transform('count')['specimenID'].astype(str) + ')'\n",
    "\n",
    "# Merge the two dataframes\n",
    "merged_df = pd.merge(signatures_df, TMB_df , left_on='specimenID', right_on='specimenID', how='inner')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order cohorts by median log TMB\n",
    "order = (\n",
    "    merged_df\n",
    "    .groupby('hist_type')['log_TMB_proxy']\n",
    "    .median()\n",
    "    .sort_values()\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# Median TMB per cohort (on raw scale)\n",
    "medians = (\n",
    "    merged_df\n",
    "    .groupby('hist_type')['TMB_proxy']\n",
    "    .median()\n",
    "    .reindex(order)\n",
    ")\n",
    "\n",
    "fig = px.strip(\n",
    "    merged_df,\n",
    "    x='hist_type',\n",
    "    y='TMB_proxy',\n",
    "    category_orders={'hist_type': order},\n",
    "    log_y=True, # log-scale\n",
    "    stripmode='overlay'\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    jitter=0.6,\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        opacity=0.6\n",
    "    )\n",
    ")\n",
    "\n",
    "for i, cohort in enumerate(order):\n",
    "    fig.add_shape(\n",
    "        type='line',\n",
    "        x0=i - 0.35,\n",
    "        x1=i + 0.35,\n",
    "        y0=medians.loc[cohort],\n",
    "        y1=medians.loc[cohort],\n",
    "        line=dict(\n",
    "            color='red',\n",
    "            width=2\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Tumor cohort',\n",
    "    yaxis_title='Mutations per Mb',\n",
    "    width=1600,\n",
    "    height=500,\n",
    "    template='simple_white'\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723388e2",
   "metadata": {},
   "source": [
    "Definetly, different tumor types from the histological point of view show different levels of mutations, although there is a large variability within each histological type. For instance, it will be very difficult to distinguish by the number of mutations a sample of a bone benign tumor or a myelodisplasic syndrome type of blood cancer. But what about the composition of these mutations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Get only relevant columns\n",
    "prop_df = merged_df[merged_df.columns[:-2]].set_index('specimenID')\n",
    "\n",
    "palette = px.colors.qualitative.Dark24 + px.colors.qualitative.Light24\n",
    "\n",
    "figures = {}\n",
    "\n",
    "for hist_type in order:\n",
    "\n",
    "    # Subset cohort\n",
    "    sub_prop_df = prop_df[prop_df['hist_type'] == hist_type].copy()\n",
    "    sub_prop_df = sub_prop_df.drop(columns=['hist_type'])\n",
    "\n",
    "    # Order signatures by total contribution\n",
    "    contribution_columns = (\n",
    "        sub_prop_df.iloc[:, :-1]\n",
    "        .sum()\n",
    "        .sort_values(ascending=False)\n",
    "        .index\n",
    "    )\n",
    "\n",
    "    # Sort specimens\n",
    "    sorted_specimens = sub_prop_df.sort_values(\n",
    "        by=list(contribution_columns),\n",
    "        ascending=False\n",
    "    )\n",
    "\n",
    "    # Create figure (single cohort → single subplot)\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for j, sig in enumerate(contribution_columns):\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=sorted_specimens.index,\n",
    "                y=sorted_specimens[sig].replace(0, None),\n",
    "                name=sig,\n",
    "                marker_color=palette[j % len(palette)]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        barmode='stack',\n",
    "        bargap=0,\n",
    "        bargroupgap=0,\n",
    "        hovermode='x',\n",
    "        template='simple_white',\n",
    "        height=400,\n",
    "        width=1000,\n",
    "        title=f\"Stacked Bar Plot – {hist_type}\",\n",
    "        xaxis=dict(showticklabels=False),\n",
    "        yaxis=dict(title=\"Contribution\"),\n",
    "        legend=dict(\n",
    "            orientation='v',\n",
    "            yanchor='middle',\n",
    "            y=0.5,\n",
    "            xanchor='left',\n",
    "            x=1.02\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_traces(marker_line_width=0)\n",
    "\n",
    "    # Save figure\n",
    "    outfile = path.join(\"plots\", f\"Barplot_signatures_{hist_type}.html\")\n",
    "    fig.write_html(outfile)\n",
    "\n",
    "    figures[hist_type] = fig\n",
    "\n",
    "figures[\"Skin_Melanoma (n=107)\"].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee85def",
   "metadata": {},
   "source": [
    "There is clearly larger differences in term of composition of the mutations which might help with the identification of tumor types just by using the proportions of signatures on a given sample, although there is still high variability. This might help with the decision of the type of data to use if we consider to build a model that looks on genomic data and wants to identify the histological (or even molecular subtype) of tumor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a995217e",
   "metadata": {},
   "source": [
    "# Unsupervised algorithms\n",
    "\n",
    "**In this practical session you will:**\n",
    "\n",
    "   - Learn to use several common unsupervised methods (dimensionality reduction and clustering algotithms) used in multi-omics data analysis.\n",
    "   - Explore part of the multi-omics dataset and discover the underlying structure of the trasncriptomic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b874ef3",
   "metadata": {},
   "source": [
    "## Dimensionality reduction methods\n",
    "\n",
    "Dimensionality reduction algorithms are techniques used to reduce the number of features (or dimensions) in a dataset while preserving its essential information: this is particularly useful for **visualization, meaningful compression and discovery of the underlying structure of the data**. Two popular dimensionality reduction algorithms are Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a91a90",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a statistical technique that on an n-dimensional matrix of values that:\n",
    "\n",
    "- Identifies the directions (specific axis in the matrix) at which, if the rest of the data is projected into, the data varies the most: the principal components.\n",
    "\n",
    "- Represents the data in a new coordinate system defined by these principal components.\n",
    "    \n",
    "Therefore, the key idea is to find a lower-dimensional representation of the data that captures the maximum amount of variance. Hence, the first principal component is the one that captures the most significant amount of variance in the data, followed by the second principal component, and so on.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "To achieve this, the algorithm uses linear algebra:\n",
    "\n",
    "1. PCA starts by computing the **covariance matrix** of the original data, which represents the relationships between the different features.\n",
    "\n",
    "2. **Eigenvectors and eigenvalues** are extracted from the covariance matrix. The **eigenvectors** are the principal components and the **eigenvalues** indicate the variance along each principal component.\n",
    "\n",
    "3. The **eigenvectors** are sorted in descending order based on the **eigenvalues**.\n",
    "\n",
    "4. The dimensionality of the data is reduced and the data is transformed **linearly** into a new coordinate system aligned with the an amount of first principal components depending on the new dimensionality.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![PCA in a nutshell](https://pbs.twimg.com/media/F9XIOm1boAEhsL2?format=jpg&name=small)](https://twitter.com/akshay_pachaar/status/1717519050706952695)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "\n",
    "**Think of PCA as rotating a 3D object in front of a light until you find the exact angle (the eigenvector, the principal component) that casts the largest and most detailed shadow on the wall (a lower dimension representation).**\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "PCA is probably the most used dimensionality reduction technique thanks to its multiple advantatges, although it also has its own problems:\n",
    "\n",
    "**Advantages:**\n",
    "- Computationally efficient for linear dimensionality reduction.\n",
    "- Preserves as much variance as possible.\n",
    "- Clear interpretation of the principal components.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Assumes linearity.\n",
    "- May not capture complex nonlinear relationships.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb6024",
   "metadata": {},
   "source": [
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "t-SNE is another common dimensionality reduction algorithm primarily used for visualizing high-dimensional data in a lower-dimensional space. However, unlike linear methods like PCA, t-SNE focuses on preserving local structures and capturing non-linear relationships between data points.\n",
    "\n",
    "To this effect, the algorithm uses:\n",
    "\n",
    "- **Measures of pairwise similarity** between data points since similar data points in the high-dimensional space are intended to remain close to each other in the low-dimensional space.\n",
    "\n",
    "- Moreover, t-SNE constructs **probability distributions** for the pairwise similarities in both the high-dimensional and low-dimensional spaces to model the similarities using conditional probabilities: Nearby points -> High probability, Far points -> low probability\n",
    "\n",
    "- The algorithm minimizes the **divergence between the probability distributions** in the high-dimensional and low-dimensional spaces low-dimensional representation to reflect the structure of the high-dimensional data.\n",
    "\n",
    "A key hyperparameter of t-SNE is `perplexity` it controls how many neighbors each point “cares about” when building the low-dimensional embedding:\n",
    "\n",
    "- Low `perplexity` focus on local structure -> Emphasizes small clusters\n",
    "\n",
    "- High `perplexity` focus on global structure -> Emphasizes bigger clusters\n",
    "\n",
    "A good rule of thumb is 5 ≤ `perplexity` ≤ n_samples / 3\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "The advantatges and disadvantatges of t-SNE remark the complementarity of this technique to PCA:\n",
    "\n",
    "**Advantages:**\n",
    "- Effective for preserving local structure and capturing non-linear relationships.\n",
    "- Well-suited for visualization of high-dimensional data.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Computationally expensive for large datasets.\n",
    "- Optimizing t-SNE involves non-convex optimization, which may result in different solutions for different initializations.\n",
    "- t-SNE is focused on reflecting the local structure, hence, it does not reflect the global structure.\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabe72a",
   "metadata": {},
   "source": [
    "### Uniform Manifold Approximation and Projection (UMAP)\n",
    "\n",
    "UMAP is a state-of-the-art dimensionality reduction technique that, like t-SNE, captures non-linear relationships. However, UMAP is grounded in algebraic topology and Riemannian geometry, allowing it to preserve **both local and global structures** better than t-SNE.\n",
    "\n",
    "To this effect, the algorithm uses:\n",
    "\n",
    "*   **Weighted Graph Construction:** Instead of using probability distributions immediately, UMAP first constructs a high-dimensional graph representation of the data. It assumes the data points are uniformly distributed on a locally connected manifold (a curved geometric surface).\n",
    "*   **Fuzzy Simplicial Sets:** To mathematically represent this structure, UMAP creates \"fuzzy\" topological structures. This accounts for varying densities in the data, ensuring that sparse regions and dense regions are connected appropriately.\n",
    "*   **Cross-Entropy Minimization:** The algorithm minimizes the **cross-entropy** (rather than KL divergence) between the high-dimensional and low-dimensional graphs. This optimization function forces similar points to be close (local structure) while also pushing dissimilar clusters further apart (global structure).\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Here the hyperparameters are also key:\n",
    "\n",
    "**1\\. `n_neighbors` (The Balance Controller)**\n",
    "\n",
    "Controls the size of the local neighborhood UMAP looks at when learning the data structure. Equivalent of `perplexity` in t-SNE.\n",
    "\n",
    "**2\\. `min_dist` (The Spacing Controller)**\n",
    "\n",
    "Controls how tightly points are allowed to be packed together in the final visualization.\n",
    "\n",
    "-   **Low `min_dist`** (\\< 0.1)  $\\to$  **Clumper**\n",
    "-   **High `min_dist`** (\\> 0.5) $\\to$  **Spreader** \n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "The advantages and disadvantages of UMAP highlight why it is often chosen over t-SNE for modern genomic datasets:\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "*   **Faster computation:** Significantly faster than t-SNE, making it scalable to very large datasets.\n",
    "*   **Global Structure:** Preserves the global distances between distinct clusters better than t-SNE (meaning the distance between Cluster A and Cluster B is actually informative).\n",
    "*   **Flexible:** Can be used for general dimension reduction, not just visualization.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "*   **Hyperparameter sensitivity:** The results can change significantly depending on the choice of \"Neighbors\" and \"Minimum Distance\" parameters.\n",
    "*   **Newer technique:** While robust, it is newer than PCA and t-SNE, so the theoretical understanding of its edge cases is still evolving.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7318ffdd",
   "metadata": {},
   "source": [
    "### Method comparison\n",
    "\n",
    "The different characteristics of these techniques is key to choose the appropiate one based on the nature of the data and the problem at hand: PCA is often preferred for linear relationships and dimensionality reduction, while t-SNE is powerful for visualizing complex, non-linear structures in high-dimensional data.\n",
    "\n",
    "| Feature | PCA | t-SNE | UMAP |\n",
    "| --- | --- | --- | --- |\n",
    "| **Method** | Linear Projection | Probabilistic (Divergence) | Topological (Graph Theory) |\n",
    "| **Structure** | Global Variance | Local Neighbors | Local + Global Balance |\n",
    "| **Speed** | Very Fast | Slow | Fast |\n",
    "| **Interpretability** | Clear interpretation | Clusters do not have a meaning | Clusters have meaning |\n",
    "| **Best For** | Simple data & preprocessing | Visualizing distinct clusters | Large, complex datasets |\n",
    "\n",
    "For a better comparison between t-SNE and UMAP see: https://pair-code.github.io/understanding-umap/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce5b1df",
   "metadata": {},
   "source": [
    "## Practical session: visualization of transcriptomes and identification of cancer types by expression\n",
    "\n",
    "The expression data across genes and specimens that we generated in the previous session is highly multidimensional, given that for each specimen we have the expression across more than 20000 thousand genes or features.\n",
    "\n",
    "In order to make any sense of this data, we can start by employing techniques such as PCA or t-SNE to preliminary investigate for interesting patterns in the data. For this purpose we can use the utilities available on the scikit-learn package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6578fb31",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "From a practical point of view, these are the steps we are going to implement:\n",
    "\n",
    "1. Standardize the data: The objetive of PCA is to maximize the variance. If the features (in this case >20000 gene expressions have different scales or units, it is important to standardize the data by subtracting the mean and dividing by the standard deviation. This step ensures that all features are on a similar scale and prevents dominance by features with larger variances. For that, we will use the StandardScaler of sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "2. Compute the covariance matrix: To understand the relationships between pairs of gene expression in the data.\n",
    "\n",
    "3. Perform the eigen-decomposition: The covariance matrix is decomposed into its eigenvectors and eigenvalues. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "4. Select the principal components: The principal components are ranked based on their corresponding eigenvalues, and the top components capturing the most variance are selected. Since we will do a 2D visualization, we need the two components that explain most of the variance in gene expression across samples.\n",
    "\n",
    "5. Project the data onto the new coordinate system: The original data is transformed by projecting it onto the selected principal components. Each data point is represented by its new coordinates in the principal component space.\n",
    "\n",
    "Steps 2, 3, 4 and 5 could be implemented easily with numpy through linear algebra operations (if anyones wants to, you can try the exercise. If you need help, see https://stackoverflow.com/questions/58666635/implementing-pca-with-numpy) however, this is a standard procedure and already implemented in machine learning packages such as **skicit-learn** as the PCA module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574a7882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 2\n",
    "\n",
    "# We load the expression data\n",
    "expression_df = pd.read_csv(path.join('data', 'gene_expression.tsv.gz'),\n",
    "                                                        sep=\"\\t\", header='infer', index_col=0, compression='gzip')\n",
    "\n",
    "# We preprocess the data with standarization\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(expression_df.T)\n",
    "\n",
    "# We perform the PCA\n",
    "pca2D = PCA(n_components=n_components)\n",
    "proj_data = pca2D.fit_transform(data)\n",
    "\n",
    "# We can check the amount of variance explained by the two Principal components\n",
    "print(pca2D.explained_variance_ratio_)\n",
    "\n",
    "pca_df = pd.DataFrame(\n",
    "    data=proj_data, \n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=expression_df.columns\n",
    ")\n",
    "\n",
    "# We plot the data (the first two components are the first two columns of proj_data)\n",
    "fig = px.scatter(\n",
    "    pca_df, \n",
    "    x='PC1', \n",
    "    y='PC2',\n",
    "    title='Transcriptome PCA',\n",
    "    labels={'PC1': 'Principal Component 1', 'PC2': 'Principal Component 2'},\n",
    "    hover_name=pca_df.index,  # Show specimen on hover\n",
    "    template=\"plotly_white\",\n",
    "    width=800,\n",
    "    height=800,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48496fab",
   "metadata": {},
   "source": [
    "A priori we cannot distinguish much, but we could try to colour each sample based on the primary tumor type and see if it is more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d58991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample dataframe with the information\n",
    "sample_df = pd.read_csv(path.join('data', 'sample_df.tsv'), sep=\"\\t\", header='infer')\n",
    "\n",
    "# Create the dictionary to map Specimen ID -> Tumor Type\n",
    "tumortype_dict = dict(zip(sample_df.icgc_specimen_id, sample_df.primary_location))\n",
    "\n",
    "# Get the tumor type for each sample (each column)\n",
    "labels = expression_df.columns.map(tumortype_dict)\n",
    "\n",
    "pca_df['Tumor Type'] = labels  # Add the labels as a new column\n",
    "\n",
    "fig = px.scatter(\n",
    "    pca_df, \n",
    "    x='PC1', \n",
    "    y='PC2',\n",
    "    color='Tumor Type',  # Automatically colors by this column and creates a legend\n",
    "    title='Transcriptome PCA by Tumor Type',\n",
    "    labels={\n",
    "        'PC1': f'Principal Component 1 ({pca2D.explained_variance_ratio_[0]:.2%})', \n",
    "        'PC2': f'Principal Component 2 ({pca2D.explained_variance_ratio_[1]:.2%})'\n",
    "    },\n",
    "    hover_name=pca_df.index,\n",
    "    width=800,\n",
    "    height=800,\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=4, opacity=0.8))\n",
    "\n",
    "fig.write_html(path.join('plots', 'PCA.html'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fba1208",
   "metadata": {},
   "source": [
    "Apparently the PCA can separate some blood cancer clusters, but has problems to separate other samples from a wide variety of primary tumor locations. Definetly, the implementation of PCA does not solve the problem at hand. We can try to include a third dimension, the third principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f0bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can even use the third PC and do a 3D plot\n",
    "n_components = 3\n",
    "pca3D = PCA(n_components=n_components)\n",
    "proj_data3D = pca3D.fit_transform(data)\n",
    "\n",
    "# Print explained variance\n",
    "print(f\"Explained Variance Ratio: {pca3D.explained_variance_ratio_}\")\n",
    "\n",
    "pca3D_df = pd.DataFrame(\n",
    "    data=proj_data3D, \n",
    "    columns=['PC1', 'PC2', 'PC3'], \n",
    "    index=expression_df.columns\n",
    ")\n",
    "pca3D_df['Tumor Type'] = labels\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    pca3D_df, \n",
    "    x='PC1', \n",
    "    y='PC2', \n",
    "    z='PC3',\n",
    "    color='Tumor Type',\n",
    "    title='Transcriptome PCA (3D)',\n",
    "    hover_name=pca3D_df.index,\n",
    "    labels={\n",
    "        'PC1': f'PC1 ({pca3D.explained_variance_ratio_[0]:.2%})', \n",
    "        'PC2': f'PC2 ({pca3D.explained_variance_ratio_[1]:.2%})',\n",
    "        'PC3': f'PC3 ({pca3D.explained_variance_ratio_[2]:.2%})'\n",
    "    },\n",
    "    width=900,\n",
    "    height=900\n",
    ")\n",
    "\n",
    "# Adjust marker size and opacity for better visibility in 3D\n",
    "fig.update_traces(marker=dict(size=3, opacity=0.7))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236abc20",
   "metadata": {},
   "source": [
    "### t-SNE\n",
    "\n",
    "t-SNE does not assume linearity and might be a better proxy to separate the different types of tumors.\n",
    "\n",
    "By default `perplexity` is 40 on sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a648605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Transpose the matrix\n",
    "transposed_matrix = expression_df.to_numpy().T\n",
    "\n",
    "# Initialize TSNE with 2 components for 2D visualization)\\\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform the data\n",
    "tsne_result = tsne.fit_transform(transposed_matrix)\n",
    "\n",
    "# Create a DataFrame with the t-SNE results\n",
    "tsne_df = pd.DataFrame(tsne_result, columns=['TSNE1', 'TSNE2'], index=expression_df.columns)\n",
    "tsne_df['Tumor Type'] = labels\n",
    "\n",
    "# We plot the data (the first two components are the first two columns of proj_data)\n",
    "fig = px.scatter(\n",
    "    tsne_df, \n",
    "    x='TSNE1', \n",
    "    y='TSNE2',\n",
    "    color='Tumor Type',\n",
    "    title='Transcriptome t-SNE',\n",
    "    labels=['t-SNE Component 1', 't-SNE Component 2'],\n",
    "    hover_name=pca_df.index,  # Show specimen on hover\n",
    "    template=\"plotly_white\",\n",
    "    width=800,\n",
    "    height=800,\n",
    ")\n",
    "\n",
    "fig.write_html(path.join('plots', 'tSNE.html'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2823d9ff",
   "metadata": {},
   "source": [
    "Let's see how it behaves with perplexities focusing more or less in local structure, on a 3D projection.\n",
    "\n",
    "Moreover, let's use **cosine distance instead of euclidian**: by treating gene expression vectors as directions rather than absolute positions, it ignores differences in total read counts (sequencing depth) and focuses on the pattern of expression. This usually creates much cleaner, more separated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c1f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = [30, 50]\n",
    "\n",
    "for perp in perplexities:\n",
    "    \n",
    "    tsne_3d = TSNE(n_components=3, metric='cosine', perplexity=perp, random_state=42)\n",
    "    tsne_result_3d = tsne_3d.fit_transform(transposed_matrix)\n",
    "    \n",
    "    tsne_df_3d = pd.DataFrame(\n",
    "        tsne_result_3d, \n",
    "        columns=['TSNE1', 'TSNE2', 'TSNE3'], \n",
    "        index=expression_df.columns\n",
    "    )\n",
    "    tsne_df_3d['Tumor Type'] = labels\n",
    "    \n",
    "    fig = px.scatter_3d(\n",
    "        tsne_df_3d, \n",
    "        x='TSNE1', \n",
    "        y='TSNE2', \n",
    "        z='TSNE3',\n",
    "        color='Tumor Type',\n",
    "        title=f'Transcriptome 3D t-SNE (Perplexity: {perp})',\n",
    "        labels={\n",
    "            'TSNE1': 't-SNE Component 1', \n",
    "            'TSNE2': 't-SNE Component 2',\n",
    "            'TSNE3': 't-SNE Component 3'\n",
    "        },\n",
    "        hover_name=tsne_df_3d.index,\n",
    "        template=\"plotly_white\",\n",
    "        width=900,\n",
    "        height=900\n",
    "    )\n",
    "    \n",
    "    fig.update_traces(marker=dict(size=3, opacity=0.7))\n",
    "    \n",
    "    filename = f'tSNE_3D_perp{perp}.html'\n",
    "    fig.write_html(path.join('plots', filename))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5bd45c",
   "metadata": {},
   "source": [
    "### UMAP\n",
    "\n",
    "With t-SNE we saw that assuming linearity is not optimal for RNA-seq dimensionality reduction, but neither t-SNE can capture the global structure of clusters. UMAP is, in fact, the model preferred for reducction of dimensionality in RNA-seq data given that preserves better the global structure of the data and it is computationally faster.\n",
    "\n",
    "However, the authors designed it to be fully **scikit-learn compatible**, which means it uses the exact same syntax (`fit`, `transform`, `fit_transform`) as the PCA and t-SNE tools you have been using.\n",
    "\n",
    "Let's try with combinations of the two hyperparameters. In order for connecting interactively multiple plotly plots, we will use Dash (a web app framework)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7308503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import numpy as np\n",
    "import umap\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Prepare subplots\n",
    "n_neighbors_list = [15, 50]\n",
    "min_dist_list = [0.1, 0.5]\n",
    "parameter_grid = list(it.product(n_neighbors_list, min_dist_list))\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    specs=[[{'type': 'scene'}, {'type': 'scene'}],\n",
    "           [{'type': 'scene'}, {'type': 'scene'}]],\n",
    "    subplot_titles=[f\"n_neigh={n}, min_dist={d}\" for n, d in parameter_grid],\n",
    "    vertical_spacing=0.05, horizontal_spacing=0.01\n",
    ")\n",
    "\n",
    "# Prepare Colors\n",
    "unique_labels = np.unique(labels)\n",
    "plotly_palette = px.colors.qualitative.Plotly \n",
    "color_map = {\n",
    "    label: plotly_palette[i % len(plotly_palette)] \n",
    "    for i, label in enumerate(unique_labels)\n",
    "}\n",
    "\n",
    "# Iterate over the grid positions\n",
    "grid_positions = [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "for (n_neigh, m_dist), (row, col) in zip(parameter_grid, grid_positions):\n",
    "    print(f\"Running UMAP: n_neighbors={n_neigh}, min_dist={m_dist}...\")\n",
    "\n",
    "    reducer = umap.UMAP(\n",
    "        metric='cosine',\n",
    "        n_components=3, \n",
    "        n_neighbors=n_neigh, \n",
    "        min_dist=m_dist, \n",
    "        random_state=42\n",
    "    )\n",
    "    umap_result_3d = reducer.fit_transform(data)\n",
    "    \n",
    "    sub_df = pd.DataFrame(\n",
    "        umap_result_3d, \n",
    "        columns=['x', 'y', 'z'], \n",
    "        index=expression_df.columns\n",
    "    )\n",
    "    sub_df['label'] = labels\n",
    "\n",
    "    # Add Traces to coordinate subplots(One trace per Tumor Type)\n",
    "    for label in unique_labels:\n",
    "        mask = sub_df['label'] == label\n",
    "        subset = sub_df[mask]\n",
    "        \n",
    "        # Only show legend item for the first subplot\n",
    "        show_legend = True if (row == 1 and col == 1) else False\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=subset['x'], y=subset['y'], z=subset['z'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=3,\n",
    "                    color=color_map[label], # Use the mapped Plotly color\n",
    "                    opacity=0.7\n",
    "                ),\n",
    "                name=str(label),\n",
    "                legendgroup=str(label), # Links the interactivity\n",
    "                showlegend=show_legend,\n",
    "                text=subset.index,\n",
    "                hovertemplate=\"<b>%{text}</b><br>\" +\n",
    "                              f\"Type: {label}<br>\"\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Transcriptome 3D UMAP Grid Search\",\n",
    "    height=1000, \n",
    "    width=1200,\n",
    "    template=\"plotly_white\",\n",
    "    legend=dict(itemsizing='constant', title='Tumor Type'),\n",
    "    margin=dict(l=10, r=10, b=10, t=80)\n",
    ")\n",
    "\n",
    "save_path = path.join('plots', 'UMAP_3D_Grid.html')\n",
    "fig.write_html(save_path)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36765a1",
   "metadata": {},
   "source": [
    "## Clustering algorithms\n",
    "\n",
    "Clustering algorithms are used to group data points together into clusters based on their relation to surrounding data points. Hence, they use similarity or distance measures in the feature space in an effort to discover dense regions of data points (hence, it is good practice to scale data prior to using clustering algorithms).\n",
    "\n",
    "There are many types of clustering algorithms but they have in common an iterative process identified clusters are evaluated and reported back to the algorithm configuration until the desired or appropriate number of clusters is achieved.\n",
    "\n",
    "Therefore, some clustering algorithms require the user to specify the number of clusters to discover in the data while others require only some minimum distance between observations, a theshold at which data points might be considered as \"close\" or \"connected\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49708258",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering generates a tree-like hierarchy of clusters known as a dendrogram through the iterative process. It does not require to specify the number of clusters beforehand but the user should subjectively define a posteriori the amount of clusters based on the dendogram. The iterative steps are the following:\n",
    "\n",
    "1. **Evaluate the distance between clusters** The algorithm computes the pairwise distance between all the clusters at the iteration (the algorithm starts by considering each data point as an individual cluster and ends when all data points are assigned to one cluster).\n",
    "\n",
    "2. **Merge the closest clusters**: The two clusters with the lowest distance between them are merged toghether into a new cluster. This distance is recorded on the dendogram as the length of the branch between the original two clusters and the new cluster (a new node in the dendogram). The algorith enters into the next iteration.\n",
    "\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Hierarchical clustering](https://dashee87.github.io/images/hierarch.gif)](https://dashee87.github.io/images/hierarch.gif)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Once the dendogram is generated, the shape could be interpreted to define the amount of desired clusters. Together with visual inspection and several performance metrics, such as the **Cophenetic Correlation Coefficient** that measures how faithfully the hierarchical clustering preserves pairwise distances between data points (close to 1 indicates good clustering) or the **Ward's method** (see below), it allows to evaluate how succesful the clustering has been.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "There are various linkage methods, that is, methods to measure the distance between clusters, where each one of them has the advantatge to proficiently detect specific shapes of clusters or the disadvantatge of be misguided by data with a different nature. Some examples are:\n",
    "\n",
    "- **Single linkage**: The distance is computed as the closest between two points such that one point lies in one cluster and the other point lies in the other. This method is able to separate non-elliptical shapes as long as the gap between the two clusters is not small, however, it has bad performance when there is noise between clusters.\n",
    "\n",
    "- **Complete linkage**: The distance is computed as the furthest between two points such that one point lies in one cluster and the other point lies in the other. In contrast, this method has a good performance when there is noise between clusters but is biased towards detecting globular clusters and tends to disgregate the large clusters.\n",
    "\n",
    "- **Average linkage**: The distance is computed as the average between all possible pairs of data points between clusters. Similar to the complete linkage, has a good performance with noise between clusters but is biased towards globular ones.\n",
    "\n",
    "- **Ward's method**: It is similar to the average linkage, but the average is computed over the sum of the square of pair-wise distances. The ward's method also serves as a performance metric where low values within each cluster suggest better performance.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Likage methods](https://miro.medium.com/v2/resize:fit:640/format:webp/0*s2KrCgCQIlEqcK_X)](https://medium.com/@u0808b100/unsupervised-learning-in-python-hierarchical-clustering-t-sne-41f17bbbd350)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "With respect to other clustering algotithms, the hierarchical clustering presents:\n",
    "\n",
    "**Advantages:**\n",
    "- No need to pre-specify the number of clusters.\n",
    "- Provides a hierarchical structure.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Computationally expensive, especially for large datasets.\n",
    "- Difficult to determine the optimal number of clusters (highly subjective).\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137aa61",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "K-Means clustering partitions the data into a predefined k number of clusters, where each cluster is defined by a centroid: a data point calculated as the mean of all the data points in the cluster. There are different algorithms, but all of them use an iterative procedure until a convergence solution is achieved. Roughly, they follow these two steps:\n",
    "\n",
    "1. **Assignment**: Each data point is assigned to the nearest centroid, generating K clusters at the current iteration. At the first iteration, the k initial cluster centroids are choosen at random in the space.\n",
    "\n",
    "2. **Update the centroids**: The k-centroids are recalculated based on the mean of the data points in each cluster. If after updating several times the data points on each cluster remain the same after assigment, the centroids remain the same after the update: convergence has been achieved and the algorithm stops.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![K-means clustering](https://sandipanweb.wordpress.com/wp-content/uploads/2016/08/k3.gif)](https://sandipanweb.wordpress.com/wp-content/uploads/2016/08/k3.gif)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Similar to hierarchical clustering, there are some metrics that reflect performance such as the **silhouette score**, which evaluates the intra-cluster compactness and between clusters separation or the **Ward's method**. Despite being unsupervised methods, if there is any information about the \"true\" clusters in the data, one can compute the **Adjusted Rand Index (ARI)** which measures the similarity between true and predicted clusters, adjusted for chance.\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "With respect to advantages and disadvantages compared to other clustering methods:\n",
    "\n",
    "**Advantages**:\n",
    "- Efficient and works well with large datasets.\n",
    "- Simple and easy to implement.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "**Disadvantages**:\n",
    "- Sensitive to initial centroid placement.\n",
    "- Assumes clusters are spherical and equally sized.\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc5108",
   "metadata": {},
   "source": [
    "### DBSCAN Clustering\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) groups data points based on the density of the region they are located in. Unlike K-Means, which assumes clusters are spherical, DBSCAN locates regions of high density that are separated from one another by regions of low density.\n",
    "\n",
    "The algorithm relies on two main hyperparameters to define \"density\": **ϵ (epsilon)**, the maximum distance between two samples for one to be considered as in the neighborhood of the other, and **MinPts**, the number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\n",
    "\n",
    "The iterative process generally follows these steps:\n",
    "\n",
    "1. **Classify Points**: The algorithm visits an arbitrary point. If it has at least *MinPts* points within a radius of ϵ, it is marked as a **Core Point** and a new cluster is started. If it has fewer neighbors, it is temporarily marked as Noise (though it might later be found to be a border point of another cluster).\n",
    "\n",
    "2. **Expand Cluster**: For every core point, the algorithm recursively visits all its density-connected neighbors. If a neighbor is also a core point, the cluster expands to include that point's neighbors. If a neighbor is not a core point but is within the ϵ radius of a core point, it is classified as a **Border Point** (part of the cluster, but does not expand it).\n",
    "\n",
    "Once the process is complete, points that were never reachable from any core point remain classified as Noise (outliers).\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![DBSCAN clustering](https://cdn-images-1.medium.com/max/640/1*tc8UF-h0nQqUfLC8-0uInQ.gif)](https://cdn-images-1.medium.com/max/640/1*tc8UF-h0nQqUfLC8-0uInQ.gif)\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "With respect to other clustering algorithms, DBSCAN presents:\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- **Arbitrary Shapes**: Can find non-linearly separable clusters (e.g., one cluster surrounding another) which K-Means cannot do.\n",
    "- **Robust to Outliers**: Explicitly identifies and isolates noise points rather than forcing them into a cluster.\n",
    "- **No pre-defined K**: Does not require the user to specify the number of clusters a priori.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- **Varying Densities**: Struggles to identify clusters correctly if the dataset has clusters of widely varying densities (since $\\epsilon$ and *MinPts* are fixed).\n",
    "- **Parameter Sensitivity**: The quality of clustering is highly sensitive to the proper selection of $\\epsilon$ and *MinPts*.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7535722",
   "metadata": {},
   "source": [
    "### Hierarchical o K-Means Clustering or DBSCAN?\n",
    "\n",
    "As always, depends on the nature of the data and the goals of the analysis. Hierarchical does not require to specify the initial number of clusters and decision can be done a posteriori evaluating the hierarchy, however it is computationally expensive. K-means clustering is more efficient, but requires a predefined expected number of clusters and is very sensitive to initializations.\n",
    "\n",
    "On the other hand, if clusters are not globular clusters and Hierarchical clustering does not offer structural insights, DBSCAN is the preferred choice:\n",
    " - when the data contains noise or outliers, or \n",
    " - when the clusters are expected to have irregular, non-convex shapes (such as crescent moons or rings).\n",
    " \n",
    "However, if the density of the data is not uniform across the dataset, DBSCAN may fail to detect all clusters simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2112f60",
   "metadata": {},
   "source": [
    "### Practical: Hierarchical clustering\n",
    "\n",
    "A priori we might not have any preliminary information like the primary types, we can try to extract clusters directly from the output of a dimensionality reduction method. t-SNE is not devoid of interpretation problems in this aspect (https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne), but UMAP is less prone to some artifacts that might generate interferences (https://github.com/lmcinnes/umap/issues/25).\n",
    "\n",
    "For starters let's assume that we do not know the underlying structure of primary types and try to extract 18 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e81e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "n_neigh = 50\n",
    "m_dist = 0.5\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    n_components=3, \n",
    "    n_neighbors=n_neigh, \n",
    "    min_dist=m_dist,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "umap_result_3d = reducer.fit_transform(data)\n",
    "\n",
    "umap_df = pd.DataFrame(\n",
    "    umap_result_3d, \n",
    "    columns=['x', 'y', 'z'], \n",
    "    index=expression_df.columns\n",
    ")\n",
    "umap_df['True Label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2686b9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster, cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def ward_linkage(X):\n",
    "    return linkage(X, method='ward')\n",
    "\n",
    "linkage_matrix = ward_linkage(umap_result_3d)\n",
    "\n",
    "# Assign cluster labels\n",
    "num_clusters = 18\n",
    "clusters = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
    "\n",
    "# Metrics\n",
    "c, coph_dists = cophenet(linkage_matrix, pdist(umap_result_3d))\n",
    "print(f\"Cophenetic correlation coefficient: {c:.4f}\")\n",
    "silhouette_avg = silhouette_score(umap_result_3d, clusters)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20846046",
   "metadata": {},
   "source": [
    "From a strictly geometric point of view, the **Cophenetic Correlation Coefficient (or Ward's method)**, which measures how faithfully the hierarchy preserves the pairwise distances between the original data points, shows a moderate level of fidelity (this metric ranges from -1 to 1, where higher values indicate better preservation). Moreover, the **Silhouette Score**, which measures cohesion of the clusters and also ranges from -1 to 1 with higher values indicating cohesed clusters, reflects a rather mid cohesion. However, these metrics do not have any value unless interpreted under the light of the nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d634fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Find the cut threshold (height) that results in 'num_clusters'\n",
    "dist_a = linkage_matrix[-(num_clusters), 2]\n",
    "dist_b = linkage_matrix[-(num_clusters-1), 2]\n",
    "threshold_distance = (dist_a + dist_b) / 2 # midpoint between the merge distances of cluster N and N+1\n",
    "\n",
    "# Plot the dendrogram with a horizontal line at the threshold\n",
    "dendro_fig = ff.create_dendrogram(\n",
    "    umap_result_3d, \n",
    "    linkagefun=ward_linkage, \n",
    "    color_threshold=threshold_distance,\n",
    "    labels=expression_df.columns\n",
    ")\n",
    "dendro_fig.add_shape(\n",
    "    type='line',\n",
    "    xref='paper', yref='y',\n",
    "    x0=0, y0=threshold_distance,\n",
    "    x1=1, y1=threshold_distance,\n",
    "    line=dict(color='red', width=2, dash='dash')\n",
    ")\n",
    "dendro_fig.update_layout(\n",
    "    title_text=f\"Interactive Dendrogram (Ward / UMAP) - Cut at {num_clusters} clusters\",\n",
    "    width=1000, \n",
    "    height=600,\n",
    "    template=\"plotly_white\",\n",
    "    xaxis_title=\"Samples\",\n",
    "    yaxis_title=\"Distance\",\n",
    "    showlegend=False\n",
    ")\n",
    "dendro_fig.update_xaxes(showticklabels=False)\n",
    "\n",
    "save_path_dendro = path.join('plots', 'Dendrogram_Plotly.html')\n",
    "dendro_fig.write_html(save_path_dendro)\n",
    "dendro_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dce9da",
   "metadata": {},
   "source": [
    "The red dashed line on the dendogram marks the threshold of ward's distance that has been used to define 18 clusters. How good is the clustering then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcebba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "umap_df['Cluster_Label'] = [f\"Cluster {c}\" for c in clusters]\n",
    "\n",
    "# Prepare colours for clusters\n",
    "unique_clusters = np.sort(np.unique(clusters))\n",
    "colors_clusters = px.colors.qualitative.Alphabet + px.colors.qualitative.Dark24\n",
    "color_map_clusters = {\n",
    "    f\"Cluster {c}\": colors_clusters[i % len(colors_clusters)] \n",
    "    for i, c in enumerate(unique_clusters)\n",
    "}\n",
    "\n",
    "# Prepare colours for true primary sites\n",
    "unique_true_labels = np.sort(umap_df['True Label'].unique())\n",
    "colors_true = px.colors.qualitative.Bold + px.colors.qualitative.Vivid\n",
    "color_map_true = {\n",
    "    label: colors_true[i % len(colors_true)] \n",
    "    for i, label in enumerate(unique_true_labels)\n",
    "}\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[[{'type': 'scene'}, {'type': 'scene'}]],\n",
    "    subplot_titles=(f\"Hierarchical Clusters (n={len(unique_clusters)})\", \"True Labels (Ground Truth)\"),\n",
    "    horizontal_spacing=0.05\n",
    ")\n",
    "\n",
    "# Clusters plot\n",
    "for clust_id in unique_clusters:\n",
    "    clust_name = f\"Cluster {clust_id}\"\n",
    "    mask = umap_df['Cluster_Label'] == clust_name\n",
    "    subset = umap_df[mask]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=subset['x'], y=subset['y'], z=subset['z'],\n",
    "            mode='markers',\n",
    "            marker=dict(size=3, color=color_map_clusters[clust_name], opacity=0.8),\n",
    "            name=clust_name,\n",
    "            legendgroup=\"group1\",  # Groups these in the legend\n",
    "            legendgrouptitle_text=\"Clusters\",\n",
    "            text=subset.index,\n",
    "            hovertemplate=f\"<b>Cluster: {clust_id}</b>\",\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# Primary sites plot\n",
    "for label in unique_true_labels:\n",
    "    mask = umap_df['True Label'] == label\n",
    "    subset = umap_df[mask]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=subset['x'], y=subset['y'], z=subset['z'],\n",
    "            mode='markers',\n",
    "            marker=dict(size=3, color=color_map_true[label], opacity=0.8),\n",
    "            name=str(label),\n",
    "            legendgroup=\"group2\",  # Groups these in the legend\n",
    "            legendgrouptitle_text=\"True Labels\",\n",
    "            text=subset.index,\n",
    "            hovertemplate=f\"<b>True Label: {label}</b>\",\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"3D UMAP Comparison: Clusters vs Ground Truth\",\n",
    "    height=800, \n",
    "    width=1600, # Wider to accommodate two plots\n",
    "    template=\"plotly_white\",\n",
    "    margin=dict(l=10, r=10, b=10, t=50),\n",
    "    legend=None,\n",
    ")\n",
    "common_scene_dict = dict(xaxis_title='UMAP 1', yaxis_title='UMAP 2', zaxis_title='UMAP 3')\n",
    "fig.update_layout(scene=common_scene_dict, scene2=common_scene_dict)\n",
    "\n",
    "save_path_compare = path.join('plots', 'UMAP_3D_Comparison.html')\n",
    "fig.write_html(save_path_compare)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87ccb00",
   "metadata": {},
   "source": [
    "Some clusters are clearly defined, and we know that reflect real expression pattern differences due to primary type location and even tumor subtypes (different blood cancers show different clusters that, in reality, reflect different types of tumors). However, on the nucleous of the UMAP plot there are some clusters that were extracted from groups of points with not so clear distinction between them. Here the expression patterns are not enough different to distinguish clear subdivisions and might not reflect any biological feature (at least it does not reflect the primary type location). We can make this interpretation thanks to external information about tumor type, but as an unsupervised method this is not implemented on its methodology.\n",
    "\n",
    "We can play with the dendogram to define other numbers of clusters (independently of this 18 clusters value we obtain from external information) or use the primary type external information to compute the **Adjusted Rand Index (ARI)** as a proxy of performance of the clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "def plotly_hierarchical_clustering_and_umap_3d(umap_result, true_labels, num_clusters_list, method='ward'):\n",
    "    \n",
    "    linkage_matrix = linkage(umap_result, method=method)\n",
    "    dendro_data = dendrogram(linkage_matrix, no_plot=True, color_threshold=0)\n",
    "    \n",
    "    # Dendogram plot\n",
    "    dendro_fig = go.Figure()\n",
    "    for i, d in zip(dendro_data['icoord'], dendro_data['dcoord']):\n",
    "            dendro_fig.add_trace(go.Scatter(\n",
    "                x=i, y=d,\n",
    "                mode='lines', \n",
    "                line=dict(color='black', width=1), # Strictly black\n",
    "                hoverinfo='none',\n",
    "                showlegend=False\n",
    "            ))\n",
    "\n",
    "    # Add threshold lines\n",
    "    threshold_palette = px.colors.qualitative.Dark24\n",
    "    for i, num_clusters in enumerate(num_clusters_list):\n",
    "        threshold_dist = linkage_matrix[-(num_clusters - 1), 2]\n",
    "        color = threshold_palette[i % len(threshold_palette)]\n",
    "        \n",
    "        dendro_fig.add_shape(\n",
    "            type='line',\n",
    "            xref='paper',\n",
    "            x0=0, x1=1,\n",
    "            y0=threshold_dist, y1=threshold_dist,\n",
    "            line=dict(color=color, width=2, dash='dash'),\n",
    "        )\n",
    "        \n",
    "        dendro_fig.add_trace(go.Scatter( # dummy legend\n",
    "            x=[None], y=[None], mode='lines',\n",
    "            line=dict(color=color, width=2, dash='dash'),\n",
    "            name=f'{num_clusters} Clusters'\n",
    "        ))\n",
    "\n",
    "    dendro_fig.update_layout(\n",
    "        title=\"Hierarchical Clustering Dendrogram (UMAP Data)\",\n",
    "        xaxis_title=\"Sample Index\",\n",
    "        yaxis_title=\"Distance\",\n",
    "        template=\"plotly_white\",\n",
    "        height=600,\n",
    "        width=800,\n",
    "        showlegend=True,\n",
    "        xaxis=dict(showticklabels=False, mirror=True)\n",
    "    )\n",
    "    dendro_fig.show()\n",
    "\n",
    "    # UMAP plots\n",
    "    df_umap = pd.DataFrame(umap_result, columns=['x', 'y', 'z'])\n",
    "    df_umap['True Label'] = true_labels\n",
    "\n",
    "    for i, num_clusters in enumerate(num_clusters_list):\n",
    "        clusters = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
    "        df_umap['Cluster_Label'] = [f\"Cluster {c}\" for c in clusters]\n",
    "        \n",
    "        ari = adjusted_rand_score(true_labels, clusters)\n",
    "        print(f'Clusters={num_clusters} | ARI: {ari:.4f}')\n",
    "\n",
    "        unique_clusters = np.sort(np.unique(clusters))\n",
    "        colors_clusters = px.colors.qualitative.Alphabet + px.colors.qualitative.Dark24\n",
    "        color_map_clusters = {\n",
    "            f\"Cluster {c}\": colors_clusters[i % len(colors_clusters)] \n",
    "            for i, c in enumerate(unique_clusters)\n",
    "        }\n",
    "\n",
    "        scatter_fig = go.Figure()\n",
    "        \n",
    "        for clust_id in unique_clusters:\n",
    "            clust_name = f\"Cluster {clust_id}\"\n",
    "            mask = df_umap['Cluster_Label'] == clust_name\n",
    "            subset = df_umap[mask]\n",
    "            \n",
    "            scatter_fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=subset['x'], y=subset['y'], z=subset['z'],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=3,\n",
    "                        color=color_map_clusters[clust_name],\n",
    "                        opacity=0.8\n",
    "                    ),\n",
    "                    name=clust_name,\n",
    "                    text=subset.index,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        scatter_fig.update_layout(\n",
    "            title_text=f\"3D UMAP (Clusters={num_clusters}) - ARI: {ari:.4f}\",\n",
    "            height=800, \n",
    "            width=1000,\n",
    "            template=\"plotly_white\",\n",
    "            legend=dict(itemsizing='constant', title='Cluster ID'),\n",
    "            scene=dict(xaxis_title='UMAP 1', yaxis_title='UMAP 2', zaxis_title='UMAP 3'),\n",
    "            margin=dict(l=10, r=10, b=10, t=50)\n",
    "        )\n",
    "        scatter_fig.show()\n",
    "\n",
    "num_clusters_list = [14, 18, 20]\n",
    "plotly_hierarchical_clustering_and_umap_3d(umap_result_3d, labels, num_clusters_list, method='ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d879eb",
   "metadata": {},
   "source": [
    "As shown in the plot, the Adjusted Rand Index is higher using 10 clusters, less than the 18. Note on the code that it is possible to change the linkage metric to another different than the **ward's method**. Feel free to experiment with other methodologies such as the **minimum** (also named single method), the **maximum** (or complete), ... The different values that the method parameter can take for the linkage function are collected in the scipy documentation (https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb093a3",
   "metadata": {},
   "source": [
    "### Practical: K-means clustering\n",
    "\n",
    "Similar to hierarchical clustering, we can use k-means clustering to extract plots from the UMAP results using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88ae384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def plotly_kmeans(umap_result, true_labels, num_clusters_list, random_seed=42):\n",
    "    \"\"\"\n",
    "    Performs K-Means clustering on UMAP 3D data and plots interactive 3D scatter plots.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_umap = pd.DataFrame(umap_result, columns=['x', 'y', 'z'])\n",
    "    df_umap['True Label'] = true_labels\n",
    "    colors_base = px.colors.qualitative.Alphabet + px.colors.qualitative.Dark24\n",
    "\n",
    "    for num_clusters in num_clusters_list:\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=random_seed, n_init='auto')\n",
    "        clusters = kmeans.fit_predict(umap_result)\n",
    "        \n",
    "        df_umap['Cluster_Label'] = [f\"Cluster {c}\" for c in clusters]\n",
    "        \n",
    "        ari = adjusted_rand_score(true_labels, clusters)\n",
    "        print(f'K-Means (k={num_clusters}) | ARI: {ari:.4f}')\n",
    "\n",
    "        unique_clusters = np.sort(np.unique(clusters))\n",
    "        color_map = {\n",
    "            f\"Cluster {c}\": colors_base[i % len(colors_base)] \n",
    "            for i, c in enumerate(unique_clusters)\n",
    "        }\n",
    "\n",
    "        fig = go.Figure()\n",
    "        for clust_id in unique_clusters:\n",
    "            clust_name = f\"Cluster {clust_id}\"\n",
    "            mask = df_umap['Cluster_Label'] == clust_name\n",
    "            subset = df_umap[mask]\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=subset['x'], y=subset['y'], z=subset['z'],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=3,\n",
    "                        color=color_map[clust_name],\n",
    "                        opacity=0.8\n",
    "                    ),\n",
    "                    name=clust_name,\n",
    "                    text=subset.index,\n",
    "                    hovertemplate=f\"<b>Cluster: {clust_id}</b>\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text=f\"K-Means Clustering (k={num_clusters}) - ARI: {ari:.4f}\",\n",
    "            height=800, \n",
    "            width=1000,\n",
    "            template=\"plotly_white\",\n",
    "            legend=dict(itemsizing='constant', title='Cluster ID'),\n",
    "            scene=dict(\n",
    "                xaxis_title='UMAP 1',\n",
    "                yaxis_title='UMAP 2',\n",
    "                zaxis_title='UMAP 3'\n",
    "            ),\n",
    "            margin=dict(l=10, r=10, b=10, t=50)\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "\n",
    "num_clusters_list = [14, 18, 20]\n",
    "random_seed = 50  # Change the random seed to see how much the algorithm depends on initialization conditions\n",
    "plotly_kmeans(umap_result_3d, labels, num_clusters_list, random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f2ef63",
   "metadata": {},
   "source": [
    "Using the code above, you can try to explore how the **Adjusted Rand Index (ARI)** (using the primary cancer types as proxy of \"true\" groups to observe) changes with different number of clusters for the **K-means clustering** algorithm.\n",
    "\n",
    "You can also try different seeds to initialize the clustering algorithm at random. You will notice how dependent on the initial random conditions the algorithm is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd05e6",
   "metadata": {},
   "source": [
    "### Practical: DBSCAN\n",
    "\n",
    "A common heuristic to choose the optimal $\\epsilon$ is the **k-distance graph**, where the distance to the $k$-th nearest neighbor is plotted; the \"elbow\" of the curve usually suggests a good $\\epsilon$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48763068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def plotly_k_distance(data, min_samples=5):\n",
    "    \"\"\"\n",
    "    Plots an interactive k-distance graph using Plotly to help find the optimal epsilon.\n",
    "    \"\"\"\n",
    "\n",
    "    neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
    "    neighbors_fit = neighbors.fit(data)\n",
    "    distances, _ = neighbors_fit.kneighbors(data)\n",
    "    sorted_distances = np.sort(distances[:, min_samples-1])\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=sorted_distances,\n",
    "        mode='lines',\n",
    "        name='K-Distance',\n",
    "        line=dict(color='royalblue', width=2),\n",
    "        hovertemplate=\"<b>Point Index</b>: %{x}<br><b>Distance (eps)</b>: %{y:.4f}<extra></extra>\"\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"K-Distance Graph (min_samples={min_samples})<br>\",\n",
    "        xaxis_title=\"Data Points (sorted by distance)\",\n",
    "        yaxis_title=f\"Distance to {min_samples}-th Nearest Neighbor\",\n",
    "        template=\"plotly_white\",\n",
    "        height=600,\n",
    "        width=900,\n",
    "        hovermode=\"x unified\" \n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "plotly_k_distance(umap_result_3d, min_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b5e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def plotly_dbscan(umap_result, true_labels, eps, min_samples):\n",
    "    \"\"\"\n",
    "    Performs DBSCAN and plots 3D results.\n",
    "    Use a distinct colour for noise.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_umap = pd.DataFrame(umap_result, columns=['x', 'y', 'z'])\n",
    "    colors_base = px.colors.qualitative.Alphabet + px.colors.qualitative.Dark24\n",
    "    \n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    clusters = dbscan.fit_predict(umap_result)\n",
    "    \n",
    "    df_umap['Cluster_Label'] = clusters\n",
    "    \n",
    "    ari = adjusted_rand_score(true_labels, clusters)\n",
    "    \n",
    "    # Calculate stats\n",
    "    n_clusters_ = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "    n_noise_ = list(clusters).count(-1)\n",
    "    \n",
    "    print(f'>> DBSCAN Config: eps={eps}, min_samples={min_samples}')\n",
    "    print(f'   Estimated clusters: {n_clusters_}')\n",
    "    print(f'   Noise points: {n_noise_} ({n_noise_/len(clusters):.1%} of data)')\n",
    "    print(f'   ARI: {ari:.4f}')\n",
    "\n",
    "    unique_clusters = np.sort(np.unique(clusters))\n",
    "    \n",
    "    # Map colors. Use a distinct color for noise (-1)\n",
    "    color_map = {}\n",
    "    color_iter = iter(colors_base)\n",
    "    for c in unique_clusters:\n",
    "        if c == -1:\n",
    "            color_map[c] = 'lightgrey'\n",
    "        else:\n",
    "            color_map[c] = next(color_iter)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for clust_id in unique_clusters:\n",
    "        mask = df_umap['Cluster_Label'] == clust_id\n",
    "        subset = df_umap[mask]\n",
    "        \n",
    "        if clust_id == -1: # Noise aesthetics\n",
    "            clust_name = \"Noise (-1)\"\n",
    "            opacity = 0.2\n",
    "            size = 2\n",
    "        else:\n",
    "            clust_name = f\"Cluster {clust_id}\"\n",
    "            opacity = 0.9\n",
    "            size = 3\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=subset['x'], y=subset['y'], z=subset['z'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=size,\n",
    "                    color=color_map[clust_id],\n",
    "                    opacity=opacity\n",
    "                ),\n",
    "                hovertemplate=f\"<b>{clust_name}</b>\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"DBSCAN (eps={eps}, min_samples={min_samples})\",\n",
    "        height=800, \n",
    "        width=1000,\n",
    "        template=\"plotly_white\",\n",
    "        legend=dict(itemsizing='constant', title='Cluster ID'),\n",
    "        scene=dict(\n",
    "            xaxis_title='UMAP 1',\n",
    "            yaxis_title='UMAP 2',\n",
    "            zaxis_title='UMAP 3'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "optimal_eps = 0.47\n",
    "min_samples = 7\n",
    "\n",
    "plotly_dbscan(umap_result_3d, labels, optimal_eps, min_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec5981",
   "metadata": {},
   "source": [
    "# Basic supervised machine learning methods\n",
    "\n",
    "**In this practical session you will:**\n",
    "\n",
    "   - Know the difference between train and test dataset.\n",
    "   - Identify signals that point to violations of the regression model assumptions.\n",
    "   - Apply feature selection for datasets with many variables.\n",
    "\n",
    "   ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c48ec48",
   "metadata": {},
   "source": [
    "As already explained, supervised methods rely on datasets whose expected ouptut is previously known to train models. Therefore, this kind of machine learning methods rely on several datasets (or partitions of the same dataset) for the development, fine-tuning and evaluation of models:\n",
    "\n",
    "- **Training dataset:** The initial portion of the available data used to train the model, when the model learns patterns, relationships, and features present in the data. This is acomplished through the machine learning algorithm by optimizing the internal parameters of the model through minimization of the difference between the predicted outputs and the expected output (the labels).\n",
    "\n",
    "- **Validation dataset:** A separate portion of the data that is not used during the training phase. It serves as an independent set to assess the model's performance during development and hyperparameter tuning, allowing an adjustment to prevent overfitting towards the training data. This dataset and the validation step might be skipped in some cases or substituted by **cross-validation** techniques (see below).\n",
    "\n",
    "- **Testing dataset:** The testing dataset is another independent portion of the data that is kept completely separate and untouched during both training and validation. It is used to evaluate the final performance of the model after the training and validation phases, providing an unbiased estimate of its ability to generalize to new, unseen data. It helps assess whether the model has learned meaningful patterns after both training and validation instead of simply memorizing specific data.\n",
    "\n",
    "The process of sividing the original dataset is known as **data splitting**, which tends to favour the training set as the largest sub-dataset and the validation and testing equally smaller. However, the relative sizes can greatly vary between ~60% to ~80% for the training dataset depending on the total size of the dataset and the machine learning method to use. Once the model is fully trained, validated and tested it can be used with new sets of data and generate trustful outputs from which meaningful biological information could be extracted.\n",
    "\n",
    "![Use of data in supervised learning](images/Scheme1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cdc29f",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "It is also very important the concept of **feature selection** as part of the data processing steps prior to the use of a supervised method. **Feature selection** algorithms try to find combinations feature subsets, along with an evaluation measure to score them, to optimize the training steps. The idea behind performing this step is that the data might contain some **irrelevant or redundant** features and excluding them provides several advantatges:\n",
    "\n",
    "- Simpler models are more easy to manage and interpret.\n",
    "- Simpler models take shorter training times.\n",
    "- Reducing the dimensionality helps to avoid the curse of dimensionality (as the number of dimensions increases, the amount of data we need to produce accurate generalistic models grows exponentially).\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Since it is virtually impossible (computationally intractable) to test each possible subset of features and find the one that minimizes the error for datasets highly multidimensional, there are several types of algorithms that approach this problem classified by the evaluation metrics:\n",
    "\n",
    "\n",
    "- **Wrapper methods** use a predictive model to score the feature subsets. Each new subset is used to train a model, which is tested on a test set, from which an error rate is obtained as the score for that specific feature subset.\n",
    "\n",
    "\n",
    "    - **Advantatge**: Usually provide the best performing feature set model and problem-wise.\n",
    "    - **Disadvantatge**: Very computationally intensive (train a new model for each subset).\n",
    "    - **Examples**: Recursive Feature Elimination (RFE), Forward Feature Selection (RFS), Backward Feature Elimination (BFE).\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "- **Filter methods** use a statistical metric, a proxy, to score the subsets instead of the error rate. This metric is fast to compute but still able to capture the usefulness of the feature set. Some algorithms also provide a feature ranking rather than the best feature subset, which allows to empirically choose some cutoff threshold through cross-validation techniques (then it is an **Hybrid method**). These methods are also used as a pre-processing step before applying more reliable but computationally expensive Wrapper methods.\n",
    "\n",
    "    - **Disadvantatge**: The feature subset is not model and problem-wise tunned.\n",
    "    - **Advantatge**: Usually less computationally intensive than wrappers. Moreover, it does not contain any predictive model assumnptions, so it is more useful for exposing relationships between the features.\n",
    "    - **Examples**: ANOVA, correlation metrics.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "- **Embedded methods** is a very diverse group of methods that perform the feature selection step as part of the model construction process. The computational complexity is usually between the one of filter and wrapper methods.\n",
    "\n",
    "    - **Examples**: LASSO regression and derivates, random forest.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "- **Hybrid methods** have both characteristic of Wrapper and Filter methods.\n",
    "\n",
    "    - **Examples**: Recursive Feature Elimination with cross-validation (RFECV)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Feature selection methods](https://miro.medium.com/v2/resize:fit:720/format:webp/1*9h2qPmOJonbCdthfeVkuyg.jpeg)](https://miro.medium.com/v2/resize:fit:720/format:webp/1*9h2qPmOJonbCdthfeVkuyg.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900fbe23",
   "metadata": {},
   "source": [
    "## Regression Methods\n",
    "\n",
    "Regression methods are used to model the output of a continous variable (the dependent variable) based on one or multiple continous explanatory variables (independent variables). These methods aim to model the relationship between the input features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fed7075",
   "metadata": {},
   "source": [
    "### Linear Regression:\n",
    "\n",
    "Linear regression is a simple and widely used regression method that models the relationship between the dependent variable and one or more independent variables.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Linear regression](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/05/2.3.png)](https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-your-first-machine-learning-model-linear-regression/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "The model assumes:\n",
    "\n",
    "- **Linear relationship** between the independent and dependent variables.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Linear relationship](https://editor.analyticsvidhya.com/uploads/96503linear-nonlinear-relationships.png)](https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-your-first-machine-learning-model-linear-regression/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "- **Normality:** Both dependent and independent variables should be normally distributed. It is important to check for deviations of normality both on skweness or kurtosis.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Normality](https://miro.medium.com/v2/resize:fit:1024/1*OylqZbAZGJJMDiuM1gHUnw.jpeg)](https://medium.com/omics-diary/how-to-test-normality-skewness-and-kurtosis-using-python-18fb2d8e35b9)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "- **Homoskedasticity:** The model assumes that the error term is constant across the explanatory variable. If the variance of the the error term depends on the explanatory variable, the linear regression model will be innapropiate to model the data (since it assumes a constant variability).\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Homoskedasticity](https://editor.analyticsvidhya.com/uploads/51367residuals.png)](https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-your-first-machine-learning-model-linear-regression/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Therefore, linear regression presents:\n",
    "\n",
    "**Advantages:**\n",
    "- Simple and interpretable.\n",
    "- Fast computation.\n",
    "\n",
    "**Disadvantages:**\n",
    "- The model has little no none predictive power beyond assumptions.\n",
    "- Assumes absence of collinearity between independent variables (if the model includes more than one)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f251c",
   "metadata": {},
   "source": [
    "We will try to generate a model that relates some clinical features (phenotype) with a genomic characteristic: the total tumor mutation burden. Mutations, and somatic mutations too, accumulate with time on the body tissues. Tumors arising on older people could show more somatic mutations, and hence, the age of the patient could be used to roughly predict the amount of mutations on their tumors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7fc494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import pandas as pd\n",
    "\n",
    "clinical_df = pd.read_excel('data/pcawg_donor_clinical_August2016_v9.xlsx')\n",
    "\n",
    "sample_df = pd.read_csv(path.join('data', 'sample_df.tsv'), sep='\\t', header='infer')\n",
    "specimen_dict = dict(zip(sample_df.icgc_specimen_id, sample_df.icgc_donor_id))\n",
    "\n",
    "TMB_df = pd.read_csv(path.join('data', 'TMB.tsv.gz'), sep='\\t', header='infer', compression='gzip')\n",
    "TMB_df['donor'] = TMB_df['specimenID'].map(specimen_dict)\n",
    "\n",
    "# Since the analysis is performed by donor clinical data, we can take the mean value across specimens\n",
    "TMB_df.groupby('donor')\n",
    "TMB_clean_df = TMB_df.groupby('donor')['TMB_proxy'].mean().to_frame().reset_index()\n",
    "\n",
    "merged_df = pd.merge(clinical_df, TMB_clean_df, left_on='icgc_donor_id', right_on='donor', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aee67c",
   "metadata": {},
   "source": [
    "One of the problems we might encounter during the training of models is **missing data** in one of the variables used by the model. There are multiple ways to deal with that, the most simple (but more restrictive) is excluding any data element whose value on some feature is unknown.\n",
    "\n",
    "Remember than on the first session we looked at the NAs across variables of the clinical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total donors: ' + str(len(merged_df)))\n",
    "df_regression = merged_df[['TMB_proxy', 'donor_age_at_diagnosis', 'project_code']].dropna()\n",
    "print('Donors with available age at diagnosis: ' + str(len(df_regression)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4f124d",
   "metadata": {},
   "source": [
    "Note that around 100 donors are dropped from analysis due to non-available age at diagnosis data. This is a small proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c7bab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Check for normality using Shapiro-Wilk test\n",
    "_, p_value_tmb = stats.shapiro(df_regression['TMB_proxy'])\n",
    "_, p_value_age = stats.shapiro(df_regression['donor_age_at_diagnosis'])\n",
    "\n",
    "# Definitely the two variables do not follow a normal distribution\n",
    "print(f'Shapiro-Wilk p-value for TMB: {p_value_tmb}')\n",
    "print(f'Shapiro-Wilk p-value for Age at Diagnosis: {p_value_age}')\n",
    "\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Histogram of TMB', 'Histogram of Age at Diagnosis')\n",
    ")\n",
    "\n",
    "# TMB\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=df_regression['TMB_proxy'],\n",
    "        histnorm='probability density',\n",
    "        name='TMB Hist',\n",
    "        marker_color='skyblue',\n",
    "        opacity=0.7\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Age\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=df_regression['donor_age_at_diagnosis'],\n",
    "        histnorm='probability density',\n",
    "        name='Age Hist',\n",
    "        marker_color='salmon',\n",
    "        opacity=0.7\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Distributions of Continuous Variables\",\n",
    "    height=500,\n",
    "    width=1200,\n",
    "    showlegend=False,\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1e13bb",
   "metadata": {},
   "source": [
    "Definetly, the two variables do not follow a normal distribution at all. The first outcome of this fact is that a linear regression model is not suitable for modelling this type of data. However, there are ways to partially solve this problem by adapting the model through transformation of the variables, although this will have implications on the interpretation of the model, since the linearity will be between two transformed variables.\n",
    "\n",
    "We can try to apply some transformations:\n",
    "- A logarithmic transformation for the TMB could work well (it looks like a gamma distribution or other long-tailed distributions)\n",
    "- The age seems to arise from a mixture of two normal distributions. We can try to identify which subpopulations are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4207b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regression['transformed_TMB'] = np.log1p(df_regression['TMB_proxy'])\n",
    "\n",
    "# Re-check for normality\n",
    "_, p_value_tmb = stats.shapiro(df_regression['transformed_TMB'])\n",
    "\n",
    "# Still not follow a normal distribution, but much closer\n",
    "print(f'Shapiro-Wilk p-value for log TMB: {p_value_tmb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a91040",
   "metadata": {},
   "source": [
    "With respect to the age at diagnosis, there are two populations: pediatric cancers and non-pediatric ones. \n",
    "\n",
    "The pediatric cancers usually arise from predisposition syndromes (germline mutations that predispose to develop a cancer), with different mutational dynamics in terms of mutation load than other cancers, which are hypothesized to arise from the accumulation of somatic mutations with time. Hence, we can try to exclude the pediatric cancers and model the relationship of the age and tumor mutation burden for non-pediatric neoplasies.\n",
    "\n",
    "There are two projects including pediatric cancers: the ones from the PBCA-DE cohort (pediatric brain cancer) and some donors from the malignant lymphomas MALY-DE cohort (which also is a bimodal, whith pediatric lymphomas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316aaa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total donors: ' + str(len(merged_df)))\n",
    "\n",
    "# Remove pediatric brain cancers and pediatric malignant lymphomas\n",
    "df_regression = df_regression[\n",
    "    (df_regression['project_code']!='PBCA-DE')&\n",
    "    ~((df_regression['project_code']=='MALY-DE')&(df_regression['donor_age_at_diagnosis']<20))\n",
    "    ]\n",
    "\n",
    "print('Non-pediatric donors with available age at diagnosis: ' + str(len(df_regression)))\n",
    "\n",
    "# Check for normality using Shapiro-Wilk test\n",
    "_, p_value_age = stats.shapiro(df_regression['donor_age_at_diagnosis'])\n",
    "\n",
    "# Definitely the two variables do not follow a normal distribution\n",
    "print(f'Shapiro-Wilk p-value for trimmed Age at Diagnosis: {p_value_age}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0772ab1",
   "metadata": {},
   "source": [
    "Let's see the distributions now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c212352",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Histogram of TMB', 'Histogram of Age at Diagnosis')\n",
    ")\n",
    "\n",
    "# TMB\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=df_regression['transformed_TMB'],\n",
    "        histnorm='probability density',\n",
    "        name='TMB Hist (log-scale)',\n",
    "        marker_color='skyblue',\n",
    "        opacity=0.7\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Age\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=df_regression['donor_age_at_diagnosis'],\n",
    "        histnorm='probability density',\n",
    "        name='Age Hist (trimmed)',\n",
    "        marker_color='salmon',\n",
    "        opacity=0.7\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Distributions of Continuous Variables\",\n",
    "    height=500,\n",
    "    width=1200,\n",
    "    showlegend=False,\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6805898",
   "metadata": {},
   "source": [
    "Still after the transformations the distributions do not look normal at all, which is reflected on the Shapiro-Wilk test. We can do a deeper analysis with qqplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a13f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import probplot\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('QQ: Transformed TMB', 'QQ: Age at diagnosis'))\n",
    "\n",
    "variables = ['transformed_TMB', 'donor_age_at_diagnosis']\n",
    "\n",
    "for i, var in enumerate(variables, start=1):\n",
    "    # Calculate QQ values: ((quantiles, values), (slope, intercept, r))\n",
    "    (osm, osr), (slope, intercept, _) = probplot(df_regression[var].dropna(), plot=None)\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=osm, y=osr, mode='markers', name=var), row=1, col=i)\n",
    "    fig.add_trace(go.Scatter(x=osm, y=slope*osm + intercept, mode='lines', line=dict(color='red')), row=1, col=i)\n",
    "\n",
    "fig.update_layout(height=500, width=1000, showlegend=False, title_text=\"QQ Plots\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ed6c44",
   "metadata": {},
   "source": [
    "- The qqplot from the logarithm of TMB indicates negative kurtosis (data too accumulated on the peak): the distribution is leptokurtic.\n",
    "- With respect to the age at diagnosis, still the distribution is still skewed. \n",
    "\n",
    "Anyway, let's train a linear regression model with this data and evaluate its usage as a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07039b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# sk-learn needs data to be provided in proper dimensions\n",
    "y = df_regression['transformed_TMB']\n",
    "X = df_regression['donor_age_at_diagnosis'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Intercept: {model.intercept_}')\n",
    "print(f'Age Coefficient: {model.coef_[0]}')\n",
    "\n",
    "# Get the root of the mean squared error as a metric of the fit\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'Root of the mean squared error: {rmse}')\n",
    "\n",
    "# Calculate R2\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "fig_reg = go.Figure()\n",
    "\n",
    "fig_reg.add_trace(go.Scatter(\n",
    "    x=X_test.squeeze(),\n",
    "    y=y_test,\n",
    "    mode='markers',\n",
    "    name='Actual Data',\n",
    "    marker=dict(color='blue')\n",
    "))\n",
    "\n",
    "fig_reg.add_trace(go.Scatter(\n",
    "    x=X_test.squeeze(),\n",
    "    y=y_pred,\n",
    "    mode='lines',\n",
    "    name='Linear Regression',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "fig_reg.update_layout(\n",
    "    title='Linear Regression',\n",
    "    xaxis_title='Age',\n",
    "    yaxis_title='Transformed TMB',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig_reg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecede38d",
   "metadata": {},
   "source": [
    "Apparently age is a poor predictor of the order of magnitude since:\n",
    "\n",
    "- Age accounts for only 6% of the variation in the order of magnitude of the TMB. The other 94% is unexplained noise.\n",
    "- The line is practically flat: Coef ≈ 0.02. This means the relationship is rather weak that a 10-year difference in age only changes the order of magnitude of mutations by 0.2.\n",
    "- Note that after transforming the dependent variable into logrithmic scale, largely complicates the interpretation of the RMSE: long story short, a RMSE computed from a log-transformation looses its original units and cannot be interpreted anymore as number of mutations but as % of deviations from the geometric mean (which in our case translates into orders of magnitude of difference of error for TMB).\n",
    "\n",
    "Still, we can look at the distribution of residuals, apparently there is homoskedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "\n",
    "fig_res = go.Figure()\n",
    "\n",
    "fig_res.add_trace(go.Scatter(\n",
    "    x=y_pred,\n",
    "    y=residuals,\n",
    "    mode='markers',\n",
    "    name='Residuals',\n",
    "    marker=dict(color='steelblue', opacity=0.7)\n",
    "))\n",
    "\n",
    "fig_res.add_hline(y=0, line_dash=\"dash\", line_color=\"red\")\n",
    "\n",
    "fig_res.update_layout(\n",
    "    title='Residuals vs Fitted Values',\n",
    "    xaxis_title='Fitted Values',\n",
    "    yaxis_title='Residuals',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e850d71",
   "metadata": {},
   "source": [
    "### Polynomial Regression:\n",
    "\n",
    "Polynomial regression is an extension of linear regression, allowing for the modeling of non-linear relationships. It includes higher-order terms of the independent variable, which allows to capture non-linear relationships in contrast to linear regressions. Therefore:\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n",
    "\n",
    "**Advantages:**\n",
    "- Captures non-linear relationships.\n",
    "\n",
    "**Disadvantages:**\n",
    "- May overfit the data.\n",
    "- Assumes absence of collinearity between independent variables (if the model includes more than one).\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b592e966",
   "metadata": {},
   "source": [
    "For the practical example, we can try to model the logarithm of the TMB (the order of magnitude) based on mutation signature proportions, whose relationship is not expected to be completely linear.\n",
    "\n",
    "Although the total TMB and the number of mutations attrributed to a specific signature are strictly related (one is a subset of another), the proportion of mutations should be independent (it is independent from the total number of mutations as it has been corrected by them). However, tumors whose somatic mutation burden is dominated by specific mutational processes usually show an hypermutation phenotype, for instance melanomas are hypermutated cancers mostly dominated by signature 7 (UV-light derived mutations) or signature 10 (associated with Polymerase-Epsilon deficient *POLE* mutants) in colorectal and endometrial cancers.\n",
    "\n",
    "Let's see which signatures are relevant for this, and what problems can pose to work with proportions as independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8792a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_signatures_df = pd.read_csv(path.join('data' , 'signatures.tsv.gz'), sep='\\t', header='infer', compression='gzip')\n",
    "raw_signatures_df = raw_signatures_df.set_index('signature')\n",
    "\n",
    "raw_signatures_df = raw_signatures_df.transpose().reset_index()\n",
    "\n",
    "# Some signatures that were extracted at the start of the cancer genomics field were subdivided into more components\n",
    "# (7 was subdivided into 7a, 7b and 7c while 17 into 17a and 17b). To simplify we will merge into one component.\n",
    "raw_signatures_df['SBS7a'] = raw_signatures_df[['SBS7a', 'SBS7b', 'SBS7c']].sum(axis=1)\n",
    "raw_signatures_df['SBS17a'] = raw_signatures_df[['SBS17a', 'SBS17b']].sum(axis=1)\n",
    "raw_signatures_df = raw_signatures_df.rename(columns={'index': 'specimenID', \n",
    "                                              'SBS7a': 'SBS7', \n",
    "                                              'SBS17a': 'SBS17',\n",
    "                                              'SBS10a': 'SBS10'})\n",
    "raw_signatures_df = raw_signatures_df.drop(['SBS7b', 'SBS7c', 'SBS17b', 'SBS60', 'SBS83'], axis=1)\n",
    "\n",
    "# Normalize the values in each column to generate the proportions of each signature\n",
    "prop_signatures_df = raw_signatures_df.iloc[:, 1:].div(raw_signatures_df.iloc[:, 1:].sum(axis=1), axis=0)\n",
    "prop_signatures_df.insert(0, 'specimenID', raw_signatures_df['specimenID'])\n",
    "\n",
    "# Merge the log of TMB with the signatures dataframe to get the dependent variable\n",
    "TMB_df = pd.read_csv(path.join('data' , 'TMB.tsv.gz'), sep='\\t', header='infer', compression='gzip')\n",
    "TMB_df ['transformed_TMB'] = np.log1p(TMB_df ['TMB_proxy'])\n",
    "\n",
    "signatures_df = pd.merge(prop_signatures_df, TMB_df , left_on='specimenID', right_on='specimenID', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280614f",
   "metadata": {},
   "source": [
    "We can start to check the relationship of some specific signatures with the total TMB. For instance, starting with signature SBS4 (tobacco).\n",
    "\n",
    "We will exclude cancers that do not show mutations attributed to this mutagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188fa6b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_filtered = signatures_df[signatures_df['SBS4'] > 0.01]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_filtered['SBS4'],\n",
    "    y=df_filtered['transformed_TMB'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=8,\n",
    "        opacity=0.7,\n",
    "        line=dict(width=1, color='DarkSlateGrey')\n",
    "    ),\n",
    "    name='Data'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Proportion of SBS4 vs log TMB',\n",
    "    xaxis_title='Proportion of SBS4',\n",
    "    yaxis_title='log TMB',\n",
    "    template='plotly_white',\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bd01a5",
   "metadata": {},
   "source": [
    "The samples that have mutations attributed to tobacco might have a non-lineal relationship with the order of magnitude of mutations, it seems to approach a plateau as the proportion increases.\n",
    "\n",
    "Let's train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4898ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "    \n",
    "def polynomial_regression(df_filtered, feature_col, target_col='transformed_TMB', degree=2):\n",
    "\n",
    "    X = df_filtered[[feature_col]]\n",
    "    y = df_filtered[target_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    pm = make_pipeline(PolynomialFeatures(degree=degree, include_bias=False), LinearRegression())\n",
    "    pm.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pm.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    lin_reg_model = pm.named_steps['linearregression']\n",
    "    \n",
    "    print(f'Intercept: {lin_reg_model.intercept_:.4f}')\n",
    "    print(f'Coefficients (Linear, Quadratic): {lin_reg_model.coef_}')\n",
    "    print(f'R^2 score: {r2:.4f}')\n",
    "\n",
    "    # Dummy data for smooth curve\n",
    "    X_range_array = np.linspace(X_train[feature_col].min(), X_train[feature_col].max(), 1000).reshape(-1, 1)\n",
    "    \n",
    "    X_range_df = pd.DataFrame(X_range_array, columns=[feature_col])\n",
    "    y_range_pred = pm.predict(X_range_df)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X_train[feature_col],\n",
    "        y=y_train,\n",
    "        mode='markers',\n",
    "        name='Training Data',\n",
    "        marker=dict(color='blue', opacity=0.6)\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X_range_df[feature_col],\n",
    "        y=y_range_pred,\n",
    "        mode='lines',\n",
    "        name=f'Poly Fit (d={degree})',\n",
    "        line=dict(color='red', width=3)\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=f\"Polynomial Regression: {feature_col} vs {target_col}\",\n",
    "        xaxis_title=f\"Proportion of {feature_col}\",\n",
    "        yaxis_title=target_col,\n",
    "        height=600, \n",
    "        width=800, \n",
    "        template='plotly_white',\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "df_filtered = signatures_df[signatures_df['SBS4'] > 0.01]\n",
    "polynomial_regression(df_filtered, feature_col='SBS4', target_col='transformed_TMB', degree=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f744619",
   "metadata": {},
   "source": [
    "You might have noticed the model uses a pipeline where features are transformed with `PolynomialFeatures(degree=degree, include_bias=False)` and then passed to `LinearRegression()`. Why is that?\n",
    "\n",
    "**1. Why use LinearRegression for a curve?**\n",
    "Mathematically, polynomial regression is still considered **linear regression**. This is because the model is \"linear in the parameters\" (the coefficients $\\beta$), even though the variables ($x$) are squared ($y = \\beta_0 + \\beta_1 x + \\beta_2 x^2$).\n",
    "\n",
    "In scikit-learn, `PolynomialFeatures` generates the new columns ($x$ and $x^2$). The `LinearRegression` solver then sees these as distinct features and simply finds the best weights for them, just like in standard multiple regression.\n",
    "\n",
    "**2. Why include_bias=False?**\n",
    "By default, `PolynomialFeatures` adds a \"bias\" column filled with 1s. However, the `LinearRegression` object *also* calculates an intercept (bias) automatically. We use `include_bias=False` to prevent double-counting this term, essentially letting the regression step handle the intercept calculation alone to avoid mathematical redundancy.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bd4c39",
   "metadata": {},
   "source": [
    "A quick look might suggest that specimens were at least tobacco-related mutations are present tend to have a higher total tumor mutation burden in logarithmic scale. However, only using this signature is not enough to have a decent predictive model. As reflected on the small R-squared, most of samples do not carry any mutation attributed to tobacco but still there is a large variability of log TMB due to other mutational processes.\n",
    "\n",
    "We can look to other mutational processes. For instance, signature 7 (attributed to UV-light exposition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f27eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = signatures_df[signatures_df['SBS7'] > 0.01]\n",
    "polynomial_regression(df_filtered, feature_col='SBS7', target_col='transformed_TMB', degree=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92b627",
   "metadata": {},
   "source": [
    "Note than in this case the variability explained is far larger and the relationship seems different: the polynomial model approximates the relationship across two differentiated populations. Samples with low contribution of the UV-light signature seem to have more mutations that the mean across samples with no presence of this signature but still lower than a population of hypermutated samples that tend to have high proportions of SBS7 signature (intermediate samples are almost non-existent).\n",
    "\n",
    "Now let's see what is the relationship with signatures largely common across specimens like SBS1, which have been attributed to the background processes that generate mutations on all types of cells (even healthy ones)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36633916",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = signatures_df[signatures_df['SBS1'] > 0.01]\n",
    "polynomial_regression(df_filtered, feature_col='SBS1', target_col='transformed_TMB', degree=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33384b",
   "metadata": {},
   "source": [
    "In this case the proportion of variability explained in the order of magnitude is around the 40%. Let's discuss the behaviour of SBS1.\n",
    "\n",
    "As stated above, it is a mutational process present in mostly all types of cells (healthy ones included, it usually reflects the dominant mutational processes across the cells of the human body and even species: https://www.nature.com/articles/s41586-022-04618-z). The polynomial regression here reflects that cells with high proportions of this 'base' mutational process  tend to have less mutations than others, although still there is a large variance that should de explained by the presence of other mutational processes (some like SBS7 or SBS10, which their presence is associated with hypermutator phenotypes).\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdadab97",
   "metadata": {},
   "source": [
    "Therefore, after reviewing the relationships with these key signatures we can conclude that we will need a combination of them to explain a large part of the variance in the order of magnitude of mutations. However, the fact that all the variables are proportions (they are not entirely independent since they all sum 1) will have implications since that largely **violates the assumption of absence in collinearity**. This, in turn, will generate problems on the model through estimations of artefactually large parameters (regression coeficients) during the training phase.\n",
    "\n",
    "To assess the strength for collinearity between independent variables, we can compute the **Variation Inflation Factor (VIF)**:\n",
    "\n",
    "$$ \\text{VIF} = \\frac{1}{1 - R^2} $$\n",
    "\n",
    "where $R^2$ is the unadjusted correlation coefficient of the one variable against all the others. This statistic ranges from 1 to +Inf, being 1 no correlation at all and infinite a total correlation. The **Tolerance Index**, which is the denominator, is also used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68875a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute VIF using sklearn linear model of one variable against all others\n",
    "def sklearn_vif(exogs, data):\n",
    "\n",
    "    vif_dict, tolerance_dict = {}, {}\n",
    "    for exog in exogs:\n",
    "        not_exog = [i for i in exogs if i != exog]\n",
    "        X, y = data[not_exog], data[exog]\n",
    "\n",
    "        r_squared = LinearRegression().fit(X, y).score(X, y)\n",
    "\n",
    "        # Handle perfect correlation if that is the case (divide by zero)\n",
    "        if r_squared == 1.0:\n",
    "            vif = float('inf')\n",
    "        else:\n",
    "            vif = 1/(1 - r_squared)\n",
    "            \n",
    "        vif_dict[exog] = vif\n",
    "        tolerance_dict[exog] = 1 - r_squared\n",
    "\n",
    "    return pd.DataFrame({'VIF': vif_dict, 'Tolerance': tolerance_dict})\n",
    "\n",
    "# Iterate across the independent variables to compute VIF\n",
    "sklearn_vif(exogs=list(signatures_df.columns[1:-3]), data=signatures_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670b18a2",
   "metadata": {},
   "source": [
    "Obviously, since all the variables sum up to 1, the correlation of one against the other is absolute and the VIF is infinite. Given that this collinearity is structural, that means, it comes mostly due to the own definition of the variables into proportions.\n",
    "\n",
    "The standard way to handle compositional data in bioinformatics (like mutational signatures or microbiome data) is the **Centered Log-Ratio (CLR)** transformation, which breaks the \"sum to 1\" constraint while keeping the data symmetric and normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e59f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gmean\n",
    "\n",
    "def clr_transformation(df):\n",
    "    \"\"\"\n",
    "    Applies Centered Log-Ratio transformation to compositional data.\n",
    "    Adds a small pseudo-count to avoid log(0).\n",
    "    \"\"\"\n",
    "\n",
    "    pseudo_count = 1e-6\n",
    "    df_adj = df + pseudo_count\n",
    "    \n",
    "    # Divide each value by the row's geometric mean and take log\n",
    "    geometric_means = gmean(df_adj, axis=1)\n",
    "    clr_data = np.log(df_adj.div(geometric_means, axis=0))\n",
    "    \n",
    "    return clr_data\n",
    "\n",
    "signature_cols = signatures_df.columns[1:-3]\n",
    "X_compositional = signatures_df[signature_cols]\n",
    "\n",
    "X_clr = clr_transformation(X_compositional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9c0e38",
   "metadata": {},
   "source": [
    "Afterwards, and considering that the CLR transformation fixes the mathematical artifact (the \"sum to 1\" problem) but it cannot fix the biological reality, that mutational signatures are often highly correlated because they are biologically linked, we will apply a model that handles collinearity automatically: Lasso Regression (an embedding method of **feature selection**).\n",
    "\n",
    "Lasso has an hyperparameter \"tuning knob\" called **Alpha (α)** (sometimes called Lambda) which controls the risk of overfitting (with low alpha, behaviour like a regular regression) or underfitting (high alpha, strict model). Since it is different to know in advance the alpha that offers the better trade-off, we will use Lasso with **Cross-Validation (CV)**: divides train data into 5 folds (4 for train the CV, 1 to test) and choose alpha values with lowest error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da99afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def analyze_lasso_regression(X, y, alphas=None):\n",
    "    \"\"\"\n",
    "    Fits a LassoCV model.\n",
    "    \n",
    "    Parameters:\n",
    "    - alphas: List of floats, Integer, or None.\n",
    "                If None (default), it sets alphas=100 to auto-tune (silencing the warning).\n",
    "                If an Integer (e.g., 100), it generates that many alphas automatically.\n",
    "                If a List, it uses those specific values.\n",
    "    \"\"\"\n",
    "\n",
    "    if alphas is None:\n",
    "        alphas = 100\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    lasso_pipeline = make_pipeline(\n",
    "        StandardScaler(), \n",
    "        LassoCV(cv=5, alphas=alphas, random_state=42, max_iter=50000)\n",
    "    )\n",
    "    lasso_pipeline.fit(X_train, y_train)\n",
    "    y_pred = lasso_pipeline.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Lasso R^2 Score: {r2:.4f}\")\n",
    "    \n",
    "    lasso_model = lasso_pipeline.named_steps['lassocv']\n",
    "    coef_df = pd.DataFrame({'Feature': X.columns, 'Coefficient': lasso_model.coef_})\n",
    "    nonzero = coef_df[coef_df['Coefficient'] != 0].copy()\n",
    "    \n",
    "    nonzero['abs_coefficient'] = nonzero['Coefficient'].abs()\n",
    "    sorted_features = nonzero.sort_values(by='abs_coefficient', ascending=False).drop(columns='abs_coefficient')\n",
    "    \n",
    "    return sorted_features\n",
    "\n",
    "\n",
    "analyze_lasso_regression(X_clr, signatures_df['transformed_TMB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0ada5",
   "metadata": {},
   "source": [
    "With this, the model is able to explain around 68% of the total variance. However, still we use a large number of features. We can further restrict the amount of features to the top best performers with a **feature selection** process known as **Recursive Feature Elimination (RFE)**, which consists on developing/training the model while removing recursively the less relevant features until defining a model with a desired number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0fed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR # Support vector regression, like linear regression but optimizes on some tolerance margin\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "def multiple_polynomial_regression_RFE(X, y, degree, k_best):\n",
    "    \"\"\"\n",
    "    Performs multiple polynomial regression with RFE feature selection.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    estimator = SVR(kernel=\"linear\")\n",
    "    pm = Pipeline([\n",
    "        ('scaler', StandardScaler()), # Scale data (Crucial for SVR)\n",
    "        ('selector', RFE(estimator, n_features_to_select=k_best)), # Select Features (RFE using Linear SVR)\n",
    "        ('poly', PolynomialFeatures(degree=degree, include_bias=False)), # Generate Polynomials (only for the selected features)\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    pm.fit(X_train, y_train)\n",
    "    y_pred = pm.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Residual Points\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=y_pred,\n",
    "        y=residuals,\n",
    "        mode='markers',\n",
    "        name='Residuals',\n",
    "        marker=dict(color='crimson', opacity=0.7)\n",
    "    ))\n",
    "\n",
    "    # Zero Line (Reference)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[y_pred.min(), y_pred.max()],\n",
    "        y=[0, 0],\n",
    "        mode='lines',\n",
    "        name='Zero Line',\n",
    "        line=dict(color='black', dash='dash')\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Residuals vs Fitted Values (RFE Selected)',\n",
    "        xaxis_title='Fitted Values (Predictions)',\n",
    "        yaxis_title='Residuals',\n",
    "        template='plotly_white',\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f'--- Model Results (Top {k_best} Features) ---')\n",
    "    print(f'R^2 Score: {r2:.4f}')\n",
    "\n",
    "    selector = pm.named_steps['selector']\n",
    "    selected_features = X.columns[selector.support_]\n",
    "    print(f'Selected Features: {list(selected_features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bbe74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with the top 2 best features, it might take some time.\n",
    "degree = 2\n",
    "k_best = 2\n",
    "multiple_polynomial_regression_RFE(X, y, degree, k_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4e5d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with the top 5 best features, it might take some time.\n",
    "degree = 2\n",
    "k_best = 5\n",
    "multiple_polynomial_regression_RFE(X, y, degree, k_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ece35",
   "metadata": {},
   "source": [
    "With this we can select which of the highly correlated independent variables can explain a large part of the variance in the dependent variable without using very complex models, prone to overfit. Still, it looks like there is not a clear homoskedasticity.\n",
    "\n",
    "However, there are much more other factors that have an influence on the amount of mutations of a tumor (the age of the patient is one, as shown on the linear regression example).\n",
    "\n",
    "At any rate, cancer is a group of complex and highly heterogeneous diseases. Models will have problems to be generalistic but, at the same time, precise on their predictions. In this situations, it might be useful to reduce the scope and build models restricting to specific types of tumors. This might remove a lot of the variance due to the own idiosincracy of the tumor type and the dynamics of the tissue where it arises. For instance, we can restrict the model to melanomas only, which is a type with a large variance in TMB and enough samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeed8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "melanoma_specimens = TMB_df[TMB_df['hist_type']=='Skin_Melanoma']['specimenID']\n",
    "unprocessed_melanoma_df = raw_signatures_df[raw_signatures_df['specimenID'].isin(melanoma_specimens)].copy()\n",
    "\n",
    "# Exclude non present signatures\n",
    "numeric_cols = unprocessed_melanoma_df.columns[1:]\n",
    "sumover = unprocessed_melanoma_df[numeric_cols].sum()\n",
    "valid_features = sumover[sumover != 0.0].index\n",
    "\n",
    "# CLR transform\n",
    "melanoma_df = clr_transformation(unprocessed_melanoma_df[valid_features])\n",
    "melanoma_df.insert(0, 'specimenID', list(unprocessed_melanoma_df['specimenID']))\n",
    "\n",
    "# Merge with the signatures dataframe to get the variables\n",
    "melanoma_df = pd.merge(melanoma_df, TMB_df, on='specimenID', how='inner')\n",
    "X_melanoma = melanoma_df[valid_features]\n",
    "y_melanoma = melanoma_df['transformed_TMB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ef755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try with Lasso specifying a very restrictive alpha\n",
    "analyze_lasso_regression(X_melanoma, y_melanoma, alphas=[0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cb3e37",
   "metadata": {},
   "source": [
    "With just 4 mutational signatures, including the UV-ligth associated mutations and the background mutation rate SBS1, almost 50% of all tha variability in the order off magnitude of mutations in melanomas can be explained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b869d8e",
   "metadata": {},
   "source": [
    "# Basic supervised machine learning methods: classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304130d6",
   "metadata": {},
   "source": [
    "Classification methods are used when the target (dependent) variable is categorical, and the goal is to assign each data point to a specific category. Here the explanatory variables (independent) can be either categorical or continous. Hence, the amount of classification algorithms is very long.\n",
    "\n",
    "Classification algorithms, unlike regressions, use evaluation metrics that are based on the amount of correct predictions of the model based on the known labels (true values). This is usually expressed as a **confusion matrix**, a table with four entries for a binary classification model: the **True Positives (TP)** and **True Negatives (TN)**, where the model correctly predicts one class or the other, and the **False Positives (FP)** and **False Negatives (FN)**, where the model incorrectly predicts the positive and negative class, respectively.\n",
    "\n",
    "From these four elements of the table, several metrics could be extracted:\n",
    "\n",
    "- **Precision** is the ratio of correctly predicted positive observations to the total predicted positives. It is a measure of the accuracy of positive predictions.\n",
    "\n",
    "- **Specificity** is the ratio of correctly predicted negative observations to the total actual negatives. It is a measure of the accuracy of negative predictions.\n",
    "\n",
    "- **Sensitivity**, also known as **recall** or **true positive rate**, is the ratio of correctly predicted positive observations to the total actual positives. It measures the model's ability to correctly identify positive instances.\n",
    "\n",
    "- **Accuracy** is the ratio of correctly predicted observations to the total observations. It provides an overall measure of the model's correctness.\n",
    "\n",
    "- The **F1 score** is the harmonic mean of precision and sensitivity. It provides a balance between precision and sensitivity.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Classification metrics](https://miro.medium.com/v2/resize:fit:640/format:webp/1*NhPwqJdAyHWllpeHAqrL_g.png)](https://medium.com/all-about-ml/evaluation-metrics-in-classification-algorithms-79c036a131cb)\n",
    "\n",
    "<!-- Add an empty line here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57feacc8",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "The perceptron is one of the most basic binary classifiers, and the fundamental building block of artificial neural networks. The idea behind the perceptron is the biological neuron, where based on the different inputs collected from the dendrites it triggers/not triggers a signal on the axon to the next neuron:\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Neuron](images/Neurons.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Analogously, the perceptron takes as input one or a vector of continous independent variables and their weights, which is summed up and passes the result through a **binary step or unit step function** to produce a binary output $\\hat{y}$:\n",
    "\n",
    "$$\\hat{y} = \\begin{cases} 1 & \\text{if } w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n > 0 \\\\ 0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "where:\n",
    "- $\\hat{y}$ is the predicted output.\n",
    "- $w_0, w_1, w_2, ..., w_n$ are the weights associated with the inputs.\n",
    "- $x_1, x_2, ..., x_n$ are the input features.\n",
    "\n",
    "As we will review in the next session while discussing advanced supervised methods there are multiple **activation functions** besides the binary step function that can be used. At any rate, the Perceptrons are trained (learn from the data) by adjusting its weights based on the error in its predictions: \n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Perceptron scheme](images/Perceptron.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "To understand the perceptron, we need to understand a concept that is key also for training neural networks: the **learning rule**: the specific mathematical algorithm used to update the model's parameters, the weights, based on the input data and the error produced by the current prediction.\n",
    "\n",
    "For the perceptron, the updated weight $w^\\prime_i$ is computed iteratively during the training process until convergence is achieved as follows:\n",
    "\n",
    "$$w^\\prime_i \\leftarrow w_i + \\alpha \\cdot (y - \\hat{y}) \\cdot x_i$$\n",
    "\n",
    "where:\n",
    "- $w_i$ is the weight associated with the $i$-th input feature.\n",
    "- $\\alpha$ is the learning rate, a small positive constant of range [0, 1] that determines the step size in weight updates.\n",
    "- $y$ is the true label (the actual class of the instance).\n",
    "- $\\hat{y}$ is the predicted output of the perceptron.\n",
    "- $x_i$ is the $i$-th input feature.\n",
    "\n",
    "Hence, the **learning rule** of the perceptron essentially adjusts the weights to reduce the difference between the predicted output ($\\hat{y}$) and the true label ($y$), where the term $(y - \\hat{y})$ represents the error of prediction at iteration $i$-th. With this, the weights are updated in the direction that minimizes this error at a **learning rate** $\\alpha$. \n",
    "\n",
    "$\\alpha$ is a constant is defined as an hyperparameter in order to maximize the search for the optimal solution: with a too small value, the algorithm will take too much time and computational resources to find the optimal solution (and also it might get stucked in local optimums) whereas a too large learning rate the algorithm might not use properly the gradient to find the optimas.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Different learning rates](images/Learning_rate.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "**Advantages:**\n",
    "- Perceptrons are conceptually **simple** and computationally **efficient**.\n",
    "- Perceptrons will **converge** to a solution if the data is linearly separable.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Perceptrons are **limited to linear separability** (can only learn linear decision boundaries).\n",
    "- Sensitivity to Outliers, they significantly impact perceptron performance.\n",
    "- Perceptrons only produce binary outputs **without associated probabilities**. \n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948b536e",
   "metadata": {},
   "source": [
    "As a practical example, we will use the output of PCA applied to the transcriptomic profile across blood tumors. We will try to train models that are able to differentiate different histological subtypes of primary blood cancers based on the two first principal components (linear combinations of genes expression that explain most of the variability in expression).\n",
    "\n",
    "In order to define histological subtypes, we will have to download that information from ICGC, contained in the **pcawg_specimen_histology_August2016_v9.xlsx** excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import path\n",
    "\n",
    "expression_df = pd.read_csv(path.join('data', 'gene_expression.tsv.gz'),\n",
    "                                                        sep=\"\\t\", header='infer', index_col=0, compression='gzip')\n",
    "\n",
    "sample_df = pd.read_csv(path.join('data', 'sample_df.tsv'), sep=\"\\t\", header='infer')\n",
    "\n",
    "# ICGC database with histological subtypes\n",
    "histology_df = pd.read_excel('data/pcawg_specimen_histology_August2016_v9.xlsx')\n",
    "histology_df.columns = ['icgc_specimen_id'] + list(histology_df.columns[1:])\n",
    "histology_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3147c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tumortype_dict = dict(zip(histology_df.icgc_specimen_id, histology_df.histology_tier3))\n",
    "\n",
    "# Get the specimensID of blood cancers\n",
    "specimens_blood = sample_df[sample_df['primary_location']=='Blood']['icgc_specimen_id']\n",
    "\n",
    "# Intersect with the available columns to find the blood cancer specimens with available expression\n",
    "subset_specimens = list(set(expression_df.columns).intersection(set(specimens_blood)))\n",
    "\n",
    "# Filter the data to do the PCA only with blood cancer specimens\n",
    "blood_expression_df = expression_df[subset_specimens].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4525d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# We preprocess the data with standarization\n",
    "scaler = StandardScaler()\n",
    "blood_data = scaler.fit_transform(blood_expression_df.T)\n",
    "\n",
    "# We perform the PCA\n",
    "blood_pca2D = PCA(n_components=2)\n",
    "blood_proj_data = blood_pca2D.fit_transform(blood_data)\n",
    "\n",
    "# We can check the amount of variance explained by the two Principcal components\n",
    "print(blood_pca2D.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a35fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "def plot_PCA_withlabels(proj_data, labels):\n",
    "    \n",
    "    fig = px.scatter(\n",
    "        x=proj_data[:, 0], \n",
    "        y=proj_data[:, 1], \n",
    "        color=labels, \n",
    "        title='Transcriptome',\n",
    "        labels={'x': 'Principal Component 1', 'y': 'Principal Component 2', 'color': 'Tumor Type'},\n",
    "        width=1000, height=800\n",
    "    )\n",
    "\n",
    "    fig.update_traces(marker=dict(size=5)) \n",
    "    fig.update_layout(template='plotly_white')\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "    unique_labels = np.unique(labels)\n",
    "    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    label_integers = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "    return label_integers\n",
    "\n",
    "\n",
    "labels_blood = blood_expression_df.columns.map(tumortype_dict)\n",
    "blood_label_integers = plot_PCA_withlabels(blood_proj_data, labels_blood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33855e07",
   "metadata": {},
   "source": [
    "There is a clear separation between chronic lymphocitic leukemias and mature B-cell lymphomas on the 2D scale.\n",
    "\n",
    "Now, we can train a perceptron using scikit-learn that can separate both groups.\n",
    "\n",
    "For this kind of classifier models it is good practice to **standarize** the data as we saw for the dimensionality reduction algorithms, however, note that the output of the PCA is already standarized so both **training** and **test** dataset, which derive from the same standarize output, will have the same scale.\n",
    "\n",
    "Finally, we will state a 10% learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe22fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the dependent and independent variables\n",
    "## The labels are the two types of blood cancers we want to separate, encoded as 0s and 1s.\n",
    "y_blood = pd.DataFrame(blood_label_integers)\n",
    "## The two features are the two first PC, already on the 2D numpy array format (columns are the PC)\n",
    "X_blood = pd.DataFrame(blood_proj_data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_blood, X_test_blood, y_train_blood, y_test_blood = train_test_split(X_blood, y_blood, test_size=0.3, random_state=42)\n",
    "\n",
    "#  Set the random state for the algorithm to start and the learning rate 10%.\n",
    "ppn = Perceptron(eta0=0.1, random_state=42)\n",
    "ppn.fit(X_train_blood, y_train_blood.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc4c05f",
   "metadata": {},
   "source": [
    "The model is already trained with the training dataset, but before plotting the results on the test dataset we can define a function to conviniently visualize the decision boundary regions that arise from training the perceptron or other classifier models that we will see on this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a55985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_decision_regions_plotly(X, y, classifier, X_test=None, y_test=None, target_names=None, resolution=200):\n",
    "    \n",
    "    # Manage input data\n",
    "    X_plot = np.array(X)\n",
    "    y_plot = np.array(y).flatten()\n",
    "    \n",
    "    if X_test is not None and y_test is not None:\n",
    "        X_test_arr = np.array(X_test)\n",
    "        y_test_arr = np.array(y_test).flatten()\n",
    "        \n",
    "        X_plot = np.vstack((X_plot, X_test_arr))\n",
    "        y_plot = np.concatenate((y_plot, y_test_arr))\n",
    "    else:\n",
    "        X_test_arr = None\n",
    "\n",
    "    # Create Grid\n",
    "    x_min, x_max = X_plot[:, 0].min() - 1, X_plot[:, 0].max() + 1\n",
    "    y_min, y_max = X_plot[:, 1].min() - 1, X_plot[:, 1].max() + 1\n",
    "    \n",
    "    xx1, xx2 = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, int(resolution)),\n",
    "        np.linspace(y_min, y_max, int(resolution))\n",
    "    )\n",
    "\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    # Decision Boundary\n",
    "    fig.add_trace(go.Contour(\n",
    "        x=np.linspace(x_min, x_max, int(resolution)),\n",
    "        y=np.linspace(y_min, y_max, int(resolution)),\n",
    "        z=Z,\n",
    "        showscale=False,\n",
    "        opacity=0.4,\n",
    "        colorscale='RdBu',\n",
    "        line_width=0,\n",
    "        hoverinfo='skip'\n",
    "    ))\n",
    "\n",
    "    # Points\n",
    "    unique_classes = np.unique(y_plot)\n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "    markers = ['square', 'x', 'circle', 'diamond']\n",
    "    \n",
    "    for idx, cl in enumerate(unique_classes):\n",
    "        mask = (y_plot == cl)\n",
    "        label_name = target_names[cl] if target_names and cl in target_names else f\"Class {cl}\"\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=X_plot[mask, 0],\n",
    "            y=X_plot[mask, 1],\n",
    "            mode='markers',\n",
    "            name=label_name,\n",
    "            marker=dict(\n",
    "                symbol=markers[idx % 4],\n",
    "                color=colors[idx % 4],\n",
    "                size=8,\n",
    "                line=dict(width=1, color='black')\n",
    "            ),\n",
    "            hovertemplate=f\"<b>{label_name}</b><br>Component1: %{{x:.2f}}<br>Component2: %{{y:.2f}}<extra></extra>\"\n",
    "        ))\n",
    "\n",
    "    # Highlight Test Points\n",
    "    if X_test_arr is not None:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=X_test_arr[:, 0],\n",
    "            y=X_test_arr[:, 1],\n",
    "            mode='markers',\n",
    "            name='Test Set (Highlight)',\n",
    "            marker=dict(\n",
    "                symbol='circle-open',\n",
    "                size=12,\n",
    "                color='black',\n",
    "                line=dict(width=2)\n",
    "            ),\n",
    "            hoverinfo='skip'\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Decision Regions',\n",
    "        xaxis_title='Component 1',\n",
    "        yaxis_title='Component 2',\n",
    "        width=800,\n",
    "        height=600,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179c692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_blood_arr = np.array(labels_blood)\n",
    "unique_ints = np.unique(blood_label_integers)\n",
    "target_map_blood = {\n",
    "    i: labels_blood_arr[blood_label_integers == i][0] \n",
    "    for i in unique_ints\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb572673",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions_plotly(\n",
    "    X=X_blood,\n",
    "    y=y_blood,\n",
    "    classifier=ppn,\n",
    "    X_test=X_test_blood,\n",
    "    y_test=y_test_blood,\n",
    "    target_names=target_map_blood\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e02d390",
   "metadata": {},
   "source": [
    "Since the two classes can be separated linearly, the perceptron works well on the classification task. This is reflected on the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce4b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def compute_evaluation_metrics(model, X_test, y_test, labels):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "    if len(np.unique(y_test)) == 2:\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        print(f'Precision: {precision:.2f}')\n",
    "        print(f'Recall: {recall:.2f}')\n",
    "        print(f'F1 Score: {f1:.2f}')\n",
    "\n",
    "\n",
    "    else:\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        print(f'Precision: {precision:.2f}')\n",
    "        print(f'Recall: {recall:.2f}')\n",
    "        print(f'F1 Score: {f1:.2f}')\n",
    "\n",
    "    print('Confusion Matrix:')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plot_labels = labels\n",
    "    if len(labels) != cm.shape[0]:\n",
    "        plot_labels = np.unique(labels)\n",
    "    \n",
    "    fig = px.imshow(\n",
    "        cm, \n",
    "        text_auto=True, \n",
    "        labels=dict(x=\"Predicted Label\", y=\"True Label\", color=\"Count\"),\n",
    "        x=plot_labels,\n",
    "        y=plot_labels,\n",
    "        color_continuous_scale='Blues'\n",
    "    )\n",
    "    fig.update_layout(title_text='Confusion Matrix', title_x=0.5)\n",
    "    fig.show()\n",
    "    \n",
    "    \n",
    "compute_evaluation_metrics(ppn, X_test_blood, y_test_blood, labels_blood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9badcf0d",
   "metadata": {},
   "source": [
    "But, it will work that well on a dataset with no clear lineal decision boundary? Let's try for instance with Kidney cancer, where there are three subtypes of adenocarcinomas depending on the affected cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a17cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get kidney data\n",
    "subtype_tumortype_dict = dict(zip(histology_df.icgc_specimen_id, histology_df.histology_tier4))\n",
    "specimens_kidney = sample_df[sample_df['primary_location']=='Kidney']['icgc_specimen_id'].copy()\n",
    "subset_specimens = list(set(expression_df.columns).intersection(set(specimens_kidney)))\n",
    "kidney_expression_df = expression_df[subset_specimens].copy()\n",
    "\n",
    "# Filter for samples with available histology information\n",
    "raw_labels = kidney_expression_df.columns.map(subtype_tumortype_dict)\n",
    "valid_mask = ~raw_labels.isna()\n",
    "kidney_expression_df = kidney_expression_df.loc[:, valid_mask]\n",
    "labels_kidney = raw_labels[valid_mask]\n",
    "\n",
    "# We perform the PCA\n",
    "scaler2 = StandardScaler()\n",
    "kidney_data = scaler2.fit_transform(kidney_expression_df.T)\n",
    "kidney_pca2D = PCA(n_components=2)\n",
    "kidney_proj_data = kidney_pca2D.fit_transform(kidney_data)\n",
    "print(kidney_pca2D.explained_variance_ratio_)\n",
    "\n",
    "kidney_label_integers = plot_PCA_withlabels(kidney_proj_data, labels_kidney)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666558d9",
   "metadata": {},
   "source": [
    "Again, we can train a model that separates the three classes.\n",
    "\n",
    "Note that sometimes not all the classes are equally represented. In these cases, the train test split is useful to do it stratifying by the proportions found on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e366b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    kidney_proj_data,\n",
    "    kidney_label_integers,\n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=kidney_label_integers # Stratify to maintain class proportions\n",
    ")\n",
    "\n",
    "ppn = Perceptron(eta0=0.1, random_state=42)\n",
    "ppn.fit(X_train, y_train)\n",
    "\n",
    "labels_kidney_arr = np.array(labels_kidney)\n",
    "unique_ints = np.unique(kidney_label_integers)\n",
    "target_map = {\n",
    "    i: labels_kidney_arr[kidney_label_integers == i][0] \n",
    "    for i in unique_ints\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328296ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions_plotly(\n",
    "    X=pd.DataFrame(X_train, columns=['PC1', 'PC2']),\n",
    "    y=pd.Series(y_train),\n",
    "    classifier=ppn,\n",
    "    X_test=pd.DataFrame(X_test, columns=['PC1', 'PC2']),\n",
    "    y_test=pd.Series(y_test),\n",
    "    target_names=target_map\n",
    ")\n",
    "\n",
    "class_names = [target_map[i] for i in sorted(target_map.keys())]\n",
    "compute_evaluation_metrics(ppn, X_test, y_test, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01754561",
   "metadata": {},
   "source": [
    "Since there is not a clear linear separation on this PCA 2D data, the perceptron is unable convergence on an optimal solution.\n",
    "\n",
    "In this scenario there are two options:\n",
    "\n",
    "- Apply a dimensionality reduction algorithm that is not linear such as PCA and use more dimensions so the different histological types can be clearly separated.\n",
    "- Use a classifier algorithm that works better with more noisy data.\n",
    "\n",
    "We will first apply the first approach, which will make our data less noisy and easier to model a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceda60bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    metric='cosine', \n",
    "    n_components=2, \n",
    "    n_neighbors=15, \n",
    "    min_dist=0.1, \n",
    "    random_state=52\n",
    ")\n",
    "kidney_umap_2d = reducer.fit_transform(kidney_data)\n",
    "\n",
    "X_kidney = kidney_umap_2d\n",
    "y_kidney = kidney_label_integers\n",
    "\n",
    "X_train_kidney, X_test_kidney, y_train_kidney, y_test_kidney = train_test_split(\n",
    "    X_kidney,\n",
    "    y_kidney,\n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y_kidney\n",
    ")\n",
    "\n",
    "ppn = Perceptron(eta0=0.1, random_state=42)\n",
    "ppn.fit(X_train_kidney, y_train_kidney)\n",
    "\n",
    "labels_kidney_arr = np.array(labels_kidney)\n",
    "unique_ints = np.unique(y_kidney)\n",
    "target_map_kidney = {\n",
    "    i: labels_kidney_arr[y_kidney == i][0] \n",
    "    for i in unique_ints\n",
    "}\n",
    "\n",
    "plot_decision_regions_plotly(\n",
    "    X=X_train_kidney, \n",
    "    y=y_train_kidney, \n",
    "    classifier=ppn, \n",
    "    X_test=X_test_kidney,\n",
    "    y_test=y_test_kidney,\n",
    "    target_names=target_map_kidney,\n",
    "    resolution=200\n",
    ")\n",
    "\n",
    "class_names_kidney = [target_map_kidney[i] for i in sorted(target_map_kidney.keys())]\n",
    "compute_evaluation_metrics(ppn, X_test_kidney, y_test_kidney, class_names_kidney)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4b557c",
   "metadata": {},
   "source": [
    "Still, we can do better, so let's introduce more supervised classifier algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998102ce",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Logistic Regression is a popular statistical method used for binary classification tasks, although, similarly to the perceptron, it is possible to implement it on multi-class taks by dividing the problem in multiple binary classifiers of one vs the rest or **OvR** (in this case we have a **multinomial logistic classification**).\n",
    "\n",
    "The logistic regression model works by transforming a linear combination of input features using a **logit function** (do not confuse with the **logistic or sigmoid function** which is the inverse) as follows:\n",
    "\n",
    "$$P(Y) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "where:\n",
    "- $P(Y)$ is the probability of the target (dependent) variable.\n",
    "- $e$ is the base of the natural logarithm.\n",
    "- $z$ is the linear combination of input features and their corresponding weights: $z = β₀ + β₁x₁ + β₂x₂ + ... + β_nx_n$.\n",
    "  \n",
    "Hence, the logit function maps any real-valued number to the range [0, 1], which is crucial for interpreting the output as a probability, and thus, assign each element to a given class based on the ratio of probabilities. Then, once the model is trained, we can use the inverse **logistic or sigmoid function** to predict the probability that a certain sample belongs to a particular class given the input features.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Logit function](https://i.sstatic.net/WY61Z.png)](https://en.wikipedia.org/wiki/Logit)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "On this setting, the linear combination of input features is related with the logarithm of the **odds-ratio** (also known as **log-odds**) since from the logistic regression formula we can derive that:\n",
    "\n",
    "$$logit(P(Y)) = ln(\\frac{P(Y)}{1 - P(Y)}) = β₀ + β₁x₁ + β₂x₂ + ... + β_nx_n$$\n",
    "\n",
    "The **odds-ratio**, expressed as $\\frac{P(Y)}{1 - P(Y)}$, is the relationship of the probability of one event with respect to the opposite one (on a binary situation) and can be used as the association strength between two events. From the relative probabilities of belonging to once class vs others, a **unit step function or quantizer** chooses the class with the highest probability to provide that as output.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Logistic classificator scheme](images/Logistic.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "\n",
    "The **learning rule** here is the maximization of a **likelihood function** (from a simplified perspective, the algorithm finds the proper weights by maximizing a function that provides the **probability of observing the training data given the parameters of the model**, if you need more information look at the bibliography).\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "**Advantages:**\n",
    "- Logistic regression provides **interpretable** results since the coefficients can be directly interpreted in terms of changes in log odds: it provides with probabilities during the classification.\n",
    "- Training logistic regression models is computationally **efficient** and scales well to large datasets.\n",
    "\n",
    "**Disadvantages:**\n",
    "- A critical assumption is the **absence of extreme outliers** in the dataset, which distort the training (could be verified by calculating Cook’s distance (Di) to identify influential data points that may negatively affect the regression model).\n",
    "- Logistic regression **assumes a linear relationship between the log-odds and the independent variables**, which translated into a linear decision boundary. This might not be suitable for datasets with complex, non-linear relationships between features (better use support vector machines or decision tress and derivates).\n",
    "- Assumes little to no multicollinearity between explanatory variables.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af88bcf9",
   "metadata": {},
   "source": [
    "Note that the main improvement over the perceptron is that it provides with a probability, but still uses a linear decision boundary. Let's see how it works for both blood  (PCA 2D) and kidney (UMAP 2D) cancer examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c5139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train_blood, y_train_blood)\n",
    "\n",
    "plot_decision_regions_plotly(\n",
    "    X=X_train_blood, \n",
    "    y=y_train_blood, \n",
    "    classifier=lr, \n",
    "    X_test=X_test_blood, \n",
    "    y_test=y_test_blood, \n",
    "    target_names=target_map_blood,\n",
    "    resolution=200\n",
    ")\n",
    "\n",
    "compute_evaluation_metrics(lr, X_test_blood, y_test_blood, labels_blood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a245fdf0",
   "metadata": {},
   "source": [
    "A perfect classification, similar to the one obatined with the perceptron. But how about the kidney UMAP 2D data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf90a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr.fit(X_train_kidney, y_train_kidney)\n",
    "\n",
    "plot_decision_regions_plotly(\n",
    "    X=X_train_kidney,\n",
    "    y=y_train_kidney, \n",
    "    classifier=lr, \n",
    "    X_test=X_test_kidney,\n",
    "    y_test=y_test_kidney,\n",
    "    target_names=target_map_kidney,\n",
    "    resolution=200\n",
    ")\n",
    "\n",
    "compute_evaluation_metrics(lr, X_test_kidney, y_test_kidney, labels_kidney)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3801c03",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)\n",
    "\n",
    "Support Vector Machines (SVM) are powerful supervised learning models used for classification and regression tasks. SVM aims to find the optimal hyperplane that separates data points of different classes in feature space. The hyperplane is defined by:\n",
    "\n",
    "$$f(x) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$$\n",
    "\n",
    "where:\n",
    "- $f(x)$ is the decision function.\n",
    "- $\\beta_0, \\beta_1, \\beta_2, ..., \\beta_n$ are the coefficients (weights) to be learned.\n",
    "- $x_1, x_2, ..., x_n$ are the input features.\n",
    "\n",
    "In contrast with the perceptron, where the objective was to minimized misclassification errors, in SVMs the **learning rule** is to **maximize the margin**, defined as the distance between the separating hyperplane (the decision boundary) and the training samples that are closest to this hyperplane, which are the so-called **support vectors** (the data points that lie closest to the decision boundary). This is illustrated in the following figure:\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Margin concept](images/Margin.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "The rationale behind having decision boundaries with large margins is that they tend to have a lower generalization error (the decision boundaries are less influenced by the training dataset) whereas models such as the perceptron with small margins are more prone to fall into **overitting** issues.\n",
    "\n",
    "However, as you might understand, this concept of **maximizing the margins** is therorethically suitable if there is an hyperplane that is able to separate all the training groups. But what about cases where that is impossible? How can we build a good enough model in this situation where margins are not that clear?\n",
    "\n",
    "To assess that the algorithm takes into account a concept known as **soft-margin classification**, where an internal extra parameter called **slack variable**  allows to relax the constrains for nonlinearly separable data to allow convergence of the optimization in the presence of misclassifications under the appropriate cost penalization. This is controlled by an **hyperparameter of the model C** where increasing values of C increases the bias and lowers the variance of the model to adjust for the **bias-variance** tradeoff:\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Regularization parameter C](images/ParamC.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "**Advantages:**\n",
    "- Effective in high-dimensional spaces.\n",
    "- Can handle **linear and non-linear relationships** using different kernel functions.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Non-linear relationships](images/Kernel.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "- **Robust to overfitting**, especially in high-dimensional spaces. Thanks to the hyperparameter C.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Training is more **computationally expensive** can be high for large datasets.\n",
    "- SVMs are **sensitive to noise** present on the data. It is important to play with the hyperparameter C to adjust for that.\n",
    "\n",
    "---\n",
    "\n",
    "Ok, let's try it on our kidney data to exemplify the effect of the **C hyperparameter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579bdcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Model 1: Small C (0.1) -> Soft Margin (More regularized)\n",
    "svm1 = SVC(kernel='linear', C=0.1, random_state=32)\n",
    "svm1.fit(X_train_kidney, y_train_kidney)\n",
    "\n",
    "plot_decision_regions_plotly(\n",
    "    X=X_train_kidney,\n",
    "    y=y_train_kidney, \n",
    "    classifier=svm1, \n",
    "    X_test=X_test_kidney, \n",
    "    y_test=y_test_kidney, \n",
    "    target_names=target_map_kidney,\n",
    "    resolution=200\n",
    ")\n",
    "compute_evaluation_metrics(svm1, X_test_kidney, y_test_kidney, labels_kidney)\n",
    "\n",
    "# Model 2: Large C (1000) -> Hard Margin (Less regularized)\n",
    "svm2 = SVC(kernel='linear', C=100, random_state=32)\n",
    "svm2.fit(X_train_kidney, y_train_kidney)\n",
    "\n",
    "plot_decision_regions_plotly(\n",
    "    X=X_train_kidney, \n",
    "    y=y_train_kidney, \n",
    "    classifier=svm2, \n",
    "    X_test=X_test_kidney, \n",
    "    y_test=y_test_kidney, \n",
    "    target_names=target_map_kidney,\n",
    "    resolution=200\n",
    ")\n",
    "\n",
    "compute_evaluation_metrics(svm2, X_test_kidney, y_test_kidney, labels_kidney)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ebc29",
   "metadata": {},
   "source": [
    "Note that the SVC classifier with **large C hyperparameter** works in a similar way to the simple perceptron that we trained at the start of this session. However, notice that with **low C hyperparameter**, the decision boundary now tends to maximize the margin between the training groups for each pair of classes, which helps avoiding overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2b27d6",
   "metadata": {},
   "source": [
    "Unlike the **logistic model** which cannot work with non-linear decision boundaries, on **SVM** we can use the kernel trick to solve non-linear classifications. This is the key aspect why SVM enjoy high popularity among machine learning practitioners.\n",
    "\n",
    "To simplify the explanation, interpret the kernel as a similarity function between pairs of samples so we can model an \"extra non-existent dimension\" to allow for non-linear boundaries. One of the most popular **kernel functions** is the **Radial Basis Function (RBF) or Gaussian kernel**, which is the one we are going to implement in our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f87a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_k = SVC(kernel='rbf', C=1, random_state=42)\n",
    "svm_k.fit(X_train_kidney, y_train_kidney)\n",
    "\n",
    "plot_decision_regions_plotly(\n",
    "    X=X_train_kidney, \n",
    "    y=y_train_kidney, \n",
    "    classifier=svm_k, \n",
    "    X_test=X_test_kidney, \n",
    "    y_test=y_test_kidney, \n",
    "    target_names=target_map_kidney,\n",
    "    resolution=200\n",
    ")\n",
    "\n",
    "compute_evaluation_metrics(svm_k, X_test_kidney, y_test_kidney, labels_kidney)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f0161",
   "metadata": {},
   "source": [
    "For instance with a **C hyperparameter** of 1, a soft margin, the model is able to non linearly separate the train set and the model does not seem overfitted when evaluated with the test. However, if we largely increase the hyperparameter we can find that using the kernel for non-linear separation could easily lead to overfitting since the algorithm fits custom regions around the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290658b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_k = SVC(kernel='rbf', C=10000, random_state=42)\n",
    "svm_k.fit(X_train_kidney, y_train_kidney)\n",
    "\n",
    "plot_decision_regions_plotly(\n",
    "    X=X_train_kidney, \n",
    "    y=y_train_kidney, \n",
    "    classifier=svm_k, \n",
    "    X_test=X_test_kidney, \n",
    "    y_test=y_test_kidney, \n",
    "    target_names=target_map_kidney,\n",
    "    resolution=200\n",
    ")\n",
    "\n",
    "compute_evaluation_metrics(svm_k, X_test_kidney, y_test_kidney, labels_kidney)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8cc08c",
   "metadata": {},
   "source": [
    "### Ensemble Method: Random Forest\n",
    "\n",
    "So far, we have looked at Perceptron, Logistic Regression, and SVM. These are all fundamentally linear classifiers (unless we use kernels). They try to draw a straight line (or plane) to separate your cancer types. When the data is messy or curved, they struggle.\n",
    "\n",
    "Random Forest is completely different. It is an **Ensemble method**, meaning it doesn't rely on a single model but combines the predictions of many. Hence, to understand the Random Forest, first we need to understand the **Decision Trees**: a classic tree-like model used for classification based on multiple \"decisions\" based on several independent variables.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Decision trees](https://miro.medium.com/v2/resize:fit:1280/0*4QE-0kavxXfzF_bR.png)](https://en.wikipedia.org/wiki/Decision_tree)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Since each of these decision trees is considered a model per se, the **Random forest** is the model arising from the combination of multiple decision trees. In other words, an **ensemble method** is a machine learning model composed of multiple small models with the idea of outperforming the accuracy obtained from individual models alone.\n",
    "\n",
    "Therefore, the algorithm behind **Random forest** models involves the recursive splitting of the dataset based on the features that best separate the data into distinct classes or groups. The goal is to create decision rules that efficiently partition the data by randomly sampling of the features that best separate the data into distinct classes or groups. Therefore, the steps for a random forest classifier are:\n",
    "\n",
    "1. **Build Decision Trees:** Create multiple decision trees using bootstrapped samples and random subsets of features. Hence, each building block of the random forest is trained on a random subset of the training data and features, allowing for diversity.\n",
    "\n",
    "2. **Aggregate Predictions:** Combine the predictions of individual trees through voting or averaging.\n",
    "\n",
    "3. **Model Evaluation:** Assess the model's performance using metrics like accuracy or F1 score.\n",
    "\n",
    "Note that random forest **can handle both classification and regression taks**, making it very powerful for complex data with continous and discrete variables.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Random forest classifier](https://www.freecodecamp.org/news/content/images/2020/08/how-random-forest-classifier-work.PNG)](https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n",
    "\n",
    "The **learning rule** of this classifier is to maximize the \"discriminative power\" for classification taks and the error distance for regression tasks. This is commonly evaluated with the **Gini impurity** metric for classification tasks or the **mean squared error** for regression tasks, although there are many other metrics such as the **entropy level** or the **misclassification error**. Let's take a closer look at what the **Gini impurity** (and other metrics) reflect.\n",
    "\n",
    "The **Gini impurity** or **Gini index** is a measure of the uncertainty at each split points (the nodes) of a decision tree. That is, **measures the likelihood of misclassifying a randomly chosen element from the set**. Mathematically, it is calculated for the set $D$ to be splited as follows:\n",
    "\n",
    "$$Gini(D) = 1 - \\sum_{i=1}^{K} (p_i)^2$$\n",
    "\n",
    "where:\n",
    "- $K$ is the number of classes in the dataset.\n",
    "- $p_i$ is the probability of randomly picking an element of class $i$ from the set $D$.\n",
    "\n",
    "This measure ranges between 0 and 1, where lower values indicating a purer or less impure dataset. Notice that Gini impurity will be 0 if the set contains only elements of a single class (and hence, there is no need to discriminate), whereas 1 indicates that all classes are equally represented.\n",
    "\n",
    "In the context of decision trees, the algorithm uses the Gini impurity to find the splits that minimizes the Gini impurity across the resulting child nodes, that means, whose child nodes are pure of one of the classes we want to separate. Hence, the split that leads to the lowest Gini impurity is chosen as the best split through an iterative work until the decision tree is completed.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n",
    "\n",
    "**Advantages:**\n",
    "- Random Forests reduce are **robust to overfitting** by averaging the predictions of multiple trees.\n",
    "- Capable of **capturing complex, non-linear relationships** in the data.\n",
    "- Provides a measure of feature importance such as the **Gini impurity**.\n",
    "- There is no need to pre-process the data in terms of feature scaling such as standarization.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Random Forests **cannot be easily interpreted**, often considered as \"black-box\" models\n",
    "- Can be **computationally expensive** for large datasets and many trees.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc22eb1",
   "metadata": {},
   "source": [
    "For the following examples instead of the our multiomics dataset, we will work with a mock dataset that is already present on the scikit-learn package, which will be better to exemplify the largest power of decision trees: working with high number of features and evaluate the importance of each of them in the classification power. This is the dataset that will be used for the first deadline of delivery exercises. First we will build a simple decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281aa737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "# Continous independent variables\n",
    "X = pd.DataFrame(iris['data'])\n",
    "# Labels or dependent variable (discrete classes)\n",
    "y = pd.DataFrame(iris['target'])\n",
    "\n",
    "print(iris.keys())\n",
    "print('')\n",
    "print('feature names:', iris['feature_names'])\n",
    "print('target names:', iris['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79597ba",
   "metadata": {},
   "source": [
    "Until here, we have seen many machine learning algorithms that require to preprocess the data (we have been using **standarization** if our original data had features that were not already scaled), however, **one of the main advantatges of decision trees and random forests is that, as non-parametric methods, we do not need to worry about scaling features**.\n",
    "\n",
    "We are going to train a decision tree of maximum of three nodes (parameter max_depth on 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e14555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# We can skip the standarization step\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_tree(decision_tree, fontsize=12, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fec622",
   "metadata": {},
   "source": [
    "The plot tree function provides with an encoded decision tree, which is translated into an understandable tree plot. As we can see, the first decision, that the petal length (in cm) is less or equal than 2.45 (x[2] means the third variable, remember that python indexes start at 0) is able to separate all the 40 setosa samples from the other 41 versicolor and 39 virginica. Note how the gini impurity index is lower on the next nodes, since they are less discriminative that the nodes above. Sadly, we only three nodes we are not able to fully separate all the remaining versicolor and virginica flowers.\n",
    "\n",
    "We can display the decision boundaries, however, since now we are working with 4 features, we will need to use another function than the one generated here. If you want more information on that, please see nthis scikit-learn documentation webpage (https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html).\n",
    "\n",
    "What we can plot is the confusion matrix and the metrics for our three classes classificator with the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24953603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check the metricSs in this case\n",
    "compute_evaluation_metrics(decision_tree, X_test, y_test, iris['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ee61c4",
   "metadata": {},
   "source": [
    "Interestingly, although for the training set our classifier is unable to classify all samples correctly, it does a perfect classification for the test dataset.\n",
    "\n",
    "Okay, now instead of an individual decision tree we will train a random forest classifier with the same dataset. As a non-parametric method, we do not need to adjust the hyperparameters as much as when dealing with **SVM**. In fact, **scikit-learn** already optimizes the size **n** (chosen to be equal to the number of samples in the original training set) of the bootstrap sample and the number of features **d** (**scikit-learn** chooses $d=\\sqrt{m}$ where $m$ is the number of features at the training set) that is randomly chosen for each iteration.\n",
    "\n",
    "Through **n** we control the bias-variance tradeoff of the random forest:\n",
    "- For larger values for **n** we decrease the randomness and thus the forest is more likely to overfit whereas we can reduce the degree of overitting by choosing smaller **n** values at the expense of the model performance.\n",
    "- The optimal for **d** is just a smaller value than the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30381b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# We check the metrics in this case\n",
    "compute_evaluation_metrics(decision_tree, X_test, y_test, iris['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193abba8",
   "metadata": {},
   "source": [
    "In the same way as the decision tree, the random forest that we trained is able to properly separate the three flower classes.\n",
    "\n",
    "One of the interesting features of the random forest classifier is that, as an **embedded method** (remember the **feature selection** theory), by building random decision trees from the data the ensemble model is able to better grasp the relative importance of each of the features in the classification of the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63a2a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris['feature_names'])\n",
    "print('feature importances:', random_forest.feature_importances_)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.bar(iris['feature_names'], random_forest.feature_importances_)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel('Relative feature importance', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ad4b4",
   "metadata": {},
   "source": [
    "We can see that to classify flowers the features with the highest relative importance is the petal lengh and width (the two features of the peatals, rather than the information from the sepals)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d0ed3",
   "metadata": {},
   "source": [
    "### Ensemble Methods: XGBoost\n",
    "\n",
    "While Logistic Regression and SVM are fundamental, **Gradient Boosting Machines (GBMs)** like **XGBoost** are currently the gold standard for tabular biological data classification due to their ability to handle non-linearities and missing data natively.\n",
    "\n",
    "We have just explored Random Forests, which are an example of **Bagging** (Bootstrap Aggregating). In Bagging, we build many independent trees in parallel and average their results to reduce variance and overfitting.\n",
    "\n",
    "**XGBoost** (Extreme Gradient Boosting) is also an ensemble method based on decision trees, but it follows a completely different philosophy called **Boosting**. Unlike Random Forest, where trees are built independently, in Boosting, the trees are built **sequentially**. Each new tree is designed specifically to correct the mistakes made by the previous trees.\n",
    "\n",
    "While a Random Forest creates a \"democracy\" of trees that vote, XGBoost creates a \"team\" of trees where each member improves upon the work of the team so far. It is called \"Gradient\" boosting because it uses a gradient descent algorithm to minimize the loss function (the errors) when adding new models.\n",
    "\n",
    "The algorithm behind **XGBoost** involves an iterative process. It starts with a naive prediction and then calculates the errors (residuals). The next tree is trained to predict these residuals rather than the target class itself.\n",
    "\n",
    "Therefore, the steps for an XGBoost classifier are:\n",
    "\n",
    "1. **Initial Prediction:** Start with a base prediction (e.g., the probability of 0.5 for binary classification).\n",
    "2. **Calculate Residuals:** Determine the difference between the predicted values and the actual values (the error).\n",
    "3. **Train Shallow Tree:** Build a decision tree (often called a \"weak learner\") specifically to predict these residuals.\n",
    "4. **Update Model:** Add this new tree to the ensemble, scaling its contribution by a **learning rate** (to prevent overfitting).\n",
    "5. **Repeat:** Repeat steps 2-4 until a specified number of trees are built or the error stops improving.\n",
    "\n",
    "---\n",
    "\n",
    "The **learning rule** of XGBoost is more mathematically complex than Random Forest. It optimizes a specific **Objective Function** that consists of two parts: a **Loss Function** (how well the model fits the data) and a **Regularization Term** (to penalize complex models and prevent overfitting).\n",
    "\n",
    "This regularization term  is what makes XGBoost \"Extreme.\" It controls the complexity of the trees (by penalizing the number of leaves and the magnitude of leaf weights), making XGBoost uniquely robust against overfitting compared to standard Gradient Boosting machines.\n",
    "\n",
    "---\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* **High Performance:** XGBoost is famous for winning machine learning competitions (Kaggle) due to its speed and accuracy.\n",
    "* **Regularization:** Includes L1 and L2 regularization to control overfitting.\n",
    "* **Handling Missing Values:** It has a built-in routine to handle missing data automatically.\n",
    "* **Parallel Processing:** Unlike standard boosting, XGBoost can parallelize the tree construction process, making it very fast.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "* **Hyperparameter Sensitivity:** Requires careful tuning of many parameters (learning rate, depth, regularization, etc.).\n",
    "* **Black Box:** Like Random Forest, it is difficult to interpret the specific path of logic for a single prediction.\n",
    "* **Outliers:** Because it tries to fix errors, it can sometimes be sensitive to outliers in the data.\n",
    "\n",
    "---\n",
    "\n",
    "For the practical example, we will continue using the **Iris dataset** from `scikit-learn` to maintain consistency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa98ce3",
   "metadata": {},
   "source": [
    "Just like with Random Forests, **XGBoost is a non-parametric method that does not strictly require feature scaling** (standardization), as it is based on tree splits. However, we do need to be careful with label encoding if our target variables were strings (here they are integers, so we are safe).\n",
    "\n",
    "We will now train the XGBoost model. Unlike Random Forest where we usually let trees grow deep, in XGBoost we often use \"shallow\" trees (controlled by `max_depth`) but use many of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0899ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=3, \n",
    "    random_state=42, \n",
    "    use_label_encoder=False, \n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "compute_evaluation_metrics(xgb_model, X_test, y_test, iris['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27150c8b",
   "metadata": {},
   "source": [
    "You will likely see that XGBoost achieves a very high accuracy, similar to the Random Forest, but often with better generalization on larger, more complex datasets.\n",
    "\n",
    "Just like Random Forest, XGBoost provides **feature importance**. However, while Random Forest calculates this based on the decrease in impurity (Gini), XGBoost calculates it based on the \"gain\" (improvement in accuracy) that each feature brings to the branches it is used in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae983cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('feature names:', iris['feature_names'])\n",
    "print('feature importances:', xgb_model.feature_importances_)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.bar(iris['feature_names'], xgb_model.feature_importances_)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel('Relative feature importance (Gain)', fontsize=18)\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a409b8c4",
   "metadata": {},
   "source": [
    "Compared to random forest, which gives a similar importance to the petal length and width, XGBoost tends to be more aggressive in selecting the most critical features that reduce the error, often resulting in a clearer distinction of which variables are the true \"signals\" in the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd48e53",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "The perceptron is one of the most basic binary classifiers, and the fundamental building block of artificial neural networks. The idea behind the perceptron is the biological neuron, where based on the different inputs collected from the dendrites it triggers/not triggers a signal on the axon to the next neuron:\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Neuron](images/Neurons.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Analogously, the perceptron takes as input one or a vector of continous independent variables and their weights, which is summed up and passes the result through a **binary step or unit step function** to produce a binary output $\\hat{y}$:\n",
    "\n",
    "$$\\hat{y} = \\begin{cases} 1 & \\text{if } w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n > 0 \\\\ 0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "where:\n",
    "- $\\hat{y}$ is the predicted output.\n",
    "- $w_0, w_1, w_2, ..., w_n$ are the weights associated with the inputs.\n",
    "- $x_1, x_2, ..., x_n$ are the input features.\n",
    "\n",
    "As we will while discussing advanced supervised methods there are multiple **activation functions** besides the binary step function that can be used. At any rate, the Perceptrons are trained (learn from the data) by adjusting its weights based on the error in its predictions: \n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Perceptron scheme](images/Perceptron.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "To understand the perceptron, we need to understand a concept that is key also for training neural networks: the **learning rule**. The learning rule is how the weights are updated based on the error in its predictions. For the perceptron, the updated weight $w^\\prime_i$ is computed iteratively during the training process until convergence is achieved as follows:\n",
    "\n",
    "$$w^\\prime_i \\leftarrow w_i + \\alpha \\cdot (y - \\hat{y}) \\cdot x_i$$\n",
    "\n",
    "where:\n",
    "- $w_i$ is the weight associated with the $i$-th input feature.\n",
    "- $\\alpha$ is the learning rate, a small positive constant of range [0, 1] that determines the step size in weight updates.\n",
    "- $y$ is the true label (the actual class of the instance).\n",
    "- $\\hat{y}$ is the predicted output of the perceptron.\n",
    "- $x_i$ is the $i$-th input feature.\n",
    "\n",
    "Hence, the **learning rule** of the perceptron essentially adjusts the weights to reduce the difference between the predicted output ($\\hat{y}$) and the true label ($y$), where the term $(y - \\hat{y})$ represents the error of prediction at iteration $i$-th. With this, the weights are updated in the direction that minimizes this error at a **learning rate** $\\alpha$. This constant is defined as an hyperparameter in order to maximize the search for the optimal solution: with a too small value, the alrgorithm will take too much time and computational resources to find the optimal solution (and also it might get stucked in local optimums) whereas a too large learning rate the algorithm might not use properly the gradient to find the optimas.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Different learning rates](images/Learning_rate.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "**Advantages:**\n",
    "- Perceptrons are conceptually **simple** and computationally **efficient**.\n",
    "- Perceptrons will **converge** to a solution if the data is linearly separable.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Perceptrons are **limited to linear separability** (can only learn linear decision boundaries).\n",
    "- Sensitivity to Outliers: Outliers can significantly impact perceptron performance.\n",
    "- Perceptrons only produce binary outputs **without associated probabilisties**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953be41",
   "metadata": {},
   "source": [
    "As a practical example, we will use the output of PCA applied to the transcriptomic profile across blood tumors. We will try to train models that are able to differentiate different histological subtypes of primary blood cancers based on the two first principal components (linear combinations of genes expression that explain most of the variability in expression).\n",
    "\n",
    "In order to define histological subtypes, we will have to download that information from ICGC, contained in the **pcawg_specimen_histology_August2016_v9.xlsx** excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99fd075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "# To ignore some plot warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We load the expression data\n",
    "expression_df = pd.read_csv(path.join('data', 'gene_expression.tsv.gz'),\n",
    "                                                        sep=\"\\t\", header='infer', index_col=0, compression='gzip')\n",
    "\n",
    "# Get sample dataframe with the information\n",
    "sample_df = pd.read_csv(path.join('data', 'sample_df.tsv'), sep=\"\\t\", header='infer')\n",
    "\n",
    "# Finally we load directly from the ICGC database the histology excel file\n",
    "histology_df = pd.read_excel('data/pcawg_specimen_histology_August2016_v9.xlsx')\n",
    "histology_df.columns = ['icgc_specimen_id'] + list(histology_df.columns[1:])\n",
    "histology_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11232648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dictionary to translate the Specimen IDs to the histological subtype\n",
    "tumortype_dict = dict(zip(histology_df.icgc_specimen_id, histology_df.histology_tier3))\n",
    "\n",
    "# Get the specimensID of blood cancers\n",
    "specimens_blood = sample_df[sample_df['primary_location']=='Blood']['icgc_specimen_id']\n",
    "\n",
    "# Intersect with the available columns to find the blood cancer specimens with available expression\n",
    "subset_specimens = list(set(expression_df.columns).intersection(set(specimens_blood)))\n",
    "\n",
    "# Filter the data to do the PCA only with blood cancer specimens\n",
    "blood_expression_df = expression_df[subset_specimens].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd7f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We preprocess the data with standarization\n",
    "scaler = StandardScaler()\n",
    "blood_data = scaler.fit_transform(blood_expression_df.T)\n",
    "\n",
    "# We perform the PCA\n",
    "blood_pca2D = PCA(n_components=2)\n",
    "blood_proj_data = blood_pca2D.fit_transform(blood_data)\n",
    "\n",
    "# We can check the amount of variance explained by the two Principcal components\n",
    "print(blood_pca2D.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c28efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def plot_PCA_withlabels(proj_data, labels, plotname):\n",
    "    \n",
    "    # Automatically create a mapping from label categories to integers\n",
    "    unique_labels = np.unique(labels)\n",
    "    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    # Convert labels to integers based on the automatic mapping\n",
    "    label_integers = np.array([label_mapping[label] for label in labels])\n",
    "    # Automatically generate a ListedColormap with unique colors based on the number of labels\n",
    "    num_colors = len(unique_labels)\n",
    "    color_map = plt.get_cmap('tab20', num_colors)\n",
    "    # Create a ListedColormap with unique colors\n",
    "    listed_color_map = ListedColormap([color_map(idx) for idx in range(num_colors)])\n",
    "\n",
    "    # Plot with colours \n",
    "    # We plot the data (the first two components are the first two columns of proj_data)\n",
    "    scatter = plt.scatter(proj_data[:,0], proj_data[:,1], s=2, c=label_integers, cmap=listed_color_map)\n",
    "\n",
    "    # Create legend handles and labels\n",
    "    legend_handles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map(idx), markersize=10, label=label)\n",
    "                      for idx, label in enumerate(unique_labels)]\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend(handles=legend_handles, title='Tumor Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.title('Transcriptome')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "\n",
    "    # Save image\n",
    "    plt.savefig(path.join('plots', plotname))\n",
    "    plt.show()\n",
    "    \n",
    "    return(label_integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b5b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tumor type for each sample (each column)\n",
    "labels_blood = blood_expression_df.columns.map(tumortype_dict)\n",
    "blood_label_integers = plot_PCA_withlabels(blood_proj_data, labels_blood, 'Blood_PCA.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11a551",
   "metadata": {},
   "source": [
    "There is a clear separation between chronic lymphocitic leukemias and mature B-cell lymphomas. Now, we can train a perceptron using scikit-learn that can separate both groups. For this kind of classifier models it is useful to **standarize** the data as we saw for the dimensionality reduction algorithms, however, the output of the PCA is already standarized so both **training** and **test** dataset, which derive from the same standarize output, will have the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45af3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the dependent and independent variables\n",
    "## The labels are the two types of blood cancers we want to separate, encoded as 0s and 1s.\n",
    "y = pd.DataFrame(blood_label_integers)\n",
    "## The two features are the two first PC, already on the 2D numpy array format (columns are the PC)\n",
    "X = pd.DataFrame(blood_proj_data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "ppn = Perceptron(eta0=0.1, random_state=42)\n",
    "ppn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c454dfa6",
   "metadata": {},
   "source": [
    "The model is already trained with the training dataset, but before plotting the results on the test dataset we can define a function to conviniently visualize the decision boundary regions that arise from training the perceptron or other classifier models that we will see on this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c786e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, y, classifier, resolution, test_idx=None, labels=None):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    cmap = plt.get_cmap('Paired', len(markers))\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n",
    "    \n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=1, c=cmap(idx), \n",
    "                    marker=markers[idx], label=labels[idx])\n",
    "        \n",
    "    # highlight test samples\n",
    "    if test_idx.any():\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c='black',\n",
    "        alpha=0.2, linewidth=0.1, marker='o',\n",
    "        s=55, label='test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7bcce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the indexes into numpy format to mark the test dataset\n",
    "test_idx = X_test.index.to_numpy()\n",
    "# The function needs the independent and dependent dataframes to be passed as numpy arrays\n",
    "X_numpy = X.to_numpy()\n",
    "y_numpy = y[0].to_numpy()\n",
    "\n",
    "# The names of the classes, not encoded as 0s ans 1s for plot purposes\n",
    "unique_labels = list(labels_blood.unique())\n",
    "\n",
    "# Call the function to generate the plot\n",
    "plot_decision_regions(X_numpy, y_numpy, ppn, 0.1, test_idx, unique_labels)\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Save image\n",
    "plt.savefig(path.join('plots', 'Perceptron_blood.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2c86e",
   "metadata": {},
   "source": [
    "Since the two classes can be separated linearly, the perceptron works well on the classification task. This is reflected on the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b974957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
    "\n",
    "def compute_evaluation_metrics(model, X_test, y_test, labels):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "    # Precision, Recall, F1 Score for binary classification\n",
    "    if len(np.unique(y_test)) == 2:\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        print(f'Precision: {precision:.2f}')\n",
    "        print(f'Recall: {recall:.2f}')\n",
    "        print(f'F1 Score: {f1:.2f}')\n",
    "\n",
    "    # Precision, Recall, F1 Score for multi-class classification\n",
    "    else:\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        print(f'Precision: {precision:.2f}')\n",
    "        print(f'Recall: {recall:.2f}')\n",
    "        print(f'F1 Score: {f1:.2f}')\n",
    "\n",
    "    # Confusion Matrix\n",
    "    print('Confusion Matrix:')\n",
    "    disp = ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, display_labels=labels, xticks_rotation='vertical')\n",
    "    \n",
    "    \n",
    "compute_evaluation_metrics(ppn, X_test, y_test, unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff71e62",
   "metadata": {},
   "source": [
    "But, it will work that well on a dataset with no clear lineal decision boundary? Let's try for instance with Kidney cancer, where there are three subtypes of adenocarcinomas depending on the affected cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33071dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dictionary to translate the Specimen IDs to the histological subtype\n",
    "subtype_tumortype_dict = dict(zip(histology_df.icgc_specimen_id, histology_df.histology_tier4))\n",
    "\n",
    "# Get the specimensID of blood cancers\n",
    "specimens_kidney = sample_df[sample_df['primary_location']=='Kidney']['icgc_specimen_id'].copy()\n",
    "\n",
    "# Intersect with the available columns to find the blood cancer specimens with available expression\n",
    "subset_specimens = list(set(expression_df.columns).intersection(set(specimens_kidney)))\n",
    "\n",
    "# Filter the data to do the PCA only with blood cancer specimens\n",
    "kidney_expression_df = expression_df[subset_specimens].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d561aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We preprocess the data with standarization\n",
    "scaler2 = StandardScaler()\n",
    "kidney_data = scaler2.fit_transform(kidney_expression_df.T)\n",
    "\n",
    "# We perform the PCA\n",
    "kidney_pca2D = PCA(n_components=2)\n",
    "kidney_proj_data = kidney_pca2D.fit_transform(kidney_data)\n",
    "\n",
    "# We can check the amount of variance explained by the two Principal components\n",
    "print(kidney_pca2D.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tumor type for each sample (each column)\n",
    "labels_kidney = kidney_expression_df.columns.map(subtype_tumortype_dict)\n",
    "kidney_label_integers = plot_PCA_withlabels(kidney_proj_data, labels_kidney, 'Kidney_PCA.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbe3466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dependent and independent variables\n",
    "## The labels are the two types of kidney cancers we want to separate, encoded as 0s, 1s and 2s.\n",
    "y = pd.DataFrame(kidney_label_integers)\n",
    "## The two features are the two first PC, already on the 2D numpy array format (columns are the PC)\n",
    "X = pd.DataFrame(kidney_proj_data)\n",
    "\n",
    "# The names of the classes, not encoded as 0s, 1s and 2s for plot purposes\n",
    "unique_labels = list(labels_kidney.unique())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "ppn = Perceptron(eta0=0.1, random_state=42)\n",
    "ppn.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_regions(X.to_numpy(), y[0].to_numpy(), ppn, 0.1, X_test.index.to_numpy(), unique_labels)\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Save image\n",
    "plt.savefig(path.join('plots', 'Perceptron_kidney.png'))\n",
    "plt.show()\n",
    "\n",
    "# We check the metrics in this case\n",
    "compute_evaluation_metrics(ppn, X_test, y_test, unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd03de9",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Logistic Regression is a popular statistical method used for binary classification tasks, although, similarly to the perceptron, it is possible to implement it on multi-class taks by dividing the problem in multiple binary classifiers of one vs the rest or **OvR** (in this case we have a **multinomial logistic classification**).\n",
    "\n",
    "The logistic regression model works by transforming a linear combination of input features using a **logit function** (do not confuse with the **logistic or sigmoid function** which is the inverse) as follows:\n",
    "\n",
    "$$P(Y) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "where:\n",
    "- $P(Y)$ is the probability of the target (dependent) variable.\n",
    "- $e$ is the base of the natural logarithm.\n",
    "- $z$ is the linear combination of input features and their corresponding weights: $z = β₀ + β₁x₁ + β₂x₂ + ... + β_nx_n$.\n",
    "  \n",
    "Hence, the logit function maps any real-valued number to the range [0, 1], which is crucial for interpreting the output as a probability, and thus, assign each element to a given class based on the ratio of probabilities. Then, once the model is trained, we can use the inverse **logistic or sigmoid function** to predict the probability that a certain sample belongs to a particular class given the input features.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Logit function](https://i.stack.imgur.com/WY61Z.png)](https://en.wikipedia.org/wiki/Logit)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "On this setting, the linear combination of input features is related with the logarithm of the **odds-ratio** (also known as **log-odds**) since from the logistic regression formula we can derive that:\n",
    "\n",
    "$$logit(P(Y)) = ln(\\frac{P(Y)}{1 - P(Y)}) = β₀ + β₁x₁ + β₂x₂ + ... + β_nx_n$$\n",
    "\n",
    "The **odds-ratio**, expressed as $\\frac{P(Y)}{1 - P(Y)}$, is the relationship of the probability of one event with respect to the opposite one (on a binary situation) and can be used as the association strength between two events. From the relative probabilities of belonging to once class vs others, a **unit step function or quantizer** chooses the class with the highest probability to provide that as output.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Logistic classificator scheme](images/Logistic.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "\n",
    "The **learning rule** here is the maximization of a **likelihood function** (from a simplified perspective, the algorithm finds the proper weights by maximizing a function that provides the **probability of observing the training data given the parameters of the model**, if you need more information look at the bibliography).\n",
    "\n",
    "**Advantages:**\n",
    "- Logistic regression provides **interpretable** results since the coefficients can be directly interpreted in terms of changes in log odds: it provides with probabilities during the classification.\n",
    "- Training logistic regression models is computationally **efficient** and scales well to large datasets.\n",
    "\n",
    "**Disadvantages:**\n",
    "- A critical assumption is the **absence of extreme outliers** in the dataset, which distort the training (could be verified by calculating Cook’s distance (Di) to identify influential data points that may negatively affect the regression model).\n",
    "- Logistic regression **assumes a linear relationship between the log-odds and the independent variables**, which translated into a linear decision boundary. This might not be suitable for datasets with complex, non-linear relationships between features (better use support vector machines or decision tress and derivates).\n",
    "- Assumes little to no multicollinearity between explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0efe40",
   "metadata": {},
   "source": [
    "Let's see how it works for both blood and kidney cancer examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3703572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the dependent and independent variables\n",
    "## The labels are the two types of blood cancers we want to separate, encoded as 0s and 1s.\n",
    "y = pd.DataFrame(blood_label_integers)\n",
    "## The two features are the two first PC, already on the 2D numpy array format (columns are the PC)\n",
    "X = pd.DataFrame(blood_proj_data)\n",
    "\n",
    "# The names of the classes, not encoded as 0s and 1s for plot purposes\n",
    "unique_labels = list(labels_blood.unique())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_regions(X.to_numpy(), y[0].to_numpy(), lr, 0.1, X_test.index.to_numpy(), unique_labels)\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Save image\n",
    "plt.savefig(path.join('plots', 'Logistic_blood.png'))\n",
    "plt.show()\n",
    "\n",
    "# We check the metrics in this case\n",
    "compute_evaluation_metrics(lr, X_test, y_test, unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8246242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dependent and independent variables\n",
    "## The labels are the two types of kidney cancers we want to separate, encoded as 0s, 1s and 2s.\n",
    "y = pd.DataFrame(kidney_label_integers)\n",
    "## The two features are the two first PC, already on the 2D numpy array format (columns are the PC)\n",
    "X = pd.DataFrame(kidney_proj_data)\n",
    "\n",
    "# The names of the classes, not encoded as 0s, 1s and 2s for plot purposes\n",
    "unique_labels = list(labels_kidney.unique())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_regions(X.to_numpy(), y[0].to_numpy(), lr, 0.1, X_test.index.to_numpy(), unique_labels)\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Save image\n",
    "plt.savefig(path.join('plots', 'Logistic_blood.png'))\n",
    "plt.show()\n",
    "\n",
    "# We check the metrics in this case\n",
    "compute_evaluation_metrics(lr, X_test, y_test, unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d135ae81",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)\n",
    "\n",
    "Support Vector Machines (SVM) are powerful supervised learning models used for classification and regression tasks. SVM aims to find the optimal hyperplane that separates data points of different classes in feature space. The hyperplane is defined by:\n",
    "\n",
    "$$f(x) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$$\n",
    "\n",
    "where:\n",
    "- $f(x)$ is the decision function.\n",
    "- $\\beta_0, \\beta_1, \\beta_2, ..., \\beta_n$ are the coefficients (weights) to be learned.\n",
    "- $x_1, x_2, ..., x_n$ are the input features.\n",
    "\n",
    "In contrast with the perceptron, where the objective was to minimized misclassification errors, in SVMs the optimization objective is to **maximize the margin** (the **learning rule** of SVM), defined as the distance between the separating hyperplane (the decision boundary) and the training samples that are closest to this hyperplane, which are the so-called **support vectors** (the data points that lie closest to the decision boundary). This is illustrated in the following figure:\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Margin concept](images/Margin.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "The rationale behind having decision boundaries with large margins is that they tend to have a lower generalization error (the decision boundaries are less influenced by the training dataset) whereas models such as the perceptron with small margins are more prone to fall into **overitting** issues. However, as you might understand, this concept of **maximizing the margins** is therorethically suitable if there is an hyperplane that is able to separate all the training groups. But what about cases where that is impossible? How can we build a good enough model in this situation where margins are not that clear?\n",
    "\n",
    "To assess that the algorithm takes into account a concept known as **soft-margin classification**, where an internal extra parameter called **slack variable** (see the bibliography for mathematical details) allows to relax the constrains for nonlinearly separable data to allow convergence of the optimization in the presence of misclassifications under the appropriate cost penalization. This is controlled by an **hyperparameter of the model C** where increasing values of C increases the bias and lowers the variance of the model to adjust for the **bias-variance** tradeoff:\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Regularization parameter C](images/ParamC.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "\n",
    "**Advantages:**\n",
    "- Effective in high-dimensional spaces.\n",
    "- Can handle **linear and non-linear relationships** using different kernel functions.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Non-linear relationships](images/Kernel.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "- **Robust to overfitting**, especially in high-dimensional spaces. Thanks to the hyperparameter C.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Training is more **computationally expensive** can be high for large datasets.\n",
    "- SVMs are **sensitive to noise** present on the data. It is important to play with the hyperparameter C to adjust for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c94880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the dependent and independent variables\n",
    "## The labels are the two types of blood cancers we want to separate, encoded as 0s and 1s.\n",
    "y = pd.DataFrame(blood_label_integers)\n",
    "## The two features are the two first PC, already on the 2D numpy array format (columns are the PC)\n",
    "X = pd.DataFrame(blood_proj_data)\n",
    "\n",
    "# The names of the classes, not encoded as 0s and 1s for plot purposes\n",
    "unique_labels = list(labels_blood.unique())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_regions(X.to_numpy(), y[0].to_numpy(), svm, 0.1, X_test.index.to_numpy(), unique_labels)\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Save image\n",
    "plt.savefig(path.join('plots', 'SVM_c1_blood.png'))\n",
    "plt.show()\n",
    "\n",
    "# We check the metrics in this case\n",
    "compute_evaluation_metrics(svm, X_test, y_test, unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c6b7fb",
   "metadata": {},
   "source": [
    "Here the classifier works in a similar way to the simple perceptron that we trained at the start of this session. However, notice that the **decision boundary** now is optimized to maximize the margin between the two training sets and avoid overfitting. If we apply the same settings to train a model for the kidney cancer, where there is no clear linear separation, the performance is less optimal and more similar to the one obtained with the **logistic model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa6752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dependent and independent variables\n",
    "## The labels are the two types of kidney cancers we want to separate, encoded as 0s, 1s and 2s.\n",
    "y = pd.DataFrame(kidney_label_integers)\n",
    "## The two features are the two first PC, already on the 2D numpy array format (columns are the PC)\n",
    "X = pd.DataFrame(kidney_proj_data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# The names of the classes, not encoded as 0s, 1s and 2s for plot purposes\n",
    "unique_labels = list(labels_kidney.unique())\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_regions(X.to_numpy(), y[0].to_numpy(), svm, 0.1, X_test.index.to_numpy(), unique_labels)\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Save image\n",
    "plt.savefig(path.join('plots', 'SVM_linear_kidney.png'))\n",
    "plt.show()\n",
    "\n",
    "# We check the metrics in this case\n",
    "compute_evaluation_metrics(svm, X_test, y_test, unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b227e48",
   "metadata": {},
   "source": [
    "However, unlike the **logistic model** which cannot work with non-linear decision boundaries, on **SVM** we can use the kernel trick to solve non-linear classifications. This is the key aspect why SVM enjoy high popularity among machine learning practitioners.\n",
    "\n",
    "To simplify, kernels can be interpreted as a similarity function between pairs of samples so we can model an \"extra non-existent dimension\" to allow for non-linear boundaries. One of the most popular **kernel functions** is the **Radial Basis Function (RBF) or Gaussian kernel**, which is the one we are going to implement in our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's simply change the kernel from linear to rbf\n",
    "# We will also play with the C parameter to try to fit the data\n",
    "svm = SVC(kernel='rbf', C=80, random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_regions(X.to_numpy(), y[0].to_numpy(), svm, 0.1, X_test.index.to_numpy(), unique_labels)\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Save image\n",
    "plt.savefig(path.join('plots', 'SVM_rbf_kidney.png'))\n",
    "plt.show()\n",
    "\n",
    "# We check the metrics in this case\n",
    "compute_evaluation_metrics(svm, X_test, y_test, unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4564ab26",
   "metadata": {},
   "source": [
    "For instance with a C hyperparameter of 80 we are forcing the **SVM** to try to fit better the model that tries to classify class 2 (the most difficult one to separate) even if we are incurring into large levels of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f892e3a",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "To understand the Random Forest, first we need to understand the **Decision Trees**: a classic tree-like model used for classification based on multiple \"decisions\" based on several independent variables.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Decision trees](https://miro.medium.com/v2/resize:fit:1280/0*4QE-0kavxXfzF_bR.png)](https://en.wikipedia.org/wiki/Decision_tree)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Since each of these decision trees is considered a model per se, a **Random forest** which is a model arising from the combination of multiple decision trees, it is an **ensemble method**, that is, a machine learning model composed of multiple small models with the idea of outperforming the accuracy obtained from individual models alone.\n",
    "\n",
    "Therefore, the algorithm behind **Random forest** models involves the recursive splitting of the dataset based on the features that best separate the data into distinct classes or groups. The goal is to create decision rules that efficiently partition the data by randomly sampling of the features that best separate the data into distinct classes or groups. Therefore, the steps for a random forest classifier are:\n",
    "\n",
    "1. **Build Decision Trees:** Create multiple decision trees using bootstrapped samples and random subsets of features. Hence, each building block of the random forest is trained on a random subset of the training data and features, allowing for diversity.\n",
    "\n",
    "2. **Aggregate Predictions:** Combine the predictions of individual trees through voting or averaging.\n",
    "\n",
    "3. **Model Evaluation:** Assess the model's performance using metrics like accuracy or F1 score.\n",
    "\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Random forest classifier](https://www.freecodecamp.org/news/content/images/2020/08/how-random-forest-classifier-work.PNG)](https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Random forest **can handle both classification and regression taks**. The **learning rule** of this classifier is to maximize the \"discriminative power\" for classification taks and the error distance for regression tasks. This is commonly evaluated with the **Gini impurity** metric for classification tasks or the **mean squared error** for regression tasks, although there are many other metrics such as the **entropy level** or the **misclassification error**. Let's take a closer look at what the **Gini impurity** (and other metrics) reflect.\n",
    "\n",
    "The **Gini impurity** or **Gini index** is a measure of the uncertainty at each split points (the nodes) of a decision tree. That is, **measures the likelihood of misclassifying a randomly chosen element from the set**. Mathematically, it is calculated for the set $D$ to be splited as follows:\n",
    "\n",
    "$$Gini(D) = 1 - \\sum_{i=1}^{K} (p_i)^2$$\n",
    "\n",
    "where:\n",
    "- $K$ is the number of classes in the dataset.\n",
    "- $p_i$ is the probability of randomly picking an element of class $i$ from the set $D$.\n",
    "\n",
    "This measure ranges between 0 and 1, where lower values indicating a purer or less impure dataset. Notice that Gini impurity will be 0 if the set contains only elements of a single class (and hence, there is no need to discriminate), whereas 1 indicates that all classes are equally represented.\n",
    "\n",
    "In the context of decision trees, the algorithm uses the Gini impurity to find the splits that minimizes the Gini impurity across the resulting child nodes, that means, whose child nodes are pure of one of the classes we want to separate. Hence, the split that leads to the lowest Gini impurity is chosen as the best split through an iterative work until the decision tree is completed.\n",
    "\n",
    "**Advantages:**\n",
    "- Random Forests reduce are **robust to overfitting** by averaging the predictions of multiple trees.\n",
    "- Capable of **capturing complex, non-linear relationships** in the data.\n",
    "- Provides a measure of feature importance such as the **Gini impurity**.\n",
    "- There is no need to pre-process the data in terms of feature scaling such as standarization.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Random Forests **cannot be easily interpreted**, often considered as \"black-box\" models\n",
    "- Can be **computationally expensive** for large datasets and many trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e328c846",
   "metadata": {},
   "source": [
    "For the following examples instead of the our multiomics dataset, we will work with a mock dataset that is already present on the scikit-learn package, which will be better to exemplify the largest power of decision trees: working with high number of features and evaluate the importance of each of them in the classification power. This is the dataset that will be used for the first deadline of delivery exercises. First we will build a simple decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b555c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "# Continous independent variables\n",
    "X = pd.DataFrame(iris['data'])\n",
    "# Labels or dependent variable (discrete classes)\n",
    "y = pd.DataFrame(iris['target'])\n",
    "\n",
    "print(iris.keys())\n",
    "print('')\n",
    "print('feature names:', iris['feature_names'])\n",
    "print('target names:', iris['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b8188",
   "metadata": {},
   "source": [
    "Until here, we have seen many machine learning algorithms that require to preprocess the data (we have been using **standarization** if our original data had features that were not already scaled), however, one of the main advantatges of decision trees and random forests is that, as non-parametric methods, we do not need to worry about scaling features.\n",
    "\n",
    "We are going to train a decision tree of maximum of three nodes (parameter max_depth on 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a291e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# We can skip the standarization step\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plot_tree(decision_tree, fontsize=12, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd036b",
   "metadata": {},
   "source": [
    "The plot tree function provides with an encoded decision tree, which is translated into an understandable tree plot. As we can see, the first decision, that the petal length (in cm) is less or equal than 2.45 (x[2] means the third variable, remember that python indexes start at 0) is able to separate all the 40 setosa samples from the other 41 versicolor and 39 virginica. Note how the gini impurity index is lower on the next nodes, since they are less discriminative that the nodes above. Sadly, we only three nodes we are not able to fully separate all the remaining versicolor and virginica flowers.\n",
    "\n",
    "We can display the decision boundaries, however, since now we are working with 4 features, we will need to use another function than the one generated here. If you want more information on that, please see nthis scikit-learn documentation webpage (https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html).\n",
    "\n",
    "What we can plot is the confusion matrix and the metrics for our three classes classificator with the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check the metrics in this case\n",
    "compute_evaluation_metrics(decision_tree, X_test, y_test, iris['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c163a4e",
   "metadata": {},
   "source": [
    "Interestingly, although for the training set our classifier is unable to classify all samples correctly, it does a perfect classification for the test dataset (hence, the \n",
    "\n",
    "Okay, now instead of an individual decision tree we will train a random forest classifier with the same dataset. As a non-parametric method, we do not need to adjust the hyperparameters as much as when dealing with **SVM**. In fact, **scikit-learn** already optimizes the size **n** (chosen to be equal to the number of samples in the original training set) of the bootstrap sample and the number of features **d** (**scikit-learn** chooses $d=\\sqrt{m}$ where $m$ is the number of features at the training set) that is randomly chosen for each iteration.\n",
    "\n",
    "Through **n** we control the bias-variance tradeoff of the random forest:\n",
    "- For larger values for **n** we decrease the randomness and thus the forest is more likely to overfit whereas we can reduce the degree of overitting by choosing smaller **n** values at the expense of the model performance.\n",
    "- The optimal for **d** is just a smaller value than the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac4cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# We check the metrics in this case\n",
    "compute_evaluation_metrics(decision_tree, X_test, y_test, iris['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d0f9b0",
   "metadata": {},
   "source": [
    "In the same way as the decision tree, the random forest that we trained is able to properly separate the three flower classes.\n",
    "\n",
    "One of the interesting features of the random forest classifier is that, as an **embedded method** (remember the **feature selection** theory), by building random decision trees from the data the ensemble model is able to better grasp the relative importance of each of the features in the classification of the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf5884",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris['feature_names'])\n",
    "print('feature importances:', random_forest.feature_importances_)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.bar(iris['feature_names'], random_forest.feature_importances_)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel('Relative feature importance', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b209ca",
   "metadata": {},
   "source": [
    "We can see that to classify flowers the features with the highest relative importance is the petal lengh and width (the two features of the peatals, rather than the information from the sepals)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325bd95",
   "metadata": {},
   "source": [
    "# Advanced supervised methods: neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e70a4c",
   "metadata": {},
   "source": [
    "Neural networks are powerful group of supervised machine learning models that inspired by the human brain's neural structure.\n",
    "\n",
    "Within the core of Neural Networks lies the **perceptron**, which serves as their basic building block. As we have seen, the perceptron is a binary classifier that takes multiple input features, applies weights, sums them up, and passes the result through an activation function to produce an output. However, we have seen that perceptrons have severe limitations, especially when it comes to handling non-linear relationships in data.\n",
    "\n",
    "To address the limitations of perceptrons, the **multi-layer perceptron (MLP)** was introduced. MLP consists of multiple layers of interconnected perceptrons, including an input layer, one hidden layer and an output layer. Each layer introduces non-linearity through activation functions, allowing the model to learn complex mappings between inputs and outputs.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Multi-layered perceptron](https://www.allaboutcircuits.com/uploads/articles/an-introduction-to-training-theory-for-neural-networks_rk_aac_image2.jpg)](https://www.allaboutcircuits.com/technical-articles/how-to-train-a-multilayer-perceptron-neural-network/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "How MLP are able to solve non-linearly separable problems is perfectly exemplified by the **XOR problem**, a classic challenge in artificial intelligence and machine learning. The XOR gate, or exclusive OR, is a logical operation that returns true (1) only when the number of true inputs is odd, and false (0) when the inputs are the same:\n",
    "\n",
    "- $0 \\oplus 0 = 0$\n",
    "- $0 \\oplus 1 = 1$\n",
    "- $1 \\oplus 0 = 1$\n",
    "- $1 \\oplus 1 = 0$\n",
    "\n",
    "The difficulty with the XOR problem lies in finding a linear decision boundary that separates the two classes (true and false) in a 2D space. A single-layer perceptron, being a linear classifier, is unable to solve the XOR problem as it can only create linear decision boundaries. Hence, to successfully solve the XOR problem, a more complex model with non-linear activation functions and multiple layers, such as a MLP or other types of neural networks are required, where the hidden layers allow to capture the non-linear relationships between input features.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![XOR problem](https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/04/bitwise_datasets.png)](https://pyimagesearch.com/2021/05/06/implementing-the-perceptron-neural-network-with-python/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "If you want a mathematically detailed explanation on how a simple MPL can solve this problem, see this video: https://youtu.be/dM_8Y41EgsY?si=BmcvwLx-qQCvO8Fd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415b2975",
   "metadata": {},
   "source": [
    "Beyond the simple **MLP** explained before there are other types of more complex artificial neural networks, which include **more than one hidden layer**: the so-called **deep neural networks (DNNs)** that are studied within the **deep learning subfield of machine learning**. There are basically two types of neural networks depending on the direction of information propagation: **feed-forward neural networks (FNNs)**, where the information is propagated unidirectionally, and **Recurrent Neural Networks (RNNs)** where the information flows bidirectionally across its layers.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Types of DNNs](https://www.researchgate.net/profile/Engin-Pekel/publication/315111480/figure/fig1/AS:472548166115333@1489675670530/Feed-forward-and-recurrent-ANN-architecture.png)](https://www.researchgate.net/figure/Feed-forward-and-recurrent-ANN-architecture_fig1_315111480)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "This distiction is relevant in terms of how to the DNNs are trained. For instance, the most common method for training FNNs is **backpropagation** and its modifications (RNNs, on the other hand, use an slightly modified algorithm that fits the particularities of these type of neural networks). In general terms, the iterative process involves three steps:\n",
    "\n",
    "1. **Forward Pass**: input data is passed through the network layer by layer, with each layer performing a computation based on the inputs it receives and passing the result to the next layer.\n",
    "\n",
    "    - Input values from the training dataset are fed into the neural network through the **input layer**, where each feature corresponds to a node. Hence, the number of neurons in the input layer is determined by the dimensionality of the input data.\n",
    "    - This input layer is then feeded to the **first hidden layer**, which process the input information as a weighted sum of inputs from the previous layer and then applies an activation function (remember that each neuron is in fact a perceptron) to generate the output of this layer, which is in turn passed to the next hidden layer (the **second hidden layer**) until the **final hidden layer** (it can also be the second if the DNN only has two hidden layers). \n",
    "    - The output of the final hidden layer is then passed into the **output layer**. These neurons also apply an activation function to the weighted sum of inputs (commonly a **softmax activation function** for classification tasks) to obtain the **predicted output**. At the end, the predicted output is compared with the actual values (the labels, remember that we are training a supervised method) through modelling of a **loss function**: a mathematical relationship that allows to model the error of the prediction, for example the **root of mean squared errors (RMSE)** for regression tasks.  \n",
    "\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "2. **Backward Pass (*stricto sensu* Backpropagation)**: By means of the **loss function**, and moving backwards from the output layer, the gradients of the loss (\"the change in the error\") is computed for each neuron in the hidden layers. This is performed though the **chain rule of calculus**, which allows to decomposes the gradient of the overall loss with respect to the weights for each layer.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "3. **Weight Update (Gradient Descent)**: The gradients are used to update the weights in the direction that reduces the loss. The **learning rule** here is to minimize the **loss function** by exploring the space of weights. As we saw for the perceptron, the **learning rate** is a key hyperparameter that will determine the size of the steps taken each iteration.\n",
    "\n",
    "This process of subsequent forward and backward passes is performed for a specified number of iterations until the algorithm reaches convergence or until the user stops the training.\n",
    "\n",
    "![Backpropagation](https://i.gifer.com/6fyP.gif)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "        \n",
    "There are a few concepts mentioned on the explanation of the algorithm that need extra discussion, some of them covered in less depth while introducing the perceptron:\n",
    "\n",
    "- **Activation functions**: we introduced the **unit or binary step** for our perceptron example, however there are multiple activation functions that could be used to learn specific complex patterns, usually combined across layers of a neural network arquitecture. Common activation functions include the **sigmoid**, **tanh (hyperbolic tangent)**, **ReLU (Rectified Linear Unit)**, and the **softmax**.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Activation functions](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/627d12431fbd5e61913b7423_60be4975a399c635d06ea853_hero_image_activation_func_dark.png)](https://www.v7labs.com/blog/neural-networks-activation-functions)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "- **Loss Function**: The choice of the loss function will depend on the task at hand (e.g., mean squared error for regression, cross-entropy for classification,...), however, all of them quantify the difference between predicted and actual values and allows to find the gradient of the loss at each neuron through the **chain rule of calculus**.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "- **Learning Rate**: The learning rate, as the hyperparameter that controls the step size during weight updates, is key on the speed and stability of the training process. \n",
    "\n",
    "\n",
    "*Small learning rates have the advantatge of approximating the minimum of the loss function better than large learning rates which could fail to closely approximate the minimum.*\n",
    "\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Small learning rate](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*JbiGghtQBtiaHYVdP_hprQ.gif)](https://medium.com/codex/introduction-to-how-an-multilayer-perceptron-works-but-without-complicated-math-a423979897ac)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Large learning rate](https://miro.medium.com/v2/resize:fit:640/format:webp/1*P_Ll2G6WSFy-1STbYPLTBQ.gif)](https://medium.com/codex/introduction-to-how-an-multilayer-perceptron-works-but-without-complicated-math-a423979897ac)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "*However, small learning rates can also get stuck in local mimima. That is why it is interesting to combine large learning rates to approximately scan the loss function lansdcape and then use small learning rates to finally find the global minimum.*\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Local minima problem](https://miro.medium.com/v2/resize:fit:720/format:webp/1*Rh387pxwuU9Owa0QCuN4Yw.gif)](https://medium.com/codex/introduction-to-how-an-multilayer-perceptron-works-but-without-complicated-math-a423979897ac)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    " - **Mini-Batch Gradient Descent**: Commonly, and due to computational efficiency, instead of updating weights after processing the entire dataset (a batch) on each iteration, weights are updated after processing a small subset (mini-batch) of the data. Here it is important to introduce the concept of **epoch**: one complete pass of the entire training dataset through the learning algorithm.\n",
    " \n",
    "*If we decide to pass the entire dataset, an epoch corresponds to one interation. This is computationally inneficient but the algorithm can find an stay on a loss minimum.*\n",
    " \n",
    " <!-- Add an empty line here -->\n",
    "\n",
    "[![Entire batch](https://miro.medium.com/v2/resize:fit:720/format:webp/1*fbESYSVwSqcGFvRJWGPZMQ.png)](https://medium.com/codex/introduction-to-how-an-multilayer-perceptron-works-but-without-complicated-math-a423979897ac)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    " \n",
    "*However, if we pass a Mini-Batch of, for instance 25% of the entire dataset, we will need 4 iterations of the algorithm for an entire epoch. It will be computationally more efficient but, since each mini-batch is a different data subset, the algorithm doesn’t settle down to a minimum point.*\n",
    "\n",
    " <!-- Add an empty line here -->\n",
    "\n",
    "[![Mini-batch](https://miro.medium.com/v2/resize:fit:720/format:webp/1*25NS5rOg8kESmH-xy78Cvg.png)](https://medium.com/codex/introduction-to-how-an-multilayer-perceptron-works-but-without-complicated-math-a423979897ac)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "\n",
    "After this review on general concepts, we can further explore types of DNNs based on the architecture, the interconexion of the different layers, and find at which kind of problem they excel. Common architectures are **Dense or fully connected Neural Networks** and **Convolutional Neural Networks (CNNs)**, with respect to the FNNs, or the different subtypes of **Recurrent Neural Networks (RNNs)**. On this course we will only explore Dense Neural Networks and CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcecbd8",
   "metadata": {},
   "source": [
    "## Dense Neural Networks\n",
    "\n",
    "The simplest form of a deep neural networks with information flowing in one direction is the one with fully connected hidden layers: a **dense neural network**. They are usually employed for basic classification and regression tasks.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Multi-layered perceptron](https://miro.medium.com/v2/resize:fit:640/format:webp/1*4_BDTvgB6WoYVXyxO8lDGA.png)](https://medium.com/codex/introduction-to-how-an-multilayer-perceptron-works-but-without-complicated-math-a423979897ac)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n",
    "\n",
    "**Advantages:**\n",
    "   - Dense neural networks are very **versatile** and can be applied to a wide range of data and tasks, including classification and regression, with a straightforward architecture of input, hidden, and output layers.\n",
    "\n",
    "**Disadvantages:**\n",
    " - Dense neural networks do **not inherently understand spatial relationships** in data. That is why they are less effective for tasks where spatial information is crucial, such as image processing.\n",
    "- Moreover, FNNs are prone to **overfitting**, especially with large numbers of parameters. Hence, regularization techniques may be required to mitigate this effect.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Add an empty line here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72780977",
   "metadata": {},
   "source": [
    "We are going to train a dense neural network as our first hands-on exercise. For that purpose we are going to use **keras** the user-friendly high-level neural networks API written in Python.\n",
    "\n",
    "The advantatge of this package is that is compatible with various backends (it can run control different numerical computation libraries), for instance, **TensorFlow** where it is integrated as its official high-level API. **TensorFlow** (developed by *Google*) is, together with **PyTorch** (developed by *Meta*) the most popular software for deep learning.\n",
    "\n",
    "If you are interested, you can interactively play online with it at https://playground.tensorflow.org for simple classification tasks to understand the basic ideas behind deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3abe2e",
   "metadata": {},
   "source": [
    "We will try to classify all the different types of primary tumors based on transcriptomic information. Remember that we were only able to separate the two blood types of cancers and roughly some types of kidney tumors while using simple supervised methods on each primary cancer type location alone!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56056af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "expression_df = pd.read_csv(path.join('data', 'gene_expression.tsv.gz'),\n",
    "                                                        sep=\"\\t\", header='infer', index_col=0, compression='gzip')\n",
    "\n",
    "sample_df = pd.read_csv(path.join('data', 'sample_df.tsv'), sep=\"\\t\", header='infer')\n",
    "\n",
    "histology_df = pd.read_excel('data/pcawg_specimen_histology_August2016_v9.xlsx')\n",
    "histology_df.columns = ['icgc_specimen_id'] + list(histology_df.columns[1:])\n",
    "tumortype_dict = dict(zip(histology_df.icgc_specimen_id, histology_df.histology_abbreviation))\n",
    "\n",
    "# Identify intersection columns\n",
    "valid_samples = [col for col in expression_df.columns if col in tumortype_dict]\n",
    "expression_df = expression_df[valid_samples]\n",
    "X = expression_df.to_numpy().T\n",
    "\n",
    "cancer_types = [tumortype_dict[x] for x in expression_df.columns]\n",
    "labels_dict = {gene_id: i for i, gene_id in enumerate(set(cancer_types))}\n",
    "y = np.array([labels_dict[x] for x in cancer_types])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c4f407-8cd8-4c53-baa0-c869a12a08da",
   "metadata": {},
   "source": [
    "We will do the train/test split with 25% of the samples in the test split. Notice that we sample the train and test dataset with stratification based on the labels (the different tumor histological types) to ensure that both datasets have the same distribution across classes.\n",
    "\n",
    "We will need to encode the labels into dummy vectors (known as **one-hot encoding**).\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![One-hot encoder](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*ggtP4a5YaRx6l09KQaYOnw.png)](https://towardsdatascience.com/building-a-one-hot-encoding-layer-with-tensorflow-f907d686bf39)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "\n",
    "**Keras** has a utility that allows to easily convert the histological labels into vectors that encode the output category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c1bc1a-78eb-4a37-b3b1-dbd1a9bb7b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y)\n",
    "\n",
    "# Standarization step (note that we scale with the same fit of the train to the train and the test).\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc6622a-ec33-4a44-bf26-a98da622ed34",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will use a PCA as a way to reduce the dimensionality and select less features: we will take the first 30 principal components as input features, which are linear combinations of the original expression of more than 20,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15622bc1-f5db-4c60-87b9-7b5df68d2fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# We fit the PCA using the training set, and then we project both training and test set\n",
    "pca = PCA(n_components = 30)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58ce56-4c13-4ac0-a723-b1fe262eb9ce",
   "metadata": {},
   "source": [
    "Now we can proceed to specify with the **keras** interface the architecture of our fully connected neural network. As the activation function on the hidden layers we will use the **sigmoid function**:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(x) = \\frac{1}{1 + \\exp{-x}}\n",
    "\\end{equation}\n",
    "\n",
    "One of the advantatges of this function is that the derivative is very convenient, hence its simplicity when applying the **chain rule of calculus** during backpropagation.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d\\sigma(x)}{dx} = (1 - \\sigma(x))·\\sigma(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927f6887",
   "metadata": {},
   "source": [
    "The activator function that we are going to use on the output layers is the **softmax function**. It converts a vector of raw numbers or scores that are inputed from the hiddern layers into a vector of probabilities that are proportional of the relative scale of each value in the input vector. Given a vector $z = [z_1, z_2, ..., z_C]$ representing the raw scores for $C$ classes, the softmax function is defined as:\n",
    "\n",
    "$$\\text{Softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}$$\n",
    "\n",
    "for each element $i$ in the range from 1 to $C$. Mathematically, this function exponentiates the raw input scores and normalizes them by the sum of the exponentiated scores thus ensuring that the resulting values lie in the range [0, 1] and sum to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36947d",
   "metadata": {},
   "source": [
    "With respect to the **loss function**, given the classification task at hand, we will use the **cross-entropy function** (a.k.a **log-loss function**) for the loss modelling. For a single training with $C$ classes the formula is:\n",
    "\n",
    "$$H(y, p) = - \\sum_{i=1}^{C} y_i \\cdot \\log(p_i)$$\n",
    "\n",
    "where:\n",
    "- $y_i$ is the true label (ground truth) for class $i$.\n",
    "- $p_i$ is the predicted probability for class $i$, obtained from the softmax fucntion activation in the output layer of the neural network.\n",
    "\n",
    "Note that this mathematical formula ensures the following properties:\n",
    "\n",
    "- The contribution to the loss is determined by the negative logarithm of the predicted probability (\\(p_i\\)) for the considered class.\n",
    "- The loss is minimized when the predicted probabilities align with the true distribution of class labels.\n",
    "- Probability predictions that diverge more from the true label are more heavily penalized than predictions that are close to the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d0f977-06eb-4d8e-ad13-045aefec7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# The shape of the input (the 30 Principal components, features that we are going to input)\n",
    "input_shape = X_train.shape[-1]\n",
    "# The amount of intermediate neurons we want on the hidden layers\n",
    "n_intermediate = 64\n",
    "# And the size of the vector we want as output (the probabilities of belonging to the 27 histological types)\n",
    "n_classes = y_train.shape[-1]\n",
    "\n",
    "# We define a networks made of sequential layers of neurons\n",
    "model = Sequential()\n",
    "## Two hidden layers densely connected of (the first one with an input of the shape of the 30 PCA values)\n",
    "## Note that we use the sigmoid activation function on both input and hidden layers\n",
    "model.add(Dense(n_intermediate, activation='sigmoid', input_shape=(X_train.shape[-1],)))\n",
    "model.add(Dense(n_intermediate, activation='sigmoid'))\n",
    "## An output layer with the softmax function\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# Show a summary of the model with all the parameters (weights) to train\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde599d6-9574-4164-b2bb-8e4c781ab15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD # Stochastic Gradient Descent optimizer, the algorithm to minimize the loss function\n",
    "\n",
    "# Compile the model with categorical crossentropy as the loss function and with a 5% of learning rate\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    # As the numerical method to minimize the loss function we will use the Stochastic gradient descent (SGD)\n",
    "    optimizer=SGD(learning_rate=0.05),\n",
    "    ## Accuracy, the ratio of correctly predicted observations to the total, as the evaluation metric\n",
    "    metrics=[\"accuracy\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb0ae5-4332-4bcb-8bd3-b75d9c8e3688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we train the model\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    # 500 epoch, i.e. 500 times we pass the entire training dataset\n",
    "    epochs=500,\n",
    "    # We will pass the input on each iteration in batches of 50 samples\n",
    "    batch_size=50, \n",
    "    verbose=1,\n",
    "    # The test dataset will be used for evaluation purposes\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb8cc8a",
   "metadata": {},
   "source": [
    "Ok, now it is time to interpret the output of the training process. Basically, there has been an update of the weights of all the connections at our neural network for **500 epochs** (500 times that the training dataset has been passed), which corresponds to **19 training iterations/epoch** given the batch size of **50 samples per iteration**. For each epoch, the **loss value** (the measure of the error on the classification) is computed together with the **accuracy** of the method. These are measures using the training dataset, while **val_loss** and **val_acc** are being computed while passing the **test_dataset** at the current training state of the neural network. \n",
    "\n",
    "Since it is hard to see how all the metrics change with time with a flat text, we can define a function to plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ad985-8745-473c-b684-7fcf66f09b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_curves(history):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    epochs = range(len(history.history['loss']))\n",
    "\n",
    "    plt.plot(epochs, loss, label='training_loss')\n",
    "    plt.plot(epochs, val_loss, label='val_loss')\n",
    "    plt.title('loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, accuracy, label='training_accuracy')\n",
    "    plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8e1b90-a3ce-48e9-a973-473bc27329ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bab1d4",
   "metadata": {},
   "source": [
    "What can we observe? Note that:\n",
    "\n",
    "- At the start both training and validation loss values are high but they both shrink as the training continues. The test loss finally reaches a value and becomes stable while the one from the training dataset keeps being minimized although slowly.\n",
    "\n",
    "- Analogously, at the start both training and validation accuracies are close to what we expect from a random classifier (probability of 1/27 classes) but they both improves greatly as the training continues. The training accuracy at some point keeps improving slowly while the one at the test dataset reaches a plateau.\n",
    "\n",
    "What can we say about the training process from these two plots? Remember that:\n",
    "\n",
    "- **Underfitting** will be recognizable if training and validation losses remained high during the training process, indicating the model fails to capture patterns in the training data and, hence, also performs poorly on unseen data.\n",
    "\n",
    "- **Overfitting** is observed when low training loss are achieved but validation loss remains high, which suggests that the model has learned the training data too well, including its noise, and fails to generalize to new data.\n",
    "\n",
    "- The desired scenario, a **properly fitted model** on the best bias-variance trade-off, is observed when both training and validation losses decrease and converge, indicating a model that generalizes well to new data.\n",
    "\n",
    "\n",
    "Considering that training loss continues to increase (same with accuracy on decrease) while validation loss and accuracy plateaus, we can see that beyond 100 and few epochs the model starts to enter into the overfitting dynamic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a1fc5",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs are designed to exploit the spatial structure of data through extracting local connectivity patterns and detecting patterns independently of their location in the input data. That is why they are mostly used for working with structured data such as images (or even continous stream of images such as videos); from a computational point of view images are 2D matrixes of values for black and white images or 3D matrixes in the case of colour images, encoded for the **Red, Green and Blue (RGB)** codes.\n",
    "   \n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![MNIST image](https://www.researchgate.net/profile/Soha-Boroojerdi/publication/361444345/figure/fig1/AS:11431281091155908@1666326170916/Representation-of-value-three-in-the-MNIST-dataset-and-its-equivalent-matrix.ppm)](https://www.researchgate.net/figure/Representation-of-value-three-in-the-MNIST-dataset-and-its-equivalent-matrix_fig1_361444345)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![RGB encoding](https://e2eml.school/images/image_processing/three_d_array.png)](https://e2eml.school/convert_rgb_to_grayscale.html)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Perhaps the best way to understand how CNNs work is to summarize some concepts on photography edition. Let's download an image and apply some photographic **filters**. From a computational point of view, applying filters to images involves multiplying the image matrixes with other matrixes, for instance, to remove the red colours.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![Filter](https://i0.wp.com/cdn.makezine.com/uploads/2011/03/filter_example.png)\n",
    "\n",
    "<!-- Add an empty line here -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcfb087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# These are python libraries to download and process images\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# URL of the image\n",
    "image_url = \"https://www.uvic.cat/sites/default/files/styles/image_960x650/public/sites/all/images/uvic-_retorn_2021_0.jpg\"\n",
    "\n",
    "# Download the image and get the underlying array\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "image_array = np.array(image)\n",
    "\n",
    "# This image can be ploted with the imshow method of matplotlib\n",
    "plt.imshow(image_array)\n",
    "plt.title(\"Uvic image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7088cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can apply several filters\n",
    "\n",
    "## Remove red\n",
    "remove_red_filter = np.array([[0,0,0],\n",
    "                              [0,1,0],\n",
    "                              [0,0,1]])\n",
    "\n",
    "### Apply the filter to the image\n",
    "no_red_image = np.dot(image_array, remove_red_filter)\n",
    "\n",
    "plt.imshow(no_red_image)\n",
    "plt.title(\"Uvic image without red\")\n",
    "plt.show()\n",
    "\n",
    "## Get only green channel\n",
    "green_only_filter = np.array([[0,0,0],\n",
    "                              [0,1,0],\n",
    "                              [0,0,0]])\n",
    "\n",
    "### Apply the filter to the image\n",
    "green_only_image = np.dot(image_array, green_only_filter)\n",
    "\n",
    "plt.imshow(green_only_image)\n",
    "plt.title(\"Uvic image only green\")\n",
    "plt.show()\n",
    "\n",
    "## Sepia filter\n",
    "sepia_filter = np.array([[.393, .769, .189],\n",
    "                         [.349, .686, .168],\n",
    "                         [.272, .534, .131]])\n",
    "\n",
    "### Apply the filter to the image\n",
    "sepia_image = np.dot(image_array, sepia_filter.T)\n",
    "\n",
    "### Unfortunately the filter lines do not have unit sum, so we need to rescale\n",
    "sepia_image /= sepia_image.max()\n",
    "\n",
    "plt.imshow(sepia_image)\n",
    "plt.title(\"Uvic image but old\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c091f870",
   "metadata": {},
   "source": [
    "The basic concept behind CNNs are convolution operations, which in fact are matrix multiplications applied not once to the entire matrix but as sliding window across the image and the result is summed an outputed as an element of a new but smaller matrix.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Convolution](https://dennybritz.com/img/Convolution_schematic.gif)](https://dennybritz.com/posts/wildml/understanding-convolutional-neural-networks-for-nlp/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "The matrix that is applied on the sliding window is called a **kernel, filter, or feature detector** and performs a task of extracting information of the matrix and summarizes it on a new matrix. Let's try two convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951fd505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this package from scipy\n",
    "from scipy.signal import convolve\n",
    "\n",
    "# Extract edges\n",
    "edges_kernel = np.array([[0,1,0],\n",
    "                        [1,-4,1],\n",
    "                        [0,1,0]])\n",
    "# Make it a 3D image\n",
    "edges_kernel = edges_kernel[:, :, None]\n",
    "\n",
    "### Apply the kernel to the image\n",
    "edges_image = convolve(image_array, edges_kernel)\n",
    "\n",
    "plt.imshow(edges_image)\n",
    "plt.title(\"UVic image but edges\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c4c22a",
   "metadata": {},
   "source": [
    "Note that imshow raises a warning: after applying the kernel, the resulting values are beyond the normal range of floats and integers that codify an image; **convolution operations might generate value ranges that are not directly interpretable as images**, which is why outputs must be properly normalized.\n",
    "\n",
    "This also applies when feeding an image to a CNN, where the input should be converted in ranges of values that the input layer accepts.\n",
    "\n",
    "---\n",
    "\n",
    "CNNs apply convolutions recurrently on different layers so the output of one layer is the input of the next one. To exemplify it, let's apply a kernel that blurs the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b1d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "\n",
    "# We will need to define some functions before\n",
    "def multi_convolver(image, kernel, iterations):\n",
    "    for i in range(iterations):\n",
    "        image = convolve2d(image, kernel, 'same', boundary = 'fill',\n",
    "                           fillvalue = 0)\n",
    "    return image\n",
    "\n",
    "def convolver_rgb(image, kernel, iterations=1):\n",
    "    convolved_image_r = multi_convolver(image[:,:,0], kernel,\n",
    "                                        iterations)\n",
    "    convolved_image_g = multi_convolver(image[:,:,1], kernel, \n",
    "                                        iterations)\n",
    "    convolved_image_b  = multi_convolver(image[:,:,2], kernel, \n",
    "                                         iterations)\n",
    "    \n",
    "    reformed_image = np.dstack((np.rint(abs(convolved_image_r)), \n",
    "                                np.rint(abs(convolved_image_g)), np.rint(abs(convolved_image_b)))) / 255\n",
    "   \n",
    "\n",
    "    return reformed_image\n",
    "\n",
    "\n",
    "\n",
    "## We will apply this blur kernel\n",
    "blur_kernel = (1 / 16.0) * np.array([[1., 2., 1.],\n",
    "                                  [2., 4., 2.],\n",
    "                                  [1., 2., 1.]])\n",
    "\n",
    "# We can apply the kernel iteratively to get an extra effect\n",
    "blur_output = convolver_rgb(image_array, blur_kernel, iterations = 1)\n",
    "plt.imshow(blur_output)\n",
    "plt.title(\"UVic image but slightly blur\")\n",
    "plt.show()\n",
    "\n",
    "blur_output2 = convolver_rgb(image_array, blur_kernel, iterations = 10)\n",
    "plt.imshow(blur_output2)\n",
    "plt.title(\"UVic image but more blur\")\n",
    "plt.show()\n",
    "\n",
    "blur_output3 = convolver_rgb(image_array, blur_kernel, iterations = 100)\n",
    "plt.imshow(blur_output3)\n",
    "plt.title(\"UVic image but super blur\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d4a301",
   "metadata": {},
   "source": [
    "Hence, ased on the same principle, CNNs apply convolutions and other types of operations combined into an architecture of layers with two main purposes:\n",
    "\n",
    "1. **Learn the features** of the images in a hierarchical way.\n",
    "\n",
    "    - This is achieved by the combination of **convolutional layers** that apply a given **kernel** and generate as output feature maps that summarize the large (and assumed as important) structural elements of the image. There are two **hyperparameters** associated with this procedure:\n",
    " \n",
    "\n",
    "        - **Padding** is the process of adding extra pixels (usually zero-valued) around the input image or feature map before applying convolutional operations in order to ensure that the size of the output image is the same as the input.\n",
    "\n",
    "        - **Stride** refers to the step size with which the convolutional filter moves across the input image (or feature map on subsequent layer) during the convolution operation. Hence, this hyperparameter controls how much reduction in the spatial dimensions of the feature map happens on each convolution.\n",
    "    \n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Convolution](https://miro.medium.com/v2/resize:fit:720/format:webp/1*O06nY1U7zoP4vE5AZEnxKA.gif)](\n",
    "https://medium.com/@Suraj_Yadav/in-depth-knowledge-of-convolutional-neural-networks-b4bfff8145ab)\n",
    "    \n",
    "<!-- Add an empty line here -->\n",
    "    \n",
    "   - Once the feature map is learned, this is passed into **pooling layers** that **downsample the feature maps** and reduce the size of the feature map by selecting the most relevant extracted on the previous layer and pass them to the next layer.\n",
    " \n",
    "<!-- Add an empty line here -->\n",
    " \n",
    "[![Pooling](https://miro.medium.com/v2/resize:fit:720/format:webp/1*vOxthD0FpBR6fJcpPxq6Hg.gif)](\n",
    "https://medium.com/@Suraj_Yadav/in-depth-knowledge-of-convolutional-neural-networks-b4bfff8145ab)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "   - Through diffent rounds of convolutional and pooling layers, the network can move from learning simple and low-level features such as edges and textures at the start to more complex and high-level features at the end.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "2. **Final classification** based on the learned features.\n",
    "\n",
    "    - Once the relevant information is extracted, a **dense or fully connected neuron layer** allows for the classification based on the information preproccessed in the previous layers.\n",
    "  \n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![CNN general structure](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*vkQ0hXDaQv57sALXAJquxA.jpeg)](\n",
    "https://medium.com/@Suraj_Yadav/in-depth-knowledge-of-convolutional-neural-networks-b4bfff8145ab)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "   \n",
    "---\n",
    "\n",
    "**Advantages:**\n",
    "- CNNs are designed to **capture spatial hierarchies** through the different rounds of convolutional layers.\n",
    "- **Computationally efficient** due to the reduced number of parameters compared to a dense architecture.\n",
    "\n",
    "**Disadvantages:**\n",
    "- CNNs have a more **complex architecture** which implies a **difficult interpretation** of what are processing the last layers.\n",
    "- CNNs usually **require a large amount of labeled data** for training, especially for deep architectures, so data scarcity might lead to suboptimal performance.\n",
    "- CNNs may struggle to capture long-range dependencies in data as they focus on local structures, hence, they are **less effective for tasks where global context is crucial**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39292b1c-ef0e-4819-a2f6-eb1b967fccaf",
   "metadata": {},
   "source": [
    "For the hands-on exercise we will use a dataset of breast histology images that we will try to classify on having cancerous cells or not. Go to https://www.kaggle.com/code/paultimothymooney/predicting-idc-in-breast-cancer-histology-images and download the dataset (on the input tab). The link includes a description explaining the context and content of the dataset together with the code that the author provides to do an analysis (I deeply encourage you to look and play with the author's code, since he applies multiple types of classifiers that we have seen here and other news).\n",
    "\n",
    "We will use part of its code for the processing of the data, but we will build our own CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b8d71-b6d3-4ffe-a692-15732e15cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the array of images that are already in numpy format\n",
    "X = np.load(path.join('data', 'X.npy')) # images (features)\n",
    "Y = np.load(path.join('data', 'Y.npy')) # labels associated to images (0 = no IDC, 1 = IDC)\n",
    "\n",
    "# A summary of the dataset\n",
    "print('Total number of images: {}'.format(len(X)))\n",
    "print('Number of IDC(-) Images: {}'.format(np.sum(Y==0)))\n",
    "print('Number of IDC(+) Images: {}'.format(np.sum(Y==1)))\n",
    "print('Percentage of positive images: {:.2f}%'.format(100*np.mean(Y)))\n",
    "print('Image shape (Width, Height, Channels): {}'.format(X[0].shape)) # The images consist of 50 to 50 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d50f9b",
   "metadata": {},
   "source": [
    "Note that the images are small (50x50 pixels) since the dataset is already highly preprocessed (see documentation). This will help with the training on this hands-on exercise (it will largely reduce the computational time) but on a real situation multiple processing steps might apply.\n",
    "\n",
    "On the theoretical part we saw the general structure of CNNs, however, the amount of possible architectures is virtually infinite. There is no architecture that is optimal for all kind of problems and which specific architecture is best to use depends on the problem at hand. Moreover, this kind of decision usually involves **trial and error** with multiple candidates, as happens at the original implementation on Kaggle.\n",
    "\n",
    "Here we are going to try a very well-known architechture known as **LeNet-5**, which was already descibed by Yann Lecun in 1998. This CNN network has 5 layers (hence its name): 3 sets of convolution layers with a combination of average pooling, followed by 2 fully connected layers with a Softmax classifier. Our implementation is going to be more simple from the ones at Kaggle, as we will not use the RGB information and we will just work with greyscale images (we will loose part of the information from the tincture at the histology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d418effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the training and test dataset (20% on testing)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Note that the training dataset consists of 4437 images RGB of 50 x 50 pixels\n",
    "print(np.shape(X_train))\n",
    "# And the test of 1110 images\n",
    "print(np.shape(X_test))\n",
    "\n",
    "# We can print the first image of the training set\n",
    "plt.imshow(X_train[0])\n",
    "plt.show()\n",
    "\n",
    "# We reduce the dimensionality by working with greyscale images. The transformation is the following for our eyes:\n",
    "## Gray code = 0.299 * red + 0.5870 * green + 0.1140 * blue\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "    return gray\n",
    "\n",
    "# We apply the function to each image in a vectorized way\n",
    "## Create an empty array for grayscale images\n",
    "X_train_gray = np.zeros((np.shape(X_train)[0], np.shape(X_train)[1], np.shape(X_train)[2], 1))\n",
    "X_test_gray = np.zeros((np.shape(X_test)[0], np.shape(X_test)[1], np.shape(X_test)[2], 1))\n",
    "\n",
    "# Apply rgb2gray function to each image for the training set\n",
    "for i in range(np.shape(X_train)[0]):\n",
    "    X_train_gray[i, :, :, 0] = rgb2gray(X_train[i, :, :, :])\n",
    "    \n",
    "# Apply rgb2gray function to each image for conversion for the test set\n",
    "for i in range(np.shape(X_test)[0]):\n",
    "    X_test_gray[i, :, :, 0] = rgb2gray(X_test[i, :, :, :])\n",
    "    \n",
    "# We can print the first image to see the conversion into grey scale as how our human eyes will see\n",
    "plt.imshow(X_train_gray[0], cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e79e7f",
   "metadata": {},
   "source": [
    "Now let's decode the operations we will be performing in each layer:\n",
    "\n",
    "1. **Convolutional Layer (CONV1)**:\n",
    "\n",
    "    Parameters: Input (N) = 50, Padding (P) = 2, Kernel (K) = 5 x 5, Stride (S) = 1\n",
    "    Convolutional Operation: ((N + 2P - K) / S) + 1 = ((50 + 4 - 5) / 1) + 1 = 50 x 50\n",
    "    We will apply 12 filters / kernels so we will get a 50 x 50 x 12 dimensional output\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "2. **Average Pooling Layer (POOL1)**:\n",
    "\n",
    "    Parameters: Input (N) = 50, Padding (P) = 0, Kernel (K) = 2 x 2, Stride (S) = 2\n",
    "    AVG Pooling Operation: ((N - K) / S) + 1 = ((50 - 2) / 2) + 1 = 25 x 25\n",
    "    We will have a 25 x 25 x 12 dimensional output at the end of this pooling\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "3. **Convolutional Layer (CONV2)**:\n",
    "\n",
    "    Parameters: Input (N) = 25, Padding (P) = 2, Kernel (K) = 5 x 5, Stride (S) = 2\n",
    "    Convolutional Operation: ((N + 2P - K) / S) + 1 = ((25 + 4 - 5) / 2) + 1 = 13 x 13\n",
    "    We will apply 20 filters / kernels so we will get a 13 x 13 x 20 dimensional output\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "4. **Average Pooling Layer (POOL2)**:\n",
    "\n",
    "    Parameters: Input (N) = 13, Padding (P) = 0, Kernel (K) = 3 x 3, Stride (S) = 2\n",
    "    AVG Pooling Operation: ((N + 2P - K) / S) + 1 = ((13 - 3) / 2) + 1 = 6 x 6\n",
    "    We will have a 6 x 6 x 20 dimensional output at the end of this pooling\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "5. **Fully Connected layer (FC1)**:\n",
    "\n",
    "    Parameters: W: 720 * 100, b: 100\n",
    "    We will have an output of 100 x 1 dimension\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "6. **Fully Connected layer (FC2)**:\n",
    "\n",
    "    Parameters: W: 128 * 40, b: 40\n",
    "    We will have an output of 40 x 1 dimension\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "7. **Output layer (Softmax)**:\n",
    "\n",
    "    Parameters: W: 40 * 2, b: 2\n",
    "    We will get an output of 2 x 1 dimension\n",
    "    \n",
    "Moreover, we will have to normalize the pixel values, from [0, 255] range to [0,1] values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bcf62f-6640-409c-b178-a372f129a0ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPool2D, Flatten\n",
    "\n",
    "# We define some starting parameters\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 100\n",
    "img_rows, img_cols = X_train_gray.shape[1], X_train_gray.shape[2]\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# We one-hot encode the training and test labels\n",
    "Y_train_one_hot = to_categorical(Y_train, num_classes=num_classes)\n",
    "Y_test_one_hot = to_categorical(Y_test, num_classes=num_classes)\n",
    "\n",
    "# Assuming X_train_gray and X_test_gray are your input images\n",
    "X_train_gray = X_train_gray / 255.0\n",
    "X_test_gray = X_test_gray / 255.0\n",
    "\n",
    "# Let's define the LeNet-5 model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=12, kernel_size=(5,5), padding='same', strides=1, activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPool2D(strides=2))\n",
    "model.add(Conv2D(filters=20, kernel_size=(5,5), padding='same', strides=2, activation='relu'))\n",
    "model.add(MaxPool2D(strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax')) # Two values as output\n",
    "\n",
    "# Compile the model with categorical crossentropy as the loss function and with a 5% of learning rate\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    # As the numerical method to minimize the loss function we will use the Stochastic gradient descent (SGD)\n",
    "    optimizer=SGD(learning_rate=0.05),\n",
    "    ## Accuracy, the ratio of correctly predicted observations to the total, as the evaluation metric\n",
    "    metrics=[\"accuracy\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf01e459",
   "metadata": {},
   "source": [
    "Finally we train the model. Note that it will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b146cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we train it\n",
    "history = model.fit(X_train_gray, Y_train_one_hot,\n",
    "          batch_size=batch_size,\n",
    "          verbose=1,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test_gray, Y_test_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039183cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abc4409",
   "metadata": {},
   "source": [
    "What can we infer from the profile? This simple architecture does not seem to capture the relevant information and build an accurate model, since the accuracy on the test dataset plateaus at around 70% of accuracy (remember that for a binary classifier, we expect 50% of accuracy of a random untrained model).\n",
    "\n",
    "Moreover, by comparing the training and testing dataset loss and accuracy, we can infer that a little earlier of the 20 epochs the model cannot improve more since the metrics on the test dataset get stuck (it fluctuates or even decays) while the training metrics keep improving, suggesting that the model enters into an overfitting dynamic.\n",
    "\n",
    "This was expected, since LeNet-5 is a very simple CNN whose main application is to distinguish digits (https://en.wikipedia.org/wiki/LeNet), not for complex textures like cancer histology. To get better results, we need a \"modern\" approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53423588",
   "metadata": {},
   "source": [
    "### Improving CNN Performance\n",
    "\n",
    "Since we observed **Overfitting** (Training accuracy > Test accuracy) and limited performance, we will upgrade our approach. We will move from a 1990s architecture to a modern VGG-style block architecture by introducing three key concepts:\n",
    "\n",
    "1. **Dropout:** Randomly ignoring neurons during training. This prevents neurons from co-adapting too much and forces the network to learn more robust features.\n",
    "2. **Batch Normalization:** Normalizing the inputs of each layer (re-centering and re-scaling). This stabilizes the learning process and drastically speeds up training.\n",
    "3. **Data Augmentation:** We have a small dataset. We can artificially increase it by rotating, shifting, and flipping our existing images. This teaches the model that a cancer cell is still a cancer cell, even if it's upside down.\n",
    "4. **Change the optimizer:** For instance, Adam optimizer, which generally performs better than SGD for this type of problem.\n",
    "5. **Change the learning rate:** The sawtooth pattern on the validation dataset points to a too high leraning rate. We can also dinamically modify it during training based on the evilution of the test dataset.\n",
    "\n",
    "Let's implement this improved model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc94ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    RandomRotation,\n",
    "    RandomTranslation,\n",
    "    RandomFlip\n",
    ")\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# 3. Data Augmentation Configuration\n",
    "# We define how we want to \"distort\" the images to create new training samples\n",
    "data_augmentation = Sequential([\n",
    "    RandomRotation(0.11),          # ≈ 20 degrees\n",
    "    RandomTranslation(0.2, 0.2),   # width & height shift\n",
    "    RandomFlip(\"horizontal_and_vertical\"),\n",
    "])\n",
    "\n",
    "# 1 and 2 Improved Architecture\n",
    "# We use a pattern common in modern nets: Conv -> BatchNorm -> Pool -> Dropout\n",
    "model = Sequential([\n",
    "    # Data augmentation runs only during training\n",
    "    data_augmentation,\n",
    "\n",
    "    # Block 1\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(50, 50, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    # Block 2\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    # Fully Connected\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# 4 We switch to Adam, with specified starting learning rate.\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=3e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "## The learning rate scheduler reduces the learning rate when the validation loss plateaus\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,      # halve the LR\n",
    "    patience=4,      # wait 4 epochs\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train with Augmentation\n",
    "history_improved = model.fit(\n",
    "    X_train_gray,\n",
    "    Y_train_one_hot,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test_gray, Y_test_one_hot),\n",
    "    callbacks=[lr_scheduler], # The learning rate scheduler\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_loss_curves(history_improved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcebb27f",
   "metadata": {},
   "source": [
    "Note that, comparing this new plot with the previous one, we observe:\n",
    "\n",
    "- The Training and Validation loss lines are closer together (reduced overfitting) when plateaus.\n",
    "- The accuracy on the test does not improve much, but remains stable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MScOmicsVic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "308px",
    "width": "346px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
